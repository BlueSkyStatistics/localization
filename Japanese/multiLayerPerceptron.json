{
  "title": "マルチレイヤーパセプトロン、RSNNSパッケージを使用",
  "navigation": "マルチレイヤーパセプトロン",
  "label1": "ダミーコードのファクタ変数を作成してください。データ > ダミー変数を計算するを参照してください（すべてのレベルを保持、すなわち1ホットエンコーディング）。数値変数をスケールおよびセンタリングしてください。データ > 変数を標準化するを参照してください。",
  "model": "モデル名を入力してください",
  "dependentvar": "従属変数",
  "independentvars": "独立変数",
  "seed": "シードを設定",
  "iter": "学習の最大反復回数",
  "tf": "学習関数",
  "label2": "隠れ層の数と隠れ層ごとのニューロン数",
  "layers": "各層のニューロン数を指定してください。例えば、1層に5ニューロンの場合は5と入力します。1層に5ニューロン、2層に6ニューロン、3層に7ニューロンの場合は5,6,7と入力します。",
  "learnfuncparams": "学習関数のパラメータ",
  "help": {
    "title": "マルチレイヤーパセプトロン、RSNNSパッケージを使用",
    "r_help": "help(mlp, package ='RSNNS')",
    "body": "\n            <b>注意</b></br>\n            単一の従属変数を指定する場合、それは数値またはファクタであることができます。指定された従属変数がファクタの場合、RSNNSパッケージのデコード関数を使用して、ファクタ変数を自動的に1ホットエンコーディングでダミーコードします。</br></br>\n            さらに、ファクタ変数をダミーコードするために1ホットエンコーディングを使用している場合、ダイアログで複数の従属変数を指定できます。この場合、従属変数は数値型でなければなりません。</br></br>\n            \"データ > ダミー変数を計算する\"を使用して、\"すべてのレベルを保持\"設定を選択して1ホットエンコーディングを取得できます。</br></br>\n            ファクタ型の従属変数の場合、構築したモデルを使用してデータセットをスコアリングする際に、混同行列、ROC、およびモデル精度統計を表示します。生成された予測はファクタ型であり、クラスを予測します。これらは、スコアリング時に予測確率とともにデータセットに保存されます。</br></br>\n            ダミーコードされた従属変数がある場合、構築したモデルを使用してデータセットをスコアリングする際に、混同行列、ROC、およびモデル精度統計は表示されません。ただし、スコアリング時にデータセットに予測が保存されます。予測は、ダミーコードされた従属変数に関連する確率です。</br></br>\n            独立変数は標準化するのが最善です（それらも数値でなければなりません）。\"データ > 変数を標準化する\"を参照してください。</br></br>\n            カテゴリカルな独立変数がある場合、ファクタ変数をダミーコードするために1ホットエンコーディングを使用してください。</br></br>\n            <b>説明</b></br>\n            この関数はマルチレイヤーパセプトロン（MLP）を作成し、トレーニングします。MLPは完全に接続されたフィードフォワードネットワークであり、おそらく最も一般的なネットワークアーキテクチャです。トレーニングは通常、誤差逆伝播法または関連する手順によって行われます。</br>\n            SNNSには、この関数と一緒に使用できるさまざまな学習関数が存在します。例：Std_Backpropagation、BackpropBatch、BackpropChunk、BackpropMomentum、BackpropWeightDecay、Rprop、Quickprop、SCG（スケーリング共役勾配）など。</br>\n            <b>使用法</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## デフォルトのS3メソッド:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>引数</b><br/>\n            <ul>\n            <li>\n            x: ネットワークのトレーニング入力の行列\n            </li>\n            <li>\n            ... : 追加の関数パラメータ（現在は使用されていません）\n            </li>\n            <li>\n            y: 対応するターゲット値\n            </li>\n            <li>\n            size: 隠れ層のユニット数\n            </li>\n            <li>\n            maxit: 学習の最大反復回数\n            </li>\n            <li>\n            initFunc: 使用する初期化関数\n            </li>\n            <li>\n            initFuncParams: 初期化関数のパラメータ\n            </li>\n            <li>\n            learnFunc: 使用する学習関数\n            </li>\n            <li>\n            learnFuncParams: 学習関数のパラメータ\n            </li>\n            <li>\n            updateFunc: 使用する更新関数\n            </li>\n            <li>\n            updateFuncParams: 更新関数のパラメータ\n            </li>\n            <li>\n            hiddenActFunc: すべての隠れユニットの活性化関数\n            </li>\n            <li>\n            shufflePatterns: パターンをシャッフルするべきか？\n            </li>\n            <li>\n            linOut: 出力ユニットの活性化関数を線形またはロジスティックに設定（outputActFuncが指定されている場合は無視される）\n            </li>\n            <li>\n            outputActFunc: すべての出力ユニットの活性化関数\n            </li>\n            <li>\n            inputsTest: ネットワークをテストするための入力の行列\n            </li>\n            <li>\n            targetsTest: テスト入力に対応するターゲット\n            </li>\n            <li>\n            pruneFunc: 使用するプルーニング関数\n            </li>\n            <li>\n            pruneFuncParams: プルーニング関数のパラメータ。他の関数とは異なり、これらは名前付きリストで指定する必要があります。詳細な説明については、プルーニングデモを参照してください。\n            </li>\n            </ul>\n            <b>詳細</b></br>\n            Std_Backpropagation、BackpropBatchなどは、学習率と最大出力差の2つのパラメータを持っています。学習率は通常0.1から1の間の値です。これは勾配降下のステップ幅を指定します。最大差は、出力とターゲット値の間の差がゼロエラーとして扱われ、逆伝播されない量を定義します。このパラメータは過剰学習を防ぐために使用されます。すべての学習関数のパラメータの完全なリストについては、SNNSユーザーマニュアルの67ページを参照してください。</br>\n            初期化および更新関数に設定されているデフォルトは通常変更する必要はありません。</br>\n            <b>値</b><br/>\n            rsnnsオブジェクト。\n            <b>参考文献</b><br/>\n            Rosenblatt, F. (1958), 'The perceptron: A probabilistic model for information storage and organization in the brain', Psychological Review 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Group, P. R. (1986), Parallel distributed processing :explorations in the microstructure of cognition, Mit, Cambridge, MA etc.<br/>\n            Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (ドイツ語)<br/>\n            <b>例</b><br/>\n            <code> \n            data(iris)<br/>\n            #ベクトルをシャッフル<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #402040法による混同行列<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>パッケージ</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>ヘルプ</b></br>\n            詳細なヘルプについては、このダイアログオーバーレイの右上にあるRアイコンをクリックするか、R構文エディタで次のコマンドを実行してください。</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
  }
}