{
  "title": "エクストリームグラデーションブースティング",
  "navigation": "エクストリームグラデーションブースティング",
  "label1": "ダミーコード因子変数、変数>計算>ダミーコード使用オプションで全レベルを保持します。複数のクラスを予測する場合は、ヘルプを読んでください（ダイアログの?アイコンをクリック）。",
  "model": "モデル名を入力してください",
  "dependentvar": "従属変数",
  "independentvars": "独立変数",
  "objective": "目的",
  "seed": "シードを設定",
  "nrounds": "最大ブースティング反復回数",
  "maxdepth": "最大深さ",
  "minchildweight": "最小子供の重み",
  "maxdeltastep": "最大デルタステップ",
  "eta": "eta（学習率）",
  "gamma": "ガンマ",
  "numclasses": "クラスの数。**multi:softmaxおよびmulti:softprobでのみ使用してください",
  "basescore": "ベーススコア",
  "Verbose": "詳細モード（0=サイレント、1=パフォーマンス情報、2=その他の情報）",
  "printevery": "詳細が>0のとき、n回ごとに評価メッセージを印刷",
  "help": {
    "title": "エクストリームグラデーションブースティング",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>説明</b></br>\n                エクストリームグラデーションブースティングモデルを作成します\n                <br/>\n                <b>注意</b></br>\n                1.因子型の従属変数を予測するには、従属変数を0から始まる数値に再コーディングする必要があります。たとえば、因子変数に3つのレベルがある場合、数値変数には0、1、2の値が含まれます。</br>\n                データ>変数を再コーディングを参照してください。あるいは、因子変数を数値に変換するだけで、通常はレベルが1から始まる整数にマッピングされ、結果の変数から1を引きます。これにより、0から始まる数値変数が得られます。</br>\n                また、目的をmulti:softmax（クラスを予測）またはmulti:softprob（確率を予測）に設定し、クラス数をクラス数のテキストボックスに入力する必要があります。</br>\n                クラス数はmulti:softmaxおよびmulti:softprobのみに入力する必要があります。他の目的のためにクラス数が入力されるとエラーが生成されます。</br>\n                2.独立因子変数をダミーコードする必要があります。1-Hotエンコーディングを使用して、データ>ダミー変数を計算してください。</br>\n                3.目的がreg:squarederror、reg:logistic、binary:logitraw、rank:pairwiseのいずれかに選択されている場合、混同行列とROC曲線は生成されません。予測関数は従属変数のクラスを返さないためです。予測関数が返す数値予測（reg:squarederrorの場合）、スコア、ランクなどはデータセットに保存されます。詳細についてはhelp(predict.xgb.Booster)を参照してください。</br>\n                4.multi:softprobが選択されている場合、混同行列は生成されません。予測関数は予測確率を返し、従属変数のクラスを返さないためです。予測確率はデータセットに保存され、ROC曲線が生成されます。混同行列を表示するには、目的をmulti:softmaxに選択してください。</br>\n                5.multi:softmaxの目的が選択されている場合、ROC曲線は生成されません。予測関数はクラスを返し、予測確率を返さないためです。ROC曲線を表示するには、目的をmulti:softprobに選択してください。</br>\n                <br/>\n                <b>使用法</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>引数</b><br/>\n                <ul>\n                <li>\n                params: パラメータのリスト。パラメータの完全なリストはhttp://xgboost.readthedocs.io/en/latest/parameter.htmlで入手できます。以下は短い要約です:<br/>\n                </li>\n                <li>\n                1.一般パラメータ<br/>\n                このダイアログはgbtree（ツリーブースター）を使用します<br/>\n                </li>\n                <li>\n                2.ブースターパラメータ<br/>\n                2.1.ツリーブースターのパラメータ<br/>\n                eta: 学習率を制御します: 現在の近似に追加されるときに各ツリーの寄与を0 < eta < 1の因子でスケールします。<br/>\n                過剰適合を防ぐためにブースティングプロセスをより保守的にします。<br/>\n                etaの値が低いほど、nroundsの値が大きくなります: etaの値が低いほど、モデルは過剰適合に対してより堅牢ですが、計算は遅くなります。デフォルト: 0.3<br/>\n                gamma: 葉ノードのさらなる分割を行うために必要な最小損失削減。大きいほど、アルゴリズムは保守的になります。<br/>\n                max_depth: ツリーの最大深さ。デフォルト: 6<br/>\n                min_child_weight: 子供に必要なインスタンスの重み（ヘッシアン）の最小合計。ツリーの分割ステップがmin_child_weight未満のインスタンスの重みの合計を持つ葉ノードを生成した場合、構築プロセスはさらなる分割を放棄します。線形回帰モードでは、これは各ノードに必要なインスタンスの最小数に対応します。大きいほど、アルゴリズムは保守的になります。デフォルト: 1<br/>\n                </li>\n                <li>\n                3.タスクパラメータ<br/>\n                objective: 学習タスクと対応する学習目的を指定します。ユーザーは自己定義関数を渡すことができます。デフォルトの目的オプションは以下の通りです:<br/>\n                reg:squarederror -二乗損失による回帰（デフォルト）。<br/>\n                reg:logistic ロジスティック回帰。<br/>\n                binary:logistic -二項分類のためのロジスティック回帰。出力確率。<br/>\n                binary:logitraw -二項分類のためのロジスティック回帰、ロジスティック変換前のスコアを出力。<br/>\n                num_class: クラスの数を設定します。マルチクラス目的でのみ使用します。<br/>\n                multi:softmax -ソフトマックス目的を使用してマルチクラス分類を行うようにxgboostを設定します。クラスは数字で表され、0からnum_class - 1の範囲である必要があります。<br/>\n                multi:softprob -softmaxと同じですが、予測はndata * nclass要素のベクトルを出力し、さらにndata、nclass行列に再形成できます。結果には、各データポイントが各クラスに属する確率が含まれます。<br/>\n                rank:pairwise -ペアワイズ損失を最小化することによってランキングタスクを行うようにxgboostを設定します。<br/>\n                base_score: すべてのインスタンスの初期予測スコア、グローバルバイアス。デフォルト: 0.5<br/>\n                eval_metric: 検証データの評価指標。ユーザーは自己定義関数を渡すことができます。デフォルト: 目的に応じてメトリックが割り当てられます（回帰の場合はrmse、分類の場合はエラー、ランキングの場合は平均平均精度）。詳細セクションにリストが提供されています。<br/>\n                data: トレーニングデータセット。xgb.trainはxgb.DMatrixのみを入力として受け入れます。xgboostは、さらに行列、dgCMatrix、またはローカルデータファイルの名前も受け入れます。<br/>\n                nrounds: 最大ブースティング反復回数。<br/>\n                verbose: 0の場合、xgboostはサイレントになります。1の場合、パフォーマンスに関する情報が印刷されます。2の場合、追加の情報が印刷されます。verbose > 0に設定すると、cb.print.evaluation(period=1)コールバック関数が自動的に有効になります。<br/>\n                print_every_n: verbose>0のとき、n回ごとに評価メッセージを印刷します。デフォルトは1で、すべてのメッセージが印刷されます。このパラメータはcb.print.evaluationコールバックに渡されます。<br/>\n                label: 応答値のベクトル。データがローカルデータファイル名またはxgb.DMatrixの場合は提供しないでください。<br/>\n                missing: デフォルトはNAに設定されており、NA値はアルゴリズムによって「欠損」と見なされるべきです。時には、0または他の極端な値が欠損値を表すために使用されることがあります。このパラメータは、入力が密な行列の場合にのみ使用されます。<br/>\n                </li>\n                </ul>\n                <b>詳細</b></br>\n                評価メトリックは、eval_metricパラメータが提供されていない場合、Xgboostによって自動的に選択されます。</br>\n                ユーザーは1つまたは複数のeval_metricパラメータを設定できます。カスタマイズされたメトリックを使用する場合、単一のメトリックのみを使用できます。以下は、Xgboostが最適化された実装を提供する組み込みメトリックのリストです:</br>\n                rmse 平均二乗誤差。http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss 負の対数尤度。http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss マルチクラスのlogloss。http://wiki.fast.ai/index.php/Log_Loss</br>\n                error 二項分類エラー率。これは（#誤ったケース） / （#すべてのケース）として計算されます。デフォルトでは、予測値の0.5の閾値を使用して負と正のインスタンスを定義します。</br>\n                異なる閾値（例：0.）は「error@0」として指定できます。</br>\n                merror マルチクラス分類エラー率。これは（#誤ったケース） / （#すべてのケース）として計算されます。</br>\n                auc 曲線の下の面積。http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curveランキング評価のため。</br>\n                aucpr PR曲線の下の面積。https://en.wikipedia.org/wiki/Precision_and_recallランキング評価のため。</br>\n                ndcg 正規化割引累積利得（ランキングタスク用）。http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                特定のパラメータが設定されると、自動的に作成されるコールバックは以下の通りです:</br>\n                cb.print.evaluationはverbose > 0のときに有効になります。print_every_nパラメータがそれに渡されます。</br>\n                cb.evaluation.logはwatchlistが存在する場合に有効になります。</br>\n                cb.early.stop: early_stopping_roundsが設定されているとき。</br>\n                cb.save.model: save_period > 0が設定されているとき。</br></br>\n                <b>値</b></br>\n                メモリ内のxgboostモデルへのハンドル（ポインタ）を持つxgb.Boosterクラスのオブジェクト。</br>\n                raw Rの生の型として保存されたxgboostモデルのキャッシュメモリダンプ。</br>\n                niter: ブースティング反復回数。</br>\n                evaluation_log: 評価履歴はデータテーブルとして保存され、最初の列は反復番号に対応し、残りは評価メトリックの値に対応します。これはcb.evaluation.logコールバックによって作成されます。</br>\n                call: 関数呼び出し。</br>\n                params: xgboostライブラリに渡されたパラメータ。cb.reset.parametersコールバックによって変更されたパラメータはキャプチャされません。</br>\n                callbacks 自動的に割り当てられたか、明示的に渡されたコールバック関数。</br>\n                best_iteration: 最良の評価メトリック値を持つ反復番号（早期停止がある場合のみ利用可能）。</br>\n                best_ntreelimit: 最良の反復に対応するntreelimit値で、predictメソッドでさらに使用できます（早期停止がある場合のみ利用可能）。</br>\n                best_score: 早期停止中の最良の評価メトリック値（早期停止がある場合のみ利用可能）。</br>\n                feature_names: トレーニングデータセットの特徴の名前（トレーニングデータで列名が定義されている場合のみ）。</br>\n                nfeatures: トレーニングデータの特徴の数。</br>\n                <b>参考文献</b></br>\n                Tianqi ChenとCarlos Guestrin、「XGBoost: A Scalable Tree Boosting System」、第22回SIGKDD会議、知識発見とデータマイニング、2016年、https://arxiv.org/abs/1603.02754</br>\n                <b>関連項目</b></br>\n                callbacks、predict.xgb.Booster、xgb.cv</br>\n                <b>例</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## シンプルなxgb.trainの例:</br>\n                # これらの関数は、次のように渡すことができます:</br>\n                ## 'xgboost'インターフェースの例:</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}