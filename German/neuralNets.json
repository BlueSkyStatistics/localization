{
  "title": "Training von Neuralnetzen mit dem neuralnet-Paket",
  "navigation": "Neuralnetze",
  "label1": "BITTE DUMMY-CODE-FEATURE-VERÄNDERLICHE, SIEHE DATEN > DUMMY-VERÄNDERLICHE BERECHNEN (ALLE LEVEL BEIBEHALTEN, A.K.A. 1-HOT-ENCODING). SKALIEREN UND ZENTRIEREN SIE NUMERISCHE VERÄNDERLICHE, SIEHE DATEN > VERÄNDERLICHE STANDARDISIEREN",
  "model": "Geben Sie einen Modellnamen ein",
  "dependentvar": "Abhängige Variable",
  "independentvars": "Unabhängige Variable(n)",
  "seed": "Setze Seed",
  "iter": "Maximale Schritte für das Lernen",
  "tf": "Algorithmus",
  "threshold": "Schwellenwert",
  "label2": "Anzahl der versteckten Schichten und der Neuronen pro versteckter Schicht",
  "layers": "Geben Sie die Anzahl der Neuronen in jeder Schicht an, z. B. 1. Für 5 Neuronen in 1 Schicht geben Sie 5 ein. Für 5 Neuronen in Schicht 1, 6 Neuronen in Schicht 2, 7 Neuronen in Schicht 3 geben Sie 5,6,7 ein.",
  "OutActFunc": "Geben Sie eine Ausgabefunktionsaktivierung an",
  "rep": "Wiederholungen für das Training des neuronalen Netzwerks",
  "label3": "Multiplikationsfaktoren für obere und untere Lernrate",
  "minus": "Oben (minus)",
  "upper": "Unten (plus)",
  "lifesign": "Einstellung, wie viel während der Berechnung des neuronalen Netzes ausgegeben werden soll",
  "lifesignstep": "Schrittgröße, um den minimalen Schwellenwert im vollständigen Lebenszeichenmodus auszugeben",
  "errfct": "Differenzierbare Funktion zur Berechnung des Fehlers",
  "linearoutput": "Aktivierungsfunktion sollte nicht auf die Ausgabeneuronen angewendet werden",
  "likelihood": "Wahrscheinlichkeit",
  "advanced_lbl" : "Erweitert",
  "help": {
    "title": "Training von Neuralnetzen mit dem neuralnet-Paket",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>HINWEIS</b></br>\n            Bei der Angabe einer einzelnen abhängigen Variable kann diese numerisch oder faktoriell sein. Wenn die angegebene abhängige Variable ein Faktor ist, codieren wir die Faktorvariable automatisch mit One-Hot-Encoding unter Verwendung der decode-Funktion im RSNNS-Paket.</br></br>\n            Darüber hinaus, wenn Sie One-Hot-Encoding verwenden, um eine Faktorvariable zu dummy-codieren, können Sie mehr als eine abhängige Variable im Dialog angeben. In diesem Fall müssen die abhängigen Variablen vom Typ numerisch sein.</br></br>\n            Sie können \"Daten > Dummy-Variablen berechnen\" verwenden, wählen Sie die Einstellung „Alle Levels beibehalten“, um One-Hot-Encoding zu erhalten.</br></br>\n            Für abhängige Variablen vom Typ Faktor zeigen wir eine Verwirrungsmatrix, ROC und Modellgenauigkeitsstatistiken an, wenn wir einen Datensatz mit dem erstellten Modell bewerten. Die generierten Vorhersagen sind vom Typ Faktor, da wir die Klasse vorhersagen. Diese werden im Datensatz zusammen mit den vorhergesagten Wahrscheinlichkeiten beim Scoring gespeichert.</br></br>\n            Wenn es dummy-codierte abhängige Variablen gibt, zeigen wir keine Verwirrungsmatrix, ROC und Modellgenauigkeitsstatistiken an, wenn wir einen Datensatz mit dem erstellten Modell bewerten. Die Vorhersagen werden jedoch im Datensatz gespeichert, wenn der Datensatz bewertet wird. Die Vorhersagen sind die Wahrscheinlichkeiten, die mit den dummy-codierten abhängigen Variablen verbunden sind.</br></br>\n            Es ist normalerweise am besten, unabhängige Variablen zu standardisieren (sie müssen ebenfalls numerisch sein). Siehe „Daten > Variablen standardisieren.“</br></br>\n            Wenn Sie kategoriale unabhängige Variablen haben, verwenden Sie One-Hot-Encoding, um die Faktorvariablen zu dummy-codieren.</br></br>\n            <b>Beschreibung</b></br>\n            Trainieren Sie neuronale Netzwerke mit Backpropagation, resilienter Backpropagation (RPROP) mit (Riedmiller, 1994) oder ohne Gewichtsrückverfolgung (Riedmiller und Braun, 1993) oder der modifizierten global konvergierenden Version (GRPROP) von Anastasiadis et al. (2005). Die Funktion ermöglicht flexible Einstellungen durch benutzerdefinierte Auswahl von Fehler- und Aktivierungsfunktionen. Darüber hinaus wird die Berechnung der verallgemeinerten Gewichte (Intrator O. und Intrator N., 1993) implementiert.\n            <b>Verwendung</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>Argumente</b><br/>\n            <ul>\n            <li>\n            formula: eine symbolische Beschreibung des Modells, das angepasst werden soll.\n            </li>\n            <li>\n            data: ein Datenrahmen, der die in der Formel angegebenen Variablen enthält.\n            </li>\n            <li>\n            hidden: ein Vektor von Ganzzahlen, der die Anzahl der versteckten Neuronen (Ecken) in jeder Schicht angibt.\n            </li>\n            <li>\n            threshold: ein numerischer Wert, der den Schwellenwert für die partiellen Ableitungen der Fehlerfunktion als Abbruchkriterium angibt.\n            </li>\n            <li>\n            stepmax: die maximalen Schritte für das Training des neuronalen Netzwerks. Das Erreichen dieses Maximums führt zu einem Stopp des Trainingsprozesses des neuronalen Netzwerks.\n            </li>\n            <li>\n            rep: die Anzahl der Wiederholungen für das Training des neuronalen Netzwerks.\n            </li>\n            <li>\n            startweights: ein Vektor mit Startwerten für die Gewichte. Auf NULL setzen für zufällige Initialisierung.\n            </li>\n            <li>\n            learningrate.limit: ein Vektor oder eine Liste, die die niedrigsten und höchsten Grenzen für die Lernrate enthält. Wird nur für RPROP und GRPROP verwendet.</li>\n            <li>\n            learningrate.factor: ein Vektor oder eine Liste, die die Multiplikationsfaktoren für die obere und untere Lernrate enthält. Wird nur für RPROP und GRPROP verwendet.\n            </li>\n            <li>\n            learningrate: ein numerischer Wert, der die Lernrate angibt, die von der traditionellen Backpropagation verwendet wird. Wird nur für die traditionelle Backpropagation verwendet.\n            </li>\n            <li>\n            lifesign: eine Zeichenfolge, die angibt, wie viel die Funktion während der Berechnung des neuronalen Netzwerks ausgeben wird. 'none', 'minimal' oder 'full'.\n            </li>\n            <li>\n            lifesign.step: eine Ganzzahl, die die Schrittgröße angibt, um den minimalen Schwellenwert im vollständigen Lebenszeichenmodus auszugeben.\n            </li>\n            <li>\n            algorithm: eine Zeichenfolge, die den Algorithmustyp zur Berechnung des neuronalen Netzwerks enthält. Die folgenden Typen sind möglich: 'backprop', 'rprop+', 'rprop-', 'sag' oder 'slr'. 'backprop' bezieht sich auf Backpropagation, 'rprop+' und 'rprop-' beziehen sich auf die resiliente Backpropagation mit und ohne Gewichtsrückverfolgung, während 'sag' und 'slr' die Verwendung des modifizierten global konvergierenden Algorithmus (grprop) induzieren. Siehe Details für weitere Informationen.\n            </li>\n            <li>\n            err.fct: eine differenzierbare Funktion, die zur Berechnung des Fehlers verwendet wird. Alternativ können die Zeichenfolgen 'sse' und 'ce' verwendet werden, die für die Summe der quadrierten Fehler und die Kreuzentropie stehen.\n            </li>\n            <li>\n            act.fct: eine differenzierbare Funktion, die zur Glättung des Ergebnisses des Kreuzprodukts der Kovariate oder Neuronen und der Gewichte verwendet wird. Darüber hinaus sind die Zeichenfolgen 'logistic' und 'tanh' für die logistische Funktion und den tangens hyperbolicus möglich.\n            </li>\n            <li>\n            linear.output: logisch. Wenn act.fct nicht auf die Ausgabeneuronen angewendet werden soll, setzen Sie linear output auf TRUE, andernfalls auf FALSE.\n            </li>\n            <li>\n            exclude: ein Vektor oder eine Matrix, die die Gewichte angibt, die von der Berechnung ausgeschlossen sind. Wenn als Vektor angegeben, müssen die genauen Positionen der Gewichte bekannt sein. Eine Matrix mit n-Zeilen und 3 Spalten schließt n Gewichte aus, wobei die erste Spalte für die Schicht, die zweite Spalte für das Eingangsneuron und die dritte Spalte für das Ausgangsneuron des Gewichts steht.\n            </li>\n            <li>\n            constant.weights: ein Vektor, der die Werte der Gewichte angibt, die vom Trainingsprozess ausgeschlossen und als fix behandelt werden.\n            </li>\n            <li>\n            likelihood: logisch. Wenn die Fehlerfunktion gleich der negativen Log-Likelihood-Funktion ist, werden die Informationskriterien AIC und BIC berechnet. Darüber hinaus ist die Verwendung von confidence.interval sinnvoll.\n            </li>\n            </ul>\n            <b>Details</b><br/>\n            Der global konvergierende Algorithmus basiert auf der resilienten Backpropagation ohne Gewichtsrückverfolgung und modifiziert zusätzlich eine Lernrate, entweder die Lernrate, die mit dem kleinsten absoluten Gradienten verbunden ist (sag) oder die kleinste Lernrate (slr) selbst. Die Lernraten im grprop-Algorithmus sind auf die in learningrate.limit definierten Grenzen beschränkt.\n            ​<b>Wert</b><br/>\n            neuralnet gibt ein Objekt der Klasse nn zurück. Ein Objekt der Klasse nn ist eine Liste, die höchstens die folgenden Komponenten enthält:<br/>\n            call: der übereinstimmende Aufruf.<br/>\n            response: extrahiert aus dem Datenargument.<br/>\n            covariate: die aus dem Datenargument extrahierten Variablen.<br/>\n            model.list: eine Liste, die die Kovariaten und die Antwortvariablen enthält, die aus dem Formelargument extrahiert wurden.<br/>\n            err.fct: die Fehlerfunktion.<br/>\n            act.fct: die Aktivierungsfunktion.<br/>\n            data: das Datenargument.<br/>\n            net.result: eine Liste, die das Gesamtergebnis des neuronalen Netzwerks für jede Wiederholung enthält.<br/>\n            weights: eine Liste, die die angepassten Gewichte des neuronalen Netzwerks für jede Wiederholung enthält.<br/>\n            generalized.weights: eine Liste, die die verallgemeinerten Gewichte des neuronalen Netzwerks für jede Wiederholung enthält.<br/>\n            result.matrix: eine Matrix, die den erreichten Schwellenwert, benötigte Schritte, Fehler, AIC und BIC (falls berechnet) und Gewichte für jede Wiederholung enthält. Jede Spalte stellt eine Wiederholung dar.<br/>\n            startweights: eine Liste, die die Startgewichte des neuronalen Netzwerks für jede Wiederholung enthält.<br/>\n            ​<b>Beispiele</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# Binäre Klassifikation\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Nicht ausgeführt: print(nn)\n            ## Nicht ausgeführt: plot(nn)\n            # Multiklass-Klassifikation\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Nicht ausgeführt: print(nn)\n            ## Nicht ausgeführt: plot(nn)\n            # Benutzerdefinierte Aktivierungsfunktion\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## Nicht ausgeführt: print(nn)\n            ## Nicht ausgeführt: plot(nn)\n            </code> <br/>\n            <b>Pakete</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>Hilfe</b></br>\n            Für detaillierte Hilfe klicken Sie auf das R-Symbol in der oberen rechten Ecke dieses Dialogüberlagerung oder führen Sie den folgenden Befehl im R-Syntax-Editor aus</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}