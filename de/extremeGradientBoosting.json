{
  "title": "Extremes Gradient Boosting",
  "navigation": "Extremes Gradient Boosting",
  "label1": "DUMMY CODE FAKTORVARIABLEN, SIEHE VARIABLEN>RECHNEN>DUMMY CODE VERWENDEN OPTION ALLE EBENEN BEIBEHALTEN. FÜR DIE VORHERSAGE MEHRERER KLASSEN, LESEN SIE HILFE (KLICKEN SIE AUF DAS ?-SYMBOL IM DIALOG).",
  "model": "Modellnamen eingeben",
  "dependentvar": "Abhängige Variable",
  "independentvars": "Unabhängige Variable(n)",
  "objective": "Ziel",
  "seed": "Setze Seed",
  "nrounds": "Maximale Anzahl der Boosting-Iterationen",
  "maxdepth": "Maximale Tiefe",
  "minchildweight": "Minimales Kindgewicht",
  "maxdeltastep": "Maximaler Delta-Schritt",
  "eta": "eta (Lernrate)",
  "gamma": "Gamma",
  "numclasses": "Anzahl der Klassen. **Nur verwenden mit multi:softmax und multi:softprob",
  "basescore": "Basiswert",
  "Verbose": "Ausführlicher Modus (0=Still, 1=Leistungsinfo, 2=andere Infos)",
  "printevery": "Drucke jede n-te Iteration Bewertungsnachricht, wenn ausführlich > 0",
  "help": {
    "title": "Extremes Gradient Boosting",
    "r_help": "hilfe(xgboost, paket ='xgboost')",
    "body": "\n                <b>Beschreibung</b></br>\n                Erstellen Sie ein eXtreme Gradient Boosting-Modell\n                <br/>\n                <b>HINWEIS</b></br>\n                1. Um die abhängige Variable vom Typ Faktor vorherzusagen, müssen Sie die abhängige Variable in eine numerische umkodieren, deren Werte bei 0 beginnen. Wenn es beispielsweise 3 Ebenen in der Faktorvariablen gibt, enthält die numerische Variable die Werte 0,1,2.</br>\n                Siehe Daten > Variablen umkodieren. Alternativ können Sie die Faktorvariable einfach in numerisch umwandeln, typischerweise werden die Ebenen auf Ganzzahlen ab 1 abgebildet und 1 von der resultierenden Variablen subtrahiert. Dies gibt Ihnen eine numerische Variable mit Werten, die bei 0 beginnen.</br>\n                Sie müssen auch das Ziel auf multi:softmax (vorhersagt Klassen) oder multi:softprob (vorhersagt Wahrscheinlichkeiten) setzen und die Anzahl der Klassen im Textfeld Anzahl der Klassen eingeben.</br>\n                Die Anzahl der Klassen muss nur für multi:softmax und multi:softprob eingegeben werden. Fehler werden generiert, wenn die Anzahl der Klassen für die anderen Ziele eingegeben wird.</br>\n                2. Sie müssen unabhängige Faktorvariablen dummy-codieren, verwenden Sie 1-Hot-Codierung siehe Daten > Dummy-Variablen berechnen</br>\n                3. Eine Verwirrungsmatrix und ROC-Kurve werden nicht generiert, wenn das ausgewählte Ziel eines von reg:squarederror, reg:logistic, binary:logitraw und rank:pairwise ist, da die Vorhersagefunktion \n                nicht die Klasse der abhängigen Variable zurückgibt. Die numerischen Vorhersagen (im Fall von reg:squarederror), Scores, Ränge usw., die die Vorhersagefunktion zurückgibt, werden im Datensatz gespeichert. Siehe hilfe(predict.xgb.Booster) für weitere Details</br>\n                4. Eine Verwirrungsmatrix wird nicht generiert, wenn das ausgewählte Ziel multi:softprob ist, da die Vorhersagefunktion die vorhergesagten Wahrscheinlichkeiten und nicht die Klasse der abhängigen Variable zurückgibt. Die vorhergesagten Wahrscheinlichkeiten werden im Datensatz gespeichert und die ROC-Kurve wird generiert. Um die Verwirrungsmatrix zu sehen, wählen Sie multi:softmax als Ziel.</br>\n                5. Eine ROC-Kurve wird nicht generiert, wenn das Ziel von multi:softmax ausgewählt ist, da die Vorhersagefunktion die Klasse und nicht die vorhergesagten Wahrscheinlichkeiten zurückgibt. Um die ROC-Kurve zu sehen, wählen Sie multi:softprob als Ziel.</br>\n                <br/>\n                <b>Verwendung</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Argumente</b><br/>\n                <ul>\n                <li>\n                params: die Liste der Parameter. Die vollständige Liste der Parameter ist verfügbar unter http://xgboost.readthedocs.io/en/latest/parameter.html. Nachfolgend eine kürzere Zusammenfassung:<br/>\n                </li>\n                <li>\n                1. Allgemeine Parameter<br/>\n                Dieser Dialog verwendet gbtree (Baum-Booster)<br/>\n                </li>\n                <li>\n                2. Booster-Parameter<br/>\n                2.1. Parameter für Baum-Booster<br/>\n                eta:  steuert die Lernrate: skaliert den Beitrag jedes Baumes um einen Faktor von 0 < eta < 1, wenn er zur aktuellen Annäherung hinzugefügt wird.<br/>\n                Wird verwendet, um Überanpassung zu verhindern, indem der Boosting-Prozess konservativer gestaltet wird.<br/>\n                Ein niedrigerer Wert für eta impliziert einen größeren Wert für nrounds: Ein niedriger eta-Wert bedeutet, dass das Modell robuster gegen Überanpassung, aber langsamer zu berechnen ist. Standard: 0.3<br/>\n                gamma: minimale Verlustreduktion, die erforderlich ist, um eine weitere Partition auf einem Blattknoten des Baumes vorzunehmen. Je größer, desto konservativer wird der Algorithmus sein.<br/>\n                max_depth: maximale Tiefe eines Baumes. Standard: 6<br/>\n                min_child_weight: minimale Summe des Instanzgewichts (Hessian), die in einem Kind benötigt wird. Wenn der Baumpartitionierungsschritt zu einem Blattknoten mit einer Summe des Instanzgewichts führt, die kleiner als min_child_weight ist, gibt der Bauprozess die weitere Partitionierung auf. Im linearen Regressionsmodus entspricht dies einfach der minimalen Anzahl von Instanzen, die in jedem Knoten vorhanden sein müssen. Je größer, desto konservativer wird der Algorithmus sein. Standard: 1<br/>\n                </li>\n                <li>\n                3. Aufgabenparameter<br/>\n                objective: gibt die Lernaufgabe und das entsprechende Lernziel an, Benutzer können eine selbstdefinierte Funktion übergeben. Die Standardzieloptionen sind unten:<br/>\n                reg:squarederror -Regression mit quadratischem Verlust (Standard).<br/>\n                reg:logistic logistische Regression.<br/>\n                binary:logistic -logistische Regression für binäre Klassifikation. Ausgabe-Wahrscheinlichkeit.<br/>\n                binary:logitraw -logistische Regression für binäre Klassifikation, Ausgabe-Score vor logistischer Transformation.<br/>\n                num_class: legt die Anzahl der Klassen fest. Nur für Multiklassenziele verwenden.<br/>\n                multi:softmax -setzt xgboost auf, um Multiklassenklassifikation mit dem Softmax-Ziel durchzuführen. Klasse wird durch eine Zahl dargestellt und sollte von 0 bis num_class - 1 sein.<br/>\n                multi:softprob -gleich wie softmax, aber die Vorhersage gibt einen Vektor von ndata * nclass-Elementen aus, der weiter in eine ndata, nclass-Matrix umgeformt werden kann. Das Ergebnis enthält die vorhergesagten Wahrscheinlichkeiten, dass jeder Datenpunkt zu jeder Klasse gehört.<br/>\n                rank:pairwise -setzt xgboost auf, um eine Ranking-Aufgabe durch Minimierung des paarweisen Verlusts durchzuführen.<br/>\n                base_score: der anfängliche Vorhersagewert aller Instanzen, globaler Bias. Standard: 0.5<br/>\n                eval_metric: Bewertungsmetriken für Validierungsdaten. Benutzer können eine selbstdefinierte Funktion übergeben. Standard: Metrik wird entsprechend dem Ziel zugewiesen (rmse für Regression,\n                und Fehler für Klassifikation, mittlere durchschnittliche Präzision für Ranking). Liste wird im Detailabschnitt bereitgestellt.<br/>\n                data: Trainingsdatensatz. xgb.train akzeptiert nur ein xgb.DMatrix als Eingabe. xgboost akzeptiert zusätzlich auch Matrix, dgCMatrix oder den Namen einer lokalen Datendatei.<br/>\n                nrounds: maximale Anzahl von Boosting-Iterationen.<br/>\n                verbose: Wenn 0, bleibt xgboost still. Wenn 1, werden Informationen zur Leistung ausgegeben. Wenn 2, werden einige zusätzliche Informationen ausgegeben. Beachten Sie, dass die Einstellung von verbose > 0 automatisch die cb.print.evaluation(period=1) Callback-Funktion aktiviert.<br/>\n                print_every_n: Drucke jede n-te Iteration Bewertungsnachricht, wenn ausführlich>0. Standard ist 1, was bedeutet, dass alle Nachrichten gedruckt werden. Dieses Parameter wird an den cb.print.evaluation Callback übergeben.<br/>\n                label: Vektor von Antwortwerten. Sollte nicht bereitgestellt werden, wenn die Daten ein lokaler Dateiname oder ein xgb.DMatrix ist.<br/>\n                missing: standardmäßig auf NA gesetzt, was bedeutet, dass NA-Werte vom Algorithmus als 'fehlend' betrachtet werden sollten. Manchmal kann 0 oder ein anderer extremer Wert verwendet werden, um fehlende Werte darzustellen. Dieses Parameter wird nur verwendet, wenn die Eingabe eine dichte Matrix ist.<br/>\n                </li>\n                </ul>\n                <b>Details</b></br>\n                Die Bewertungsmetrik wird automatisch von Xgboost ausgewählt (entsprechend dem Ziel), wenn das eval_metric-Parameter nicht bereitgestellt wird.</br>\n                Benutzer können ein oder mehrere eval_metric-Parameter festlegen. Beachten Sie, dass bei Verwendung einer benutzerdefinierten Metrik nur diese einzelne Metrik verwendet werden kann. Die folgende Liste enthält integrierte Metriken, für die Xgboost eine optimierte Implementierung bereitstellt:</br>\n                rmse Wurzel des mittleren quadratischen Fehlers. http://de.wikipedia.org/wiki/Wurzel_mittlerer_quadratischer_Fehlers</br>\n                logloss negativer Logarithmus der Wahrscheinlichkeit. http://de.wikipedia.org/wiki/Log-Likelihood</br>\n                mlogloss Multiklassen-Logarithmus der Wahrscheinlichkeit. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error Fehlerquote bei binärer Klassifikation. Sie wird berechnet als (# falsche Fälle) / (# alle Fälle). Standardmäßig wird der Schwellenwert von 0,5 für vorhergesagte Werte verwendet, um negative und positive Instanzen zu definieren.</br>\n                Ein anderer Schwellenwert (z.B. 0.) könnte als \"error@0.\" angegeben werden.</br>\n                merror Fehlerquote bei Multiklassenklassifikation. Sie wird berechnet als (# falsche Fälle) / (# alle Fälle).</br>\n                auc Fläche unter der Kurve. http://de.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve für Ranking-Bewertung.</br>\n                aucpr Fläche unter der PR-Kurve. https://de.wikipedia.org/wiki/Pr%C3%A4zision_und_R%C3%BCckruf für Ranking-Bewertung.</br>\n                ndcg Normalized Discounted Cumulative Gain (für Ranking-Aufgabe). http://de.wikipedia.org/wiki/NDCG</br>\n                </br>\n                Die folgenden Rückrufe werden automatisch erstellt, wenn bestimmte Parameter gesetzt sind:</br>\n                cb.print.evaluation wird aktiviert, wenn ausführlich > 0; und das print_every_n-Parameter wird an ihn übergeben.</br>\n                cb.evaluation.log ist aktiv, wenn die Beobachtungsliste vorhanden ist.</br>\n                cb.early.stop: wenn early_stopping_rounds gesetzt ist.</br>\n                cb.save.model: wenn save_period > 0 gesetzt ist.</br></br>\n                <b>Wert</b></br>\n                Ein Objekt der Klasse xgb.Booster mit den folgenden Elementen:</br>\n                handle ein Handle (Zeiger) auf das xgboost-Modell im Speicher.</br>\n                raw ein zwischengespeicherter Speicherabbild des xgboost-Modells, das als R's Rohtyp gespeichert ist.</br>\n                niter: Anzahl der Boosting-Iterationen.</br>\n                evaluation_log: Bewertungsverlauf, der als data.table gespeichert ist, wobei die erste Spalte der Iterationsnummer und die restlichen Spalten den Werten der Bewertungsmetriken entsprechen. Es wird durch den cb.evaluation.log Callback erstellt.</br>\n                call: ein Funktionsaufruf.</br>\n                params: Parameter, die an die xgboost-Bibliothek übergeben wurden. Beachten Sie, dass es keine Parameter erfasst, die durch den cb.reset.parameters Callback geändert wurden.</br>\n                callbacks Callback-Funktionen, die entweder automatisch zugewiesen oder explizit übergeben wurden.</br>\n                best_iteration: Iterationsnummer mit dem besten Bewertungsmetrikwert (nur verfügbar mit frühem Stoppen).</br>\n                best_ntreelimit: der ntreelimit-Wert, der der besten Iteration entspricht, der weiter in der Vorhersagemethode verwendet werden kann (nur verfügbar mit frühem Stoppen).</br>\n                best_score: der beste Bewertungsmetrikwert während des frühen Stoppens. (nur verfügbar mit frühem Stoppen).</br>\n                feature_names: Namen der Merkmale des Trainingsdatensatzes (nur wenn die Spaltennamen im Trainingsdatensatz definiert wurden).</br>\n                nfeatures: Anzahl der Merkmale im Trainingsdatensatz.</br>\n                <b>Referenzen</b></br>\n                Tianqi Chen und Carlos Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 22. SIGKDD-Konferenz über Wissensentdeckung und Datenbergbau, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>Siehe auch</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Beispiele</b></br>\n                data(agaricus.train, paket='xgboost')</br>\n                data(agaricus.test, paket='xgboost')</br>\n                ## Ein einfaches xgb.train-Beispiel:</br>\n                # Diese Funktionen könnten verwendet werden, indem sie entweder übergeben werden:</br>\n                ## Ein 'xgboost'-Schnittstellenbeispiel:</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}