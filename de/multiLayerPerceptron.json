{
  "title": "Mehrschichtige Perzeptron, unter Verwendung des RSNNS-Pakets",
  "navigation": "Mehrschichtige Perzeptron",
  "label1": "BITTE DUMMY-CODE-FACHTORVARIABLEN, SEHEN SIE DATEN > DUMMY-VARIABLEN BERECHNEN (ALLE EBENEN BEIBEHALTEN, A.K.A. 1-HOT-ENCODING). SKALIEREN UND ZENTRIEREN SIE NUMERISCHE VARIABLEN, SEHEN SIE DATEN > VARIABLEN STANDARDISIEREN",
  "model": "Geben Sie einen Modellnamen ein",
  "dependentvar": "Abhängige Variable",
  "independentvars": "Unabhängige Variable(n)",
  "seed": "Setzen Sie den Seed",
  "iter": "Maximale Iterationen zum Lernen",
  "tf": "Lernfunktion",
  "label2": "Anzahl der versteckten Schichten und der Neuronen pro versteckter Schicht",
  "layers": "Geben Sie die Anzahl der Neuronen in jeder Schicht an, z.B. 1. Für 5 Neuronen in 1 Schicht geben Sie 5 ein. Für 5 Neuronen in Schicht 1, 6 Neuronen in Schicht 2, 7 Neuronen in Schicht 3 geben Sie 5,6,7 ein.",
  "learnfuncparams": "Parameter der Lernfunktion",
  "help": {
    "title": "Mehrschichtige Perzeptron, unter Verwendung des RSNNS-Pakets",
    "r_help": "help(mlp, package ='RSNNS')",
    "body": "\n            <b>HINWEIS</b></br>\n            Wenn Sie eine einzelne abhängige Variable angeben, kann diese numerisch oder faktoriell sein. Wenn die angegebene abhängige Variable ein Faktor ist, codieren wir die Faktorvariable automatisch mit One-Hot-Encoding unter Verwendung der Decode-Funktion im RSNNS-Paket.</br></br>\n            Darüber hinaus, wenn Sie One-Hot-Encoding verwenden, um eine Faktorvariable zu dummy-codieren, können Sie mehr als eine abhängige Variable im Dialog angeben. In diesem Fall müssen die abhängigen Variablen vom Typ numerisch sein.</br></br>\n            Sie können \"Daten > Dummy-Variablen berechnen\" verwenden, wählen Sie die Einstellung „Alle Ebenen beibehalten“, um One-Hot-Encoding zu erhalten.</br></br>\n            Für abhängige Variablen vom Typ Faktor zeigen wir eine Verwirrungsmatrix, ROC und Modellgenauigkeitsstatistiken an, wenn wir einen Datensatz mit dem erstellten Modell bewerten. Die generierten Vorhersagen sind vom Typ Faktor, da wir die Klasse vorhersagen. Diese werden im Datensatz zusammen mit den vorhergesagten Wahrscheinlichkeiten beim Scoring gespeichert.</br></br>\n            Wenn es dummy-codierte abhängige Variablen gibt, zeigen wir keine Verwirrungsmatrix, ROC und Modellgenauigkeitsstatistiken an, wenn wir einen Datensatz mit dem erstellten Modell bewerten. Die Vorhersagen werden jedoch im Datensatz gespeichert, wenn der Datensatz bewertet wird. Die Vorhersagen sind die Wahrscheinlichkeiten, die mit den dummy-codierten abhängigen Variablen verbunden sind.</br></br>\n            Es ist normalerweise am besten, unabhängige Variablen zu standardisieren (sie müssen ebenfalls numerisch sein). Siehe „Daten > Variablen standardisieren.“</br></br>\n            Wenn Sie kategoriale unabhängige Variablen haben, verwenden Sie One-Hot-Encoding, um die Faktorvariablen zu dummy-codieren.</br></br>\n            <b>Beschreibung</b></br>\n            Diese Funktion erstellt ein mehrschichtiges Perzeptron (MLP) und trainiert es. MLPs sind vollständig verbundene Feedforward-Netzwerke und wahrscheinlich die gebräuchlichste Netzwerkarchitektur, die verwendet wird. Das Training erfolgt normalerweise durch Fehler-Rückpropagation oder ein verwandtes Verfahren.</br>\n            Es gibt viele verschiedene Lernfunktionen in SNNS, die zusammen mit dieser Funktion verwendet werden können, z.B. Std_Backpropagation, BackpropBatch, BackpropChunk, BackpropMomentum, BackpropWeightDecay, Rprop, Quickprop, SCG (skalierte konjugierte Gradient), ...</br>\n            <b>Verwendung</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## Standard S3-Methode:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>Argumente</b><br/>\n            <ul>\n            <li>\n            x: eine Matrix mit Trainingsinputs für das Netzwerk\n            </li>\n            <li>\n            ... : zusätzliche Funktionsparameter (derzeit nicht verwendet)\n            </li>\n            <li>\n            y: die entsprechenden Zielwerte\n            </li>\n            <li>\n            size: Anzahl der Einheiten in der versteckten Schicht(en)\n            </li>\n            <li>\n            maxit: maximale Anzahl an Iterationen zum Lernen\n            </li>\n            <li>\n            initFunc: die zu verwendende Initialisierungsfunktion\n            </li>\n            <li>\n            initFuncParams: die Parameter für die Initialisierungsfunktion\n            </li>\n            <li>\n            learnFunc: die zu verwendende Lernfunktion\n            </li>\n            <li>\n            learnFuncParams: die Parameter für die Lernfunktion\n            </li>\n            <li>\n            updateFunc: die zu verwendende Aktualisierungsfunktion\n            </li>\n            <li>\n            updateFuncParams: die Parameter für die Aktualisierungsfunktion\n            </li>\n            <li>\n            hiddenActFunc: die Aktivierungsfunktion aller versteckten Einheiten\n            </li>\n            <li>\n            shufflePatterns: sollen die Muster gemischt werden?\n            </li>\n            <li>\n            linOut: setzt die Aktivierungsfunktion der Ausgabeeinheiten auf linear oder logistischer (ignoriert, wenn outputActFunc angegeben ist)\n            </li>\n            <li>\n            outputActFunc: die Aktivierungsfunktion aller Ausgabeeinheiten\n            </li>\n            <li>\n            inputsTest: eine Matrix mit Eingaben zum Testen des Netzwerks\n            </li>\n            <li>\n            targetsTest: die entsprechenden Ziele für die Testeingabe\n            </li>\n            <li>\n            pruneFunc: die zu verwendende Beschneidungsfunktion\n            </li>\n            <li>\n            pruneFuncParams: die Parameter für die Beschneidungsfunktion. Im Gegensatz zu den anderen Funktionen müssen diese in einer benannten Liste angegeben werden. Siehe die Beschneidungsdemos für weitere Erklärungen.\n            </li>\n            </ul>\n            <b>Details</b></br>\n            Std_Backpropagation, BackpropBatch, z.B. haben zwei Parameter, die Lernrate und den maximalen Ausgabedifferenz. Die Lernrate ist normalerweise ein Wert zwischen 0,1 und 1. Sie gibt die Schrittweite des Gradientenabstiegs an. Der maximale Unterschied definiert, wie viel Unterschied zwischen Ausgabe und Zielwert als Nullfehler behandelt wird und nicht zurückpropagiert wird. Dieser Parameter wird verwendet, um Übertraining zu verhindern. Für eine vollständige Liste der Parameter aller Lernfunktionen siehe das SNNS-Benutzerhandbuch, S. 67.</br>\n            Die Standardwerte, die für Initialisierungs- und Aktualisierungsfunktionen festgelegt sind, müssen normalerweise nicht geändert werden.</br>\n            <b>Wert</b><br/>\n            ein rsnns-Objekt.\n            <b>Referenzen</b><br/>\n            Rosenblatt, F. (1958), 'Der Perzeptron: Ein probabilistisches Modell für Informationsspeicherung und -organisation im Gehirn', Psychologische Überprüfung 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Gruppe, P. R. (1986), Parallel verteilte Verarbeitung: Erkundungen in der Mikrostruktur der Kognition, Mit, Cambridge, MA usw.<br/>\n            Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator Benutzerhandbuch, Version 4.2', IPVR, Universität Stuttgart und WSI, Universität Tübingen.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), Simulation neuronaler Netze, Addison-Wesley. (auf Deutsch)<br/>\n            <b>Beispiele</b><br/>\n            <code> \n            data(iris)<br/>\n            #mischen Sie den Vektor<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #Verwirrungsmatrix mit 402040-Methode<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>Pakete</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>Hilfe</b></br>\n            Für detaillierte Hilfe klicken Sie auf das R-Symbol in der oberen rechten Ecke dieses Dialogüberlagerung oder führen Sie den folgenden Befehl im R-Syntax-Editor aus</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
  }
}