{
  "title": "Acuerdo Categórico",
  "navigation": "Acuerdo Categórico",
  "ratervars": "Variables de Evaluador:",
  "cilevel": "Nivel de Confianza:",
  "options": "Avanzado",
  "categLabels": "Especifique opcionalmente todas las calificaciones posibles separadas por , por ejemplo 1,2,3,4,5 o \"bajo\",\"medio\",\"alto\" ",
  "weightschkbox": "Mostrar pesos de categoría",
  "weights": "Seleccionar pesos de categoría",
  "N": "Tamaño de población finita (si lo hay)",
  "help": {
    "title": "Acuerdo Categórico",
    "r_help": "help(fleiss.kappa.raw, package=irrCAC)",
    "body": "\n<b>Descripción</b></br>\nCalcula el porcentaje de acuerdo, kappa de Conger/Cohen, kappa de Fleiss, AC1/AC2 de Gwet, alfa de Krippendorff y el coeficiente de Brennan-Prediger entre múltiples evaluadores (2, 3, +) cuando los datos de entrada representan las calificaciones en bruto reportadas para cada sujeto y cada evaluador.  Versiones no ponderadas y ponderadas de estas estadísticas están disponibles, con diferentes métodos de ponderación de categorías.</br>\nLos valores faltantes se incorporan en los cálculos de acuerdo por azar según Gwet (2014).</br>\nTambién proporcionamos la opción de mostrar los pesos de las categorías de calificación.</br>\nPara más información, consulte: </br></br>Gwet, K.L. (2014, ISBN:978-0970806284). “Manual de Fiabilidad entre Evaluadores,” 4ta Edición. Advanced Analytics, LLC\n</br></br>Klein, D. (2018) doi:https://doi.org/10.1177/1536867X1801800408. “Implementando un marco general para evaluar el acuerdo entre evaluadores en Stata,” The Stata Journal Volumen 18, Número 4, pp. 871-901.</br></br> \n<b>Uso</b>\n<br/>\n<br/>\n<ul>\n<li>\n<b>Variables de Evaluador:</b>  Variables correspondientes a cada evaluador donde cada columna representa un evaluador y cada fila un sujeto. Pueden ser variables numéricas, de factor, ordinales o de carácter.</br>\n</li>\n<li>\n<b>Nivel de Confianza:</b>  El nivel de confianza asociado con el intervalo de confianza. Su valor predeterminado es 0.95.</br>\n</li>\n<li>\n<b>Mostrar pesos de categoría:</b> muestra los pesos numéricos para las categorías de evaluador según los pesos seleccionados.  El peso seleccionado debe corresponder a la cercanía deseada entre las categorías.\n</li>\n<li>\n<b>Seleccionar pesos de categoría:</b> Un parámetro obligatorio que es una variable de cadena o una matriz. La cadena describe uno de los pesos predefinidos y debe tomar uno de los valores (\"no ponderado\",\"cuadrático\", \"ordinal\", \"lineal\", \"radical\", \"razón\", \"circular\", \"bipolar\"). </br>\nSi este parámetro es una matriz, entonces debe ser una matriz cuadrada qxq donde q es el número de categorías posibles donde un sujeto puede ser clasificado. Si algunas de las q categorías posibles no se utilizan, se recomienda encarecidamente especificar la lista completa de categorías posibles. De lo contrario, el programa puede no funcionar.</br>\nNOTA: Especificar una matriz NO es compatible en la sintaxis. Necesita pegar el código y editar para especificar una matriz.</br>\n</li>\n<li>\n<b>Tamaño de población finita (si lo hay):</b> Parámetro opcional que representa el tamaño de la población (si lo hay). Puede ser utilizado para realizar una corrección de población finita a la varianza. Su valor predeterminado es infinito.\n</li>\n<li>\n<b>Especifique opcionalmente todas las calificaciones posibles:</b> Lista opcional utilizada en el caso de que ningún evaluador use una categoría posible. Si no se especifica nada, se asume que las categorías observadas son el conjunto completo de categorías.\n</li>\n</ul>\n<b>Paquete R</b></br>\nirrCAC</br>\n<b>Ayuda</b></br>\nEste diálogo utiliza las funciones pa.coeff.raw, conger.kappa.raw, fleiss.kappa.raw, gwet.ac1.raw, krippen.alpha.raw y bp.coeff.raw.\nPara ayuda detallada, consulte la ayuda del paquete irrCAC en el menú de tres puntos > Ayuda > Ayuda del Paquete R.\n"
  }
}