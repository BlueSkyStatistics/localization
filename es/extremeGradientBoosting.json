{
  "title": "Aumento de Gradiente Extremo",
  "navigation": "Aumento de Gradiente Extremo",
  "label1": "VARIABLES DE FACTOR DE CÓDIGO DUMMY, VEA VARIABLES>CALCULAR>OPCIÓN DE USO DE CÓDIGO DUMMY MANTENER TODOS LOS NIVELES. PARA PREDICCIÓN DE MÚLTIPLES CLASES, LEA AYUDA (HAGA CLIC EN EL ICONO ? EN EL DIÁLOGO).",
  "model": "Ingrese el nombre del modelo",
  "dependentvar": "Variable dependiente",
  "independentvars": "Variable(s) independiente(s)",
  "objective": "Objetivo",
  "seed": "Establecer semilla",
  "nrounds": "Máximo número de iteraciones de aumento",
  "maxdepth": "Profundidad máxima",
  "minchildweight": "Peso mínimo del hijo",
  "maxdeltastep": "Paso delta máximo",
  "eta": "eta (tasa de aprendizaje)",
  "gamma": "gamma",
  "numclasses": "Número de clases. **Usar solo con multi:softmax y multi:softprob",
  "basescore": "Puntuación base",
  "Verbose": "Modo detallado (0=Silencioso, 1=info de rendimiento, 2= otra info)",
  "printevery": "Imprimir mensajes de evaluación de cada n-ésima iteración cuando verbose > 0",
  "help": {
    "title": "Aumento de Gradiente Extremo",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>Descripción</b></br>\n                Crear un modelo de Aumento de Gradiente Extremo\n                <br/>\n                <b>NOTA</b></br>\n                1. Para predecir la variable dependiente de tipo factor, necesita recodificar la variable dependiente a un numérico con valores que comienzan desde 0. Por ejemplo, si hay 3 niveles en la variable factor, la variable numérica contendrá valores 0,1,2.</br>\n                Vea Datos > Recodificar Variables. Alternativamente, simplemente convierta la variable factor a numérica, típicamente los niveles se mapearán a enteros comenzando desde 1 y reste 1 de la variable resultante. Esto le dará una variable numérica con valores que comienzan desde 0.</br>\n                También necesita establecer el objetivo en multi:softmax (predice clases) o multi:softprob (predice probabilidades) e ingresar el número de clases en el cuadro de texto del número de clases.</br>\n                El número de clases debe ingresarse solo para multi:softmax y multi:softprob. Se generarán errores si se ingresa el número de clases para los otros objetivos.</br>\n                2. Necesita codificar variables independientes de factor, use codificación 1-Hot vea Datos > Calcular Variables Dummy</br>\n                3. No se genera una matriz de confusión y curva ROC cuando el objetivo seleccionado es uno de reg:squarederror, reg:logistic, binary:logitraw y rank:pairwise ya que la función de predicción \n                no devuelve la clase de la variable dependiente. Las predicciones numéricas (en el caso de reg:squarederror), puntajes, rangos, etc. que devuelve la función de predicción se almacenan en el conjunto de datos. Vea help(predict.xgb.Booster) para más detalles</br>\n                4. No se genera una matriz de confusión cuando el objetivo seleccionado es multi:softprob ya que la función de predicción devuelve las probabilidades predichas y no la clase de la variable dependiente. Las probabilidades predichas se guardan en el conjunto de datos y se genera la curva ROC. Para ver la matriz de confusión, seleccione multi:softmax como el objetivo.</br>\n                5. No se genera una curva ROC cuando se selecciona el objetivo de multi:softmax ya que la función de predicción devuelve la clase y no las probabilidades predichas. Para ver la curva ROC seleccione multi:softprob como el objetivo.</br>\n                <br/>\n                <b>Uso</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Argumentos</b><br/>\n                <ul>\n                <li>\n                params: la lista de parámetros. La lista completa de parámetros está disponible en http://xgboost.readthedocs.io/en/latest/parameter.html. A continuación se presenta un resumen más corto:<br/>\n                </li>\n                <li>\n                1. Parámetros Generales<br/>\n                Este diálogo utiliza gbtree (impulsor de árbol)<br/>\n                </li>\n                <li>\n                2. Parámetros del Impulsor<br/>\n                2.1. Parámetro para el Impulsor de Árbol<br/>\n                eta: controla la tasa de aprendizaje: escala la contribución de cada árbol por un factor de 0 < eta < 1 cuando se agrega a la aproximación actual.<br/>\n                Se utiliza para prevenir el sobreajuste haciendo que el proceso de aumento sea más conservador.<br/>\n                Un valor más bajo para eta implica un valor más grande para nrounds: un valor bajo de eta significa que el modelo es más robusto al sobreajuste pero más lento de calcular. Predeterminado: 0.3<br/>\n                gamma: reducción mínima de pérdida requerida para hacer una partición adicional en un nodo hoja del árbol. cuanto más grande, más conservador será el algoritmo.<br/>\n                max_depth: profundidad máxima de un árbol. Predeterminado: 6<br/>\n                min_child_weight: suma mínima del peso de instancia (hessiano) necesaria en un hijo. Si el paso de partición del árbol resulta en un nodo hoja con la suma del peso de instancia menor que min_child_weight, entonces el proceso de construcción renunciará a más particiones. En modo de regresión lineal, esto simplemente corresponde al número mínimo de instancias necesarias en cada nodo. Cuanto más grande, más conservador será el algoritmo. Predeterminado: 1<br/>\n                </li>\n                <li>\n                3. Parámetros de Tarea<br/>\n                objective: especificar la tarea de aprendizaje y el objetivo de aprendizaje correspondiente, los usuarios pueden pasar una función definida por sí mismos a ella. Las opciones de objetivo predeterminadas son las siguientes:<br/>\n                reg:squarederror -Regresión con pérdida cuadrada (Predeterminado).<br/>\n                reg:logistic regresión logística.<br/>\n                binary:logistic -regresión logística para clasificación binaria. Probabilidad de salida.<br/>\n                binary:logitraw -regresión logística para clasificación binaria, salida de puntaje antes de la transformación logística.<br/>\n                num_class: establecer el número de clases. Para usar solo con objetivos multiclasificados.<br/>\n                multi:softmax -establecer xgboost para hacer clasificación multiclasificada utilizando el objetivo softmax. La clase está representada por un número y debe ser de 0 a num_class - 1.<br/>\n                multi:softprob -igual que softmax, pero la predicción produce un vector de elementos ndata * nclass, que se puede volver a dar forma a una matriz ndata, nclass. El resultado contiene las probabilidades predichas de cada punto de datos perteneciendo a cada clase.<br/>\n                rank:pairwise -establecer xgboost para hacer una tarea de clasificación minimizando la pérdida por pares.<br/>\n                base_score: la puntuación de predicción inicial de todas las instancias, sesgo global. Predeterminado: 0.5<br/>\n                eval_metric: métricas de evaluación para datos de validación. Los usuarios pueden pasar una función definida por sí mismos a ella. Predeterminado: la métrica se asignará de acuerdo con el objetivo (rmse para regresión,\n                y error para clasificación, precisión media para clasificación). La lista se proporciona en la sección de detalles.<br/>\n                data: conjunto de datos de entrenamiento. xgb.train acepta solo un xgb.DMatrix como entrada. xgboost, además, también acepta matriz, dgCMatrix, o nombre de un archivo de datos local.<br/>\n                nrounds: número máximo de iteraciones de aumento.<br/>\n                verbose: Si 0, xgboost permanecerá en silencio. Si 1, imprimirá información sobre el rendimiento. Si 2, se imprimirá información adicional. Tenga en cuenta que establecer verbose > 0 activa automáticamente la función de devolución de llamada cb.print.evaluation(period=1).<br/>\n                print_every_n: Imprimir mensajes de evaluación de cada n-ésima iteración cuando verbose>0. El valor predeterminado es 1, lo que significa que se imprimen todos los mensajes. Este parámetro se pasa a la devolución de llamada cb.print.evaluation.<br/>\n                label: vector de valores de respuesta. No debe proporcionarse cuando los datos son un nombre de archivo de datos local o un xgb.DMatrix.<br/>\n                missing: por defecto se establece en NA, lo que significa que los valores NA deben considerarse como 'faltantes' por el algoritmo. A veces, 0 u otro valor extremo pueden usarse para representar valores faltantes. Este parámetro solo se utiliza cuando la entrada es una matriz densa.<br/>\n                </li>\n                </ul>\n                <b>Detalles</b></br>\n                La métrica de evaluación se elige automáticamente por Xgboost (de acuerdo con el objetivo) cuando el parámetro eval_metric no se proporciona.</br>\n                El usuario puede establecer uno o varios parámetros eval_metric. Tenga en cuenta que al usar una métrica personalizada, solo se puede usar esta única métrica. La siguiente es la lista de métricas integradas para las cuales Xgboost proporciona una implementación optimizada:</br>\n                rmse error cuadrático medio. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss logaritmo negativo de la verosimilitud. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss logloss multiclasificado. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error Tasa de error de clasificación binaria. Se calcula como (# casos incorrectos) / (# todos los casos). Por defecto, utiliza el umbral de 0.5 para los valores predichos para definir instancias negativas y positivas.</br>\n                Un umbral diferente (por ejemplo, 0.) podría especificarse como \"error@0.\"</br>\n                merror Tasa de error de clasificación multiclasificada. Se calcula como (# casos incorrectos) / (# todos los casos).</br>\n                auc Área bajo la curva. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve para evaluación de clasificación.</br>\n                aucpr Área bajo la curva PR. https://en.wikipedia.org/wiki/Precision_and_recall para evaluación de clasificación.</br>\n                ndcg Ganancia Cumulativa Descuento Normalizada (para tarea de clasificación). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                Las siguientes devoluciones de llamada se crean automáticamente cuando se establecen ciertos parámetros:</br>\n                cb.print.evaluation se activa cuando verbose > 0; y se pasa el parámetro print_every_n a ella.</br>\n                cb.evaluation.log está activado cuando la lista de vigilancia está presente.</br>\n                cb.early.stop: cuando se establece early_stopping_rounds.</br>\n                cb.save.model: cuando se establece save_period > 0.</br></br>\n                <b>Valor</b></br>\n                Un objeto de clase xgb.Booster con los siguientes elementos:</br>\n                handle un manejador (puntero) al modelo xgboost en memoria.</br>\n                raw un volcado de memoria en caché del modelo xgboost guardado como tipo raw de R.</br>\n                niter: número de iteraciones de aumento.</br>\n                evaluation_log: historial de evaluación almacenado como un data.table con la primera columna correspondiente al número de iteración y el resto correspondiente a los valores de las métricas de evaluación. Se crea mediante la devolución de llamada cb.evaluation.log.</br>\n                call: una llamada de función.</br>\n                params: parámetros que se pasaron a la biblioteca xgboost. Tenga en cuenta que no captura parámetros cambiados por la devolución de llamada cb.reset.parameters.</br>\n                callbacks funciones de devolución de llamada que fueron asignadas automáticamente o pasadas explícitamente.</br>\n                best_iteration: número de iteración con el mejor valor de métrica de evaluación (solo disponible con detención temprana).</br>\n                best_ntreelimit: el valor de ntreelimit correspondiente a la mejor iteración, que podría usarse posteriormente en el método de predicción (solo disponible con detención temprana).</br>\n                best_score: el mejor valor de métrica de evaluación durante la detención temprana. (solo disponible con detención temprana).</br>\n                feature_names: nombres de las características del conjunto de datos de entrenamiento (solo cuando se definieron nombres de columna en los datos de entrenamiento).</br>\n                nfeatures: número de características en los datos de entrenamiento.</br>\n                <b>Referencias</b></br>\n                Tianqi Chen y Carlos Guestrin, \"XGBoost: Un Sistema de Aumento de Árboles Escalable\", 22ª Conferencia SIGKDD sobre Descubrimiento de Conocimiento y Minería de Datos, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>Ver También</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Ejemplos</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## Un ejemplo simple de xgb.train:</br>\n                # Estas funciones podrían usarse pasando:</br>\n                ## Un ejemplo de interfaz 'xgboost':</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}