{
  "title": "Perceptrone a più strati, utilizzando il pacchetto RSNNS",
  "navigation": "Perceptrone a più strati",
  "label1": "PER FAVORE CODIFICARE LE VARIABILI FATTORIALI, VEDI DATI > CALCOLA VARIABILI DUMMY (MANTIENI TUTTI I LIVELLI A.K.A CODIFICA ONE HOT). SCALA E CENTRA LE VARIABILI NUMERICHE, VEDI DATI > STANDARDIZZA VARIABILI",
  "model": "Inserisci un nome per il modello",
  "dependentvar": "Variabile dipendente",
  "independentvars": "Variabile(i) indipendente(i)",
  "seed": "Imposta il seme",
  "iter": "Iterazioni massime per apprendere",
  "tf": "Funzione di apprendimento",
  "label2": "Numero di strati nascosti e neuroni per strato nascosto",
  "layers": "Specifica il numero di neuroni in ciascun strato, ad esempio 1. Per 5 neuroni in 1 strato, inserisci 5. Per 5 neuroni nello strato 1, 6 neuroni nello strato 2, 7 neuroni nello strato 3 inserisci 5,6,7",
  "learnfuncparams": "Parametri della funzione di apprendimento",
  "help": {
    "title": "Perceptrone a più strati, utilizzando il pacchetto RSNNS",
    "r_help": "help(mlp, package ='RSNNS')",
    "body": "\n            <b>NOTA</b></br>\n            Quando specifichi una singola variabile dipendente, può essere numerica o fattoriale. Se la variabile dipendente specificata è un fattore, codifichiamo automaticamente la variabile fattoriale utilizzando la codifica one-hot tramite la funzione decode nel pacchetto RSNNS.</br></br>\n            Inoltre, se stai utilizzando la codifica one-hot per codificare una variabile fattoriale, puoi specificare più di una variabile dipendente nella finestra di dialogo. In questo caso, le variabili dipendenti devono essere di tipo numerico.</br></br>\n            Puoi utilizzare \"Dati > Calcola variabili dummy\", scegli l'impostazione \"Mantieni tutti i livelli\" per ottenere la codifica one-hot.</br></br>\n            Per le variabili dipendenti di tipo fattore, mostreremo una matrice di confusione, ROC e statistiche di accuratezza del modello quando si valuta un dataset utilizzando il modello costruito. Le previsioni generate sono di tipo fattore poiché prevediamo la classe. Queste saranno salvate nel dataset insieme alle probabilità previste durante la valutazione.</br></br>\n            Quando ci sono variabili dipendenti codificate dummy, non mostreremo una matrice di confusione, ROC e statistiche di accuratezza del modello quando si valuta un dataset utilizzando il modello costruito. Tuttavia, le previsioni saranno salvate nel dataset durante la valutazione del dataset. Le previsioni sono le probabilità associate alle variabili dipendenti codificate dummy.</br></br>\n            È solitamente meglio standardizzare le variabili indipendenti (devono essere numeriche, anche loro) Vedi \"Dati > Standardizza Variabili.\"</br></br>\n            Se hai variabili indipendenti categoriali, utilizza la codifica one-hot per codificare le variabili fattoriali.</br></br>\n            <b>Descrizione</b></br>\n            Questa funzione crea un perceptrone a più strati (MLP) e lo addestra. Gli MLP sono reti completamente connesse feedforward, e probabilmente l'architettura di rete più comune in uso. L'addestramento viene solitamente eseguito tramite retropropagazione dell'errore o una procedura correlata.</br>\n            Ci sono molte diverse funzioni di apprendimento presenti in SNNS che possono essere utilizzate insieme a questa funzione, ad esempio, Std_Backpropagation, BackpropBatch, BackpropChunk, BackpropMomentum, BackpropWeightDecay, Rprop, Quickprop, SCG (gradiente coniugato scalato), ...</br>\n            <b>Utilizzo</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## Metodo S3 predefinito:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>Argomenti</b><br/>\n            <ul>\n            <li>\n            x: una matrice con gli input di addestramento per la rete\n            </li>\n            <li>\n            ... : parametri aggiuntivi della funzione (attualmente non utilizzati)\n            </li>\n            <li>\n            y: i valori corrispondenti ai target\n            </li>\n            <li>\n            size: numero di unità negli strati nascosti\n            </li>\n            <li>\n            maxit: massimo numero di iterazioni per apprendere\n            </li>\n            <li>\n            initFunc: la funzione di inizializzazione da utilizzare\n            </li>\n            <li>\n            initFuncParams: i parametri per la funzione di inizializzazione\n            </li>\n            <li>\n            learnFunc: la funzione di apprendimento da utilizzare\n            </li>\n            <li>\n            learnFuncParams: i parametri per la funzione di apprendimento\n            </li>\n            <li>\n            updateFunc: la funzione di aggiornamento da utilizzare\n            </li>\n            <li>\n            updateFuncParams: i parametri per la funzione di aggiornamento\n            </li>\n            <li>\n            hiddenActFunc: la funzione di attivazione di tutte le unità nascoste\n            </li>\n            <li>\n            shufflePatterns: i modelli devono essere mescolati?\n            </li>\n            <li>\n            linOut: imposta la funzione di attivazione delle unità di output su lineare o logistica (ignorato se outputActFunc è fornito)\n            </li>\n            <li>\n            outputActFunc: la funzione di attivazione di tutte le unità di output\n            </li>\n            <li>\n            inputsTest: una matrice con gli input per testare la rete\n            </li>\n            <li>\n            targetsTest: i target corrispondenti per l'input di test\n            </li>\n            <li>\n            pruneFunc: la funzione di potatura da utilizzare\n            </li>\n            <li>\n            pruneFuncParams: i parametri per la funzione di potatura. A differenza delle altre funzioni, questi devono essere forniti in una lista nominata. Vedi le dimostrazioni di potatura per ulteriori spiegazioni.\n            </li>\n            </ul>\n            <b>Dettagli</b></br>\n            Std_Backpropagation, BackpropBatch, ad esempio, hanno due parametri, il tasso di apprendimento e la massima differenza di output. Il tasso di apprendimento è solitamente un valore compreso tra 0.1 e 1. Specifica la larghezza del passo del gradiente discendente. La massima differenza definisce quanto la differenza tra il valore di output e il valore target viene trattata come errore zero, e non retropropagata. Questo parametro viene utilizzato per prevenire l'overtraining. Per un elenco completo dei parametri di tutte le funzioni di apprendimento, vedere il Manuale dell'Utente SNNS, pp. 67.</br>\n            I valori predefiniti impostati per le funzioni di inizializzazione e aggiornamento di solito non devono essere cambiati.</br>\n            <b>Valore</b><br/>\n            un oggetto rsnns.\n            <b>Riferimenti</b><br/>\n            Rosenblatt, F. (1958), 'Il perceptrone: un modello probabilistico per la memorizzazione e l'organizzazione delle informazioni nel cervello', Psychological Review 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Group, P. R. (1986), Parallel distributed processing : esplorazioni nella microstruttura della cognizione, Mit, Cambridge, MA ecc.<br/>\n            Zell, A. et al. (1998), 'Manuale dell'Utente del Simulatori di Reti Neurali SNNS, Versione 4.2', IPVR, Università di Stoccarda e WSI, Università di Tubinga.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), Simulazione di Reti Neurali, Addison-Wesley. (in tedesco)<br/>\n            <b>Esempi</b><br/>\n            <code> \n            data(iris)<br/>\n            #mescola il vettore<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #matrice di confusione con metodo 402040<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>Pacchetto</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>Aiuto</b></br>\n            Per aiuto dettagliato clicca sull'icona R in alto a destra di questo overlay di dialogo o esegui il seguente comando nell'editor di sintassi R</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
  }
}