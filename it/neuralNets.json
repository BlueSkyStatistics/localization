{
  "title": "Addestramento delle reti neurali, utilizzando il pacchetto neuralnet",
  "navigation": "Reti neurali",
  "label1": "PER FAVORE CODIFICA LE VARIABILI FATTORIALI, VEDI DATI > CALCOLA VARIABILI DUMMY (MANTIENI TUTTI I LIVELLI A.K.A CODIFICA ONE HOT). SCALA E CENTRA LE VARIABILI NUMERICHE, VEDI DATI > STANDARDIZZA VARIABILI",
  "model": "Inserisci un nome per il modello",
  "dependentvar": "Variabile dipendente",
  "independentvars": "Variabile(i) indipendente(i)",
  "seed": "Imposta il seme",
  "iter": "Passi massimi per l'apprendimento",
  "tf": "Algoritmo",
  "threshold": "Soglia",
  "label2": "Numero di strati nascosti e neuroni per strato nascosto",
  "layers": "Specifica il numero di neuroni in ciascun strato, ad esempio 1. Per 5 neuroni in 1 strato, inserisci 5. Per 5 neuroni nello strato 1, 6 neuroni nello strato 2, 7 neuroni nello strato 3 inserisci 5,6,7",
  "OutActFunc": "Specifica una funzione di attivazione in uscita",
  "rep": "Ripetizioni per l'addestramento della rete neurale",
  "label3": "Fattori di moltiplicazione per il tasso di apprendimento superiore e inferiore",
  "minus": "Superiore (meno)",
  "upper": "Inferiore (più)",
  "lifesign": "Impostazione di quanto stampare durante il calcolo della rete neurale",
  "lifesignstep": "Dimensione del passo per stampare la soglia minima in modalità lifesign completa",
  "errfct": "Funzione differenziabile utilizzata per il calcolo dell'errore",
  "linearoutput": "La funzione di attivazione non dovrebbe essere applicata ai neuroni di uscita",
  "likelihood": "Probabilità",
  "advanced_lbl": "Avanzato",
  "help": {
    "title": "Addestramento delle reti neurali, utilizzando il pacchetto neuralnet",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>NOTA</b></br>\n            Quando si specifica una singola variabile dipendente, può essere numerica o fattoriale. Se la variabile dipendente specificata è un fattore, codifichiamo automaticamente la variabile fattoriale utilizzando la codifica one-hot tramite la funzione decode nel pacchetto RSNNS.</br></br>\n            Inoltre, se stai utilizzando la codifica one-hot per codificare una variabile fattoriale, puoi specificare più di una variabile dipendente nella finestra di dialogo. In questo caso, le variabili dipendenti devono essere di tipo numerico.</br></br>\n            Puoi utilizzare \"Dati > Calcola variabili dummy\", scegliere l'impostazione \"Mantieni tutti i livelli\" per ottenere la codifica one-hot.</br></br>\n            Per le variabili dipendenti di tipo fattore, mostreremo una matrice di confusione, ROC e statistiche di accuratezza del modello quando si valuta un dataset utilizzando il modello costruito. Le previsioni generate sono di tipo fattore poiché prevediamo la classe. Queste saranno salvate nel dataset insieme alle probabilità previste durante la valutazione.</br></br>\n            Quando ci sono variabili dipendenti codificate dummy, non mostreremo una matrice di confusione, ROC e statistiche di accuratezza del modello quando si valuta un dataset utilizzando il modello costruito. Tuttavia, le previsioni saranno salvate nel dataset durante la valutazione del dataset. Le previsioni sono le probabilità associate alle variabili dipendenti codificate dummy.</br></br>\n            È solitamente meglio standardizzare le variabili indipendenti (devono essere numeriche, anche loro) Vedi \"Dati > Standardizza Variabili.\"</br></br>\n            Se hai variabili indipendenti categoriali, utilizza la codifica one-hot per codificare le variabili fattoriali.</br></br>\n            <b>Descrizione</b></br>\n            Addestra reti neurali utilizzando il backpropagation, il backpropagation resiliente (RPROP) con (Riedmiller, 1994) o senza backtracking del peso (Riedmiller e Braun, 1993) o la versione modificata globalmente convergente (GRPROP) di Anastasiadis et al. (2005). La funzione consente impostazioni flessibili attraverso la scelta personalizzata della funzione di errore e di attivazione. Inoltre, il calcolo dei pesi generalizzati (Intrator O. e Intrator N., 1993) è implementato.\n            <b>Utilizzo</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>Argomenti</b><br/>\n            <ul>\n            <li>\n            formula: una descrizione simbolica del modello da adattare.\n            </li>\n            <li>\n            data: un data frame contenente le variabili specificate nella formula.\n            </li>\n            <li>\n            hidden: un vettore di interi che specifica il numero di neuroni nascosti (vertici) in ciascun strato.\n            </li>\n            <li>\n            threshold: un valore numerico che specifica la soglia per le derivate parziali della funzione di errore come criterio di arresto.\n            </li>\n            <li>\n            stepmax: i passi massimi per l'addestramento della rete neurale. Raggiungere questo massimo porta a un arresto del processo di addestramento della rete neurale.\n            </li>\n            <li>\n            rep: il numero di ripetizioni per l'addestramento della rete neurale.\n            </li>\n            <li>\n            startweights: un vettore contenente valori iniziali per i pesi. Imposta su NULL per l'inizializzazione casuale.\n            </li>\n            <li>\n            learningrate.limit: un vettore o un elenco contenente il limite più basso e più alto per il tasso di apprendimento. Utilizzato solo per RPROP e GRPROP.</li>\n            <li>\n            learningrate.factor: un vettore o un elenco contenente i fattori di moltiplicazione per il tasso di apprendimento superiore e inferiore. Utilizzato solo per RPROP e GRPROP.\n            </li>\n            <li>\n            learningrate: un valore numerico che specifica il tasso di apprendimento utilizzato dal backpropagation tradizionale. Utilizzato solo per il backpropagation tradizionale.\n            </li>\n            <li>\n            lifesign: una stringa che specifica quanto stamperà la funzione durante il calcolo della rete neurale. 'none', 'minimal' o 'full'.\n            </li>\n            <li>\n            lifesign.step: un intero che specifica la dimensione del passo per stampare la soglia minima in modalità lifesign completa.\n            </li>\n            <li>\n            algorithm: una stringa contenente il tipo di algoritmo per calcolare la rete neurale. I seguenti tipi sono possibili: 'backprop', 'rprop+', 'rprop-', 'sag' o 'slr'. 'backprop' si riferisce al backpropagation, 'rprop+' e 'rprop-' si riferiscono al backpropagation resiliente con e senza backtracking del peso, mentre 'sag' e 'slr' inducono l'uso dell'algoritmo modificato globalmente convergente (grprop). Vedi Dettagli per ulteriori informazioni.\n            </li>\n            <li>\n            err.fct: una funzione differenziabile utilizzata per il calcolo dell'errore. In alternativa, le stringhe 'sse' e 'ce' che stanno per la somma degli errori quadratici e l'entropia incrociata possono essere utilizzate.\n            </li>\n            <li>\n            act.fct: una funzione differenziabile utilizzata per smussare il risultato del prodotto incrociato delle covariate o dei neuroni e dei pesi. Inoltre, le stringhe 'logistic' e 'tanh' sono possibili per la funzione logistica e l'iperbolico tangente.\n            </li>\n            <li>\n            linear.output: logico. Se act.fct non dovrebbe essere applicata ai neuroni di uscita, imposta l'uscita lineare su TRUE, altrimenti su FALSE.\n            </li>\n            <li>\n            exclude: un vettore o una matrice che specifica i pesi, che sono esclusi dal calcolo. Se fornito come vettore, devono essere noti i posizioni esatte dei pesi. Una matrice con n-righe e 3 colonne escluderà n pesi, dove la prima colonna rappresenta lo strato, la seconda colonna il neurone di input e la terza colonna il neurone di output del peso.\n            </li>\n            <li>\n            constant.weights: un vettore che specifica i valori dei pesi che sono esclusi dal processo di addestramento e trattati come fissi.\n            </li>\n            <li>\n            likelihood: logico. Se la funzione di errore è uguale alla funzione di log-verosimiglianza negativa, verranno calcolati i criteri informativi AIC e BIC. Inoltre, l'uso di confidence.interval è significativo.\n            </li>\n            </ul>\n            <b>Dettagli</b><br/>\n            L'algoritmo globalmente convergente si basa sul backpropagation resiliente senza backtracking del peso e modifica inoltre un tasso di apprendimento, sia il tasso di apprendimento associato al gradiente assoluto più piccolo (sag) o il tasso di apprendimento più piccolo (slr) stesso. I tassi di apprendimento nell'algoritmo grprop sono limitati ai confini definiti in learningrate.limit.\n            ​<b>Valore</b><br/>\n            neuralnet restituisce un oggetto di classe nn. Un oggetto di classe nn è un elenco contenente al massimo i seguenti componenti:<br/>\n            call: la chiamata corrispondente.<br/>\n            response: estratto dall'argomento dati.<br/>\n            covariate: le variabili estratte dall'argomento dati.<br/>\n            model.list: un elenco contenente le covariate e le variabili di risposta estratte dall'argomento formula.<br/>\n            err.fct: la funzione di errore.<br/>\n            act.fct: la funzione di attivazione.<br/>\n            data: l'argomento dati.<br/>\n            net.result: un elenco contenente il risultato complessivo della rete neurale per ogni ripetizione.<br/>\n            weights: un elenco contenente i pesi adattati della rete neurale per ogni ripetizione.<br/>\n            generalized.weights: un elenco contenente i pesi generalizzati della rete neurale per ogni ripetizione.<br/>\n            result.matrix: una matrice contenente la soglia raggiunta, i passi necessari, l'errore, AIC e BIC (se calcolati) e i pesi per ogni ripetizione. Ogni colonna rappresenta una ripetizione.<br/>\n            startweights: un elenco contenente i pesi iniziali della rete neurale per ogni ripetizione.<br/>\n            ​<b>Esempi</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# Classificazione binaria\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Non eseguito: print(nn)\n            ## Non eseguito: plot(nn)\n            # Classificazione multiclass\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Non eseguito: print(nn)\n            ## Non eseguito: plot(nn)\n            # Funzione di attivazione personalizzata\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## Non eseguito: print(nn)\n            ## Non eseguito: plot(nn)\n            </code> <br/>\n            <b>Pacchetto</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>Aiuto</b></br>\n            Per aiuto dettagliato clicca sull'icona R in alto a destra di questo overlay della finestra di dialogo o esegui il seguente comando nell'editor di sintassi R</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}