{
  "title": "Estremo Gradient Boosting",
  "navigation": "Estremo Gradient Boosting",
  "label1": "VARIABILI FATTORIALI CODIFICATE DUMMY, VEDI VARIABILI>CALCOLA>OPZIONE DI UTILIZZO CODIFICATO DUMMY MANTIENI TUTTI I LIVELLI. PER PREDIRE CLASSI MULTIPLE, LEGGI AIUTO (FAI CLIC SULL'ICONA ? NEL DIALOGO).",
  "model": "Inserisci il nome del modello",
  "dependentvar": "Variabile dipendente",
  "independentvars": "Variabile(i) indipendente(i)",
  "objective": "Obiettivo",
  "seed": "Imposta il seme",
  "nrounds": "Numero massimo di iterazioni di boosting",
  "maxdepth": "Profondità massima",
  "minchildweight": "Peso minimo del figlio",
  "maxdeltastep": "Passo massimo delta",
  "eta": "eta (tasso di apprendimento)",
  "gamma": "gamma",
  "numclasses": "Numero di classi. **Usa solo con multi:softmax e multi:softprob",
  "basescore": "Punteggio di base",
  "Verbose": "Modalità verbosa (0=Silenzioso, 1=info sulle prestazioni, 2= altre info)",
  "printevery": "Stampa i messaggi di valutazione di ogni n-esima iterazione quando verbose > 0",
  "OptvarTaskparam": "Parametri di attività",
  "OptvarAdvDiagnostics": "Diagnostica avanzata",
  "OptvarTreeBoostparam": "Parametri per il Tree Boosting",
  "help": {
    "title": "Estremo Gradient Boosting",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>Descrizione</b></br>\n                Crea un modello di eXtreme Gradient Boosting\n                <br/>\n                <b>NOTA</b></br>\n                1. Per predire la variabile dipendente di tipo fattore, è necessario ricodificare la variabile dipendente in un numerico con valori che partono da 0. Ad esempio, se ci sono 3 livelli nella variabile fattoriale, la variabile numerica conterrà valori 0,1,2.</br>\n                Vedi Dati > Ricodifica Variabili. In alternativa, basta convertire la variabile fattoriale in numerica, tipicamente i livelli verranno mappati a interi a partire da 1 e sottrarre 1 dalla variabile risultante. Questo ti darà una variabile numerica con valori che partono da 0.</br>\n                Devi anche impostare l'obiettivo su multi:softmax (predice classi) o multi:softprob (predice probabilità) e inserire il numero di classi nella casella di testo del numero di classi.</br>\n                Il numero di classi deve essere inserito solo per multi:softmax e multi:softprob. Verranno generati errori se il numero di classi viene inserito per gli altri obiettivi.</br>\n                2. Devi codificare dummy le variabili indipendenti fattoriali, utilizzare la codifica 1-Hot vedi Dati > Calcola Variabili Dummy</br>\n                3. Una matrice di confusione e una curva ROC non vengono generate quando l'obiettivo selezionato è uno di reg:squarederror, reg:logistic, binary:logitraw e rank:pairwise poiché la funzione di previsione \n                non restituisce la classe della variabile dipendente. Le previsioni numeriche (nel caso di reg:squarederror), punteggi, classifiche ecc. che la funzione di previsione restituisce sono memorizzate nel dataset. Vedi help(predict.xgb.Booster) per ulteriori dettagli</br>\n                4. Una matrice di confusione non viene generata quando l'obiettivo selezionato è multi:softprob poiché la funzione di previsione restituisce le probabilità previste e non la classe della variabile dipendente. Le probabilità previste sono salvate nel dataset e la curva ROC viene generata. Per vedere la matrice di confusione, seleziona multi:softmax come obiettivo.</br>\n                5. Una curva ROC non viene generata quando l'obiettivo di multi:softmax è selezionato poiché la funzione di previsione restituisce la classe e non le probabilità previste. Per vedere la curva ROC seleziona multi:softprob come obiettivo.</br>\n                <br/>\n                <b>Utilizzo</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Argomenti</b><br/>\n                <ul>\n                <li>\n                params: la lista dei parametri. L'elenco completo dei parametri è disponibile su http://xgboost.readthedocs.io/en/latest/parameter.html. Di seguito è riportato un riepilogo più breve:<br/>\n                </li>\n                <li>\n                1. Parametri Generali<br/>\n                Questa finestra utilizza gbtree (albero booster)<br/>\n                </li>\n                <li>\n                2. Parametri Booster<br/>\n                2.1. Parametro per Tree Booster<br/>\n                eta: controlla il tasso di apprendimento: scala il contributo di ciascun albero di un fattore di 0 < eta < 1 quando viene aggiunto all'approssimazione corrente.<br/>\n                Utilizzato per prevenire l'overfitting rendendo il processo di boosting più conservativo.<br/>\n                Un valore più basso per eta implica un valore maggiore per nrounds: un valore basso di eta significa che il modello è più robusto all'overfitting ma più lento da calcolare. Predefinito: 0.3<br/>\n                gamma: riduzione minima della perdita richiesta per effettuare una ulteriore partizione su un nodo foglia dell'albero. più grande è, più conservativo sarà l'algoritmo.<br/>\n                max_depth: profondità massima di un albero. Predefinito: 6<br/>\n                min_child_weight: somma minima del peso delle istanze (hessian) necessaria in un figlio. Se il passo di partizione dell'albero risulta in un nodo foglia con la somma del peso delle istanze inferiore a min_child_weight, allora il processo di costruzione rinuncerà a ulteriori partizioni. In modalità regressione lineare, questo corrisponde semplicemente al numero minimo di istanze necessarie in ciascun nodo. Più grande è, più conservativo sarà l'algoritmo. Predefinito: 1<br/>\n                </li>\n                <li>\n                3. Parametri di Attività<br/>\n                objective: specifica il compito di apprendimento e il corrispondente obiettivo di apprendimento, gli utenti possono passare una funzione definita dall'utente. Le opzioni di obiettivo predefinite sono le seguenti:<br/>\n                reg:squarederror - Regressione con perdita quadrata (Predefinito).<br/>\n                reg:logistic regressione logistica.<br/>\n                binary:logistic - regressione logistica per classificazione binaria. Probabilità di output.<br/>\n                binary:logitraw - regressione logistica per classificazione binaria, punteggio di output prima della trasformazione logistica.<br/>\n                num_class: imposta il numero di classi. Da utilizzare solo con obiettivi multiclass.<br/>\n                multi:softmax - imposta xgboost per fare classificazione multiclass utilizzando l'obiettivo softmax. La classe è rappresentata da un numero e dovrebbe essere da 0 a num_class - 1.<br/>\n                multi:softprob - stesso di softmax, ma la previsione restituisce un vettore di elementi ndata * nclass, che può essere ulteriormente rimodellato in matrice ndata, nclass. Il risultato contiene le probabilità previste di ciascun punto dati appartenente a ciascuna classe.<br/>\n                rank:pairwise - imposta xgboost per eseguire il compito di ranking minimizzando la perdita pairwise.<br/>\n                base_score: il punteggio di previsione iniziale di tutte le istanze, bias globale. Predefinito: 0.5<br/>\n                eval_metric: metriche di valutazione per i dati di validazione. Gli utenti possono passare una funzione definita dall'utente a essa. Predefinito: la metrica sarà assegnata in base all'obiettivo (rmse per la regressione,\n                e errore per la classificazione, media della precisione per il ranking). L'elenco è fornito nella sezione dettagliata.<br/>\n                data: dataset di addestramento. xgb.train accetta solo un xgb.DMatrix come input. xgboost, inoltre, accetta anche matrice, dgCMatrix o nome di un file di dati locale.<br/>\n                nrounds: numero massimo di iterazioni di boosting.<br/>\n                verbose: Se 0, xgboost rimarrà in silenzio. Se 1, stamperà informazioni sulle prestazioni. Se 2, verranno stampate alcune informazioni aggiuntive. Nota che impostare verbose > 0 attiva automaticamente la funzione di callback cb.print.evaluation(period=1).<br/>\n                print_every_n: Stampa i messaggi di valutazione di ogni n-esima iterazione quando verbose>0. Il predefinito è 1, il che significa che tutti i messaggi vengono stampati. Questo parametro viene passato alla callback cb.print.evaluation.<br/>\n                label: vettore di valori di risposta. Non dovrebbe essere fornito quando i dati sono un nome di file di dati locale o un xgb.DMatrix.<br/>\n                missing: per impostazione predefinita è impostato su NA, il che significa che i valori NA devono essere considerati come 'mancanti' dall'algoritmo. A volte, 0 o un altro valore estremo potrebbe essere utilizzato per rappresentare valori mancanti. Questo parametro viene utilizzato solo quando l'input è una matrice densa.<br/>\n                </li>\n                </ul>\n                <b>Dettagli</b></br>\n                La metrica di valutazione viene scelta automaticamente da Xgboost (in base all'obiettivo) quando il parametro eval_metric non è fornito.</br>\n                L'utente può impostare uno o più parametri eval_metric. Nota che quando si utilizza una metrica personalizzata, solo questa singola metrica può essere utilizzata. Di seguito è riportato l'elenco delle metriche integrate per le quali Xgboost fornisce un'implementazione ottimizzata:</br>\n                rmse errore quadratico medio. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss log-verosimiglianza negativa. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss logloss multiclass. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error Tasso di errore di classificazione binaria. Viene calcolato come (# casi errati) / (# tutti i casi). Per impostazione predefinita, utilizza la soglia 0.5 per i valori previsti per definire istanze negative e positive.</br>\n                Una soglia diversa (ad es., 0.) potrebbe essere specificata come \"error@0.\"</br>\n                merror Tasso di errore di classificazione multiclass. Viene calcolato come (# casi errati) / (# tutti i casi).</br>\n                auc Area sotto la curva. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve per la valutazione del ranking.</br>\n                aucpr Area sotto la curva PR. https://en.wikipedia.org/wiki/Precision_and_recall per la valutazione del ranking.</br>\n                ndcg Guadagno cumulativo normalizzato scontato (per il compito di ranking). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                I seguenti callback vengono creati automaticamente quando vengono impostati determinati parametri:</br>\n                cb.print.evaluation è attivato quando verbose > 0; e il parametro print_every_n viene passato a esso.</br>\n                cb.evaluation.log è attivato quando è presente la watchlist.</br>\n                cb.early.stop: quando è impostato early_stopping_rounds.</br>\n                cb.save.model: quando è impostato save_period > 0.</br></br>\n                <b>Valore</b></br>\n                Un oggetto di classe xgb.Booster con i seguenti elementi:</br>\n                handle un handle (puntatore) al modello xgboost in memoria.</br>\n                raw un dump di memoria cache del modello xgboost salvato come tipo raw di R.</br>\n                niter: numero di iterazioni di boosting.</br>\n                evaluation_log: storia di valutazione memorizzata come un data.table con la prima colonna corrispondente al numero di iterazione e le restanti corrispondenti ai valori delle metriche di valutazione. Viene creata dalla callback cb.evaluation.log.</br>\n                call: una chiamata di funzione.</br>\n                params: parametri che sono stati passati alla libreria xgboost. Nota che non cattura i parametri modificati dalla callback cb.reset.parameters.</br>\n                callbacks funzioni di callback che sono state assegnate automaticamente o esplicitamente passate.</br>\n                best_iteration: numero di iterazione con il miglior valore della metrica di valutazione (disponibile solo con l'arresto anticipato).</br>\n                best_ntreelimit: il valore ntreelimit corrispondente alla migliore iterazione, che potrebbe essere ulteriormente utilizzato nel metodo predict (disponibile solo con l'arresto anticipato).</br>\n                best_score: il miglior valore della metrica di valutazione durante l'arresto anticipato. (disponibile solo con l'arresto anticipato).</br>\n                feature_names: nomi delle caratteristiche del dataset di addestramento (solo quando i nomi delle colonne sono stati definiti nei dati di addestramento).</br>\n                nfeatures: numero di caratteristiche nei dati di addestramento.</br>\n                <b>Riferimenti</b></br>\n                Tianqi Chen e Carlos Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>Vedi Anche</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Esempi</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## Un semplice esempio di xgb.train:</br>\n                # Queste funzioni potrebbero essere utilizzate passando loro sia:</br>\n                ## Un esempio di interfaccia 'xgboost':</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}