{
  "title": "极端梯度提升",
  "navigation": "极端梯度提升",
  "label1": "虚拟编码因子变量，请参见变量>计算>虚拟编码使用选项保留所有级别。要预测多个类别，请阅读帮助（点击对话框上的？图标）。",
  "model": "输入模型名称",
  "dependentvar": "因变量",
  "independentvars": "自变量",
  "objective": "目标",
  "seed": "设置种子",
  "nrounds": "最大提升迭代次数",
  "maxdepth": "最大深度",
  "minchildweight": "最小子权重",
  "maxdeltastep": "最大增量步长",
  "eta": "学习率（eta）",
  "gamma": "伽马",
  "numclasses": "类别数量。**仅与multi:softmax和multi:softprob一起使用",
  "basescore": "基础分数",
  "Verbose": "详细模式（0=静默，1=性能信息，2=其他信息）",
  "printevery": "当详细信息>0时，打印每n次迭代评估消息",
  "help": {
    "title": "极端梯度提升",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>描述</b></br>\n                创建一个极端梯度提升模型\n                <br/>\n                <b>注意</b></br>\n                1.要预测因子类型的因变量，您需要将因变量重新编码为从0开始的数值。例如，如果因子变量中有3个级别，则数值变量将包含值0,1,2。</br>\n                请参见数据>重新编码变量。或者只需将因子变量转换为数值，通常级别将映射到从1开始的整数，并从结果变量中减去1。这将为您提供一个从0开始的数值变量。</br>\n                您还需要将目标设置为multi:softmax（预测类别）或multi:softprob（预测概率），并在类别数量文本框中输入类别数量。</br>\n                类别数量仅在multi:softmax和multi:softprob中输入。对于其他目标，如果输入类别数量，将会生成错误。</br>\n                2.您需要对独立因子变量进行虚拟编码，使用1-Hot编码，请参见数据>计算虚拟变量</br>\n                3.当选择的目标是reg:squarederror、reg:logistic、binary:logitraw和rank:pairwise之一时，不会生成混淆矩阵和ROC曲线，因为预测函数\n                不返回因变量的类别。预测函数返回的数值预测（在reg:squarederror的情况下）、分数、排名等存储在数据集中。有关更多详细信息，请参见help(predict.xgb.Booster)</br>\n                4.当选择的目标是multi:softprob时，不会生成混淆矩阵，因为预测函数返回预测概率而不是因变量的类别。预测概率保存在数据集中，并生成ROC曲线。要查看混淆矩阵，请选择multi:softmax作为目标。</br>\n                5.当选择multi:softmax作为目标时，不会生成ROC曲线，因为预测函数返回类别而不是预测概率。要查看ROC曲线，请选择multi:softprob作为目标。</br>\n                <br/>\n                <b>用法</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>参数</b><br/>\n                <ul>\n                <li>\n                params: 参数列表。完整的参数列表可在http://xgboost.readthedocs.io/en/latest/parameter.html找到。以下是简短的摘要:<br/>\n                </li>\n                <li>\n                1. 一般参数<br/>\n                此对话框使用gbtree（树提升器）<br/>\n                </li>\n                <li>\n                2. 提升器参数<br/>\n                2.1. 树提升器的参数<br/>\n                eta: 控制学习率：将每棵树的贡献按0 < eta < 1的因子缩放，当它被添加到当前近似值时。<br/>\n                用于通过使提升过程更保守来防止过拟合。<br/>\n                eta值越低，nrounds值越大：低eta值意味着模型对过拟合更稳健，但计算速度较慢。默认值：0.3<br/>\n                gamma: 进一步划分树叶节点所需的最小损失减少。越大，算法越保守。<br/>\n                max_depth: 树的最大深度。默认值：6<br/>\n                min_child_weight: 子节点中所需的实例权重（海森矩阵）的最小总和。如果树划分步骤导致的叶节点的实例权重总和小于min_child_weight，则构建过程将放弃进一步的划分。在线性回归模式下，这简单对应于每个节点中所需的最小实例数量。越大，算法越保守。默认值：1<br/>\n                </li>\n                <li>\n                3. 任务参数<br/>\n                objective: 指定学习任务和相应的学习目标，用户可以传递自定义函数。默认目标选项如下:<br/>\n                reg:squarederror -平方损失回归（默认）。<br/>\n                reg:logistic 逻辑回归。<br/>\n                binary:logistic -二元分类的逻辑回归。输出概率。<br/>\n                binary:logitraw -二元分类的逻辑回归，在逻辑变换之前输出分数。<br/>\n                num_class: 设置类别数量。仅与多类目标一起使用。<br/>\n                multi:softmax -设置xgboost进行多类分类，使用softmax目标。类别由数字表示，应该从0到num_class - 1。<br/>\n                multi:softprob -与softmax相同，但预测输出一个ndata * nclass元素的向量，可以进一步重塑为ndata，nclass矩阵。结果包含每个数据点属于每个类别的预测概率。<br/>\n                rank:pairwise -设置xgboost进行通过最小化成对损失的排名任务。<br/>\n                base_score: 所有实例的初始预测分数，全局偏差。默认值：0.5<br/>\n                eval_metric: 验证数据的评估指标。用户可以传递自定义函数。默认情况下，指标将根据目标分配（回归的rmse，分类的错误，排名的平均精度）。详细部分提供了列表。<br/>\n                data: 训练数据集。xgb.train仅接受xgb.DMatrix作为输入。xgboost还接受矩阵、dgCMatrix或本地数据文件的名称。<br/>\n                nrounds: 最大提升迭代次数。<br/>\n                verbose: 如果为0，xgboost将保持静默。如果为1，它将打印性能信息。如果为2，将打印一些额外信息。请注意，设置verbose > 0会自动启用cb.print.evaluation(period=1)回调函数。<br/>\n                print_every_n: 当verbose>0时，打印每n次迭代评估消息。默认值为1，表示打印所有消息。此参数传递给cb.print.evaluation回调。<br/>\n                label: 响应值的向量。当数据是本地数据文件名或xgb.DMatrix时，不应提供。<br/>\n                missing: 默认设置为NA，这意味着算法应将NA值视为“缺失”。有时，0或其他极端值可能用于表示缺失值。此参数仅在输入为稠密矩阵时使用。<br/>\n                </li>\n                </ul>\n                <b>详细信息</b></br>\n                当未提供eval_metric参数时，评估指标由Xgboost自动选择（根据目标）。</br>\n                用户可以设置一个或多个eval_metric参数。请注意，当使用自定义指标时，仅可以使用此单个指标。以下是Xgboost提供优化实现的内置指标列表：</br>\n                rmse 均方根误差。http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss 负对数似然。http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss 多类对数损失。http://wiki.fast.ai/index.php/Log_Loss</br>\n                error 二元分类错误率。计算为（#错误案例）/（#所有案例）。默认情况下，它使用0.5阈值来定义负实例和正实例。</br>\n                可以指定不同的阈值（例如，0.）作为“error@0。”</br>\n                merror 多类分类错误率。计算为（#错误案例）/（#所有案例）。</br>\n                auc 曲线下面积。http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve用于排名评估。</br>\n                aucpr PR曲线下面积。https://en.wikipedia.org/wiki/Precision_and_recall用于排名评估。</br>\n                ndcg 标准化折扣累积增益（用于排名任务）。http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                当设置某些参数时，以下回调会自动创建：</br>\n                cb.print.evaluation在verbose > 0时启用；并且print_every_n参数传递给它。</br>\n                cb.evaluation.log在watchlist存在时启用。</br>\n                cb.early.stop: 当设置early_stopping_rounds时。</br>\n                cb.save.model: 当设置save_period > 0时。</br></br>\n                <b>值</b></br>\n                一个xgb.Booster类的对象，具有以下元素：</br>\n                handle 指向内存中xgboost模型的句柄（指针）。</br>\n                raw xgboost模型的缓存内存转储，保存为R的原始类型。</br>\n                niter: 提升迭代次数。</br>\n                evaluation_log: 评估历史记录存储为数据表，第一列对应于迭代次数，其余列对应于评估指标的值。由cb.evaluation.log回调创建。</br>\n                call: 函数调用。</br>\n                params: 传递给xgboost库的参数。请注意，它不捕获cb.reset.parameters回调更改的参数。</br>\n                callbacks 回调函数，可能是自动分配或显式传递的。</br>\n                best_iteration: 具有最佳评估指标值的迭代次数（仅在提前停止时可用）。</br>\n                best_ntreelimit: 对应于最佳迭代的ntreelimit值，可以在predict方法中进一步使用（仅在提前停止时可用）。</br>\n                best_score: 在提前停止期间最佳评估指标值（仅在提前停止时可用）。</br>\n                feature_names: 训练数据集特征的名称（仅在训练数据中定义了列名时）。</br>\n                nfeatures: 训练数据中的特征数量。</br>\n                <b>参考文献</b></br>\n                Tianqi Chen和Carlos Guestrin，“XGBoost：可扩展的树提升系统”，第22届SIGKDD知识发现与数据挖掘会议，2016年，https://arxiv.org/abs/1603.02754</br>\n                <b>另请参见</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>示例</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## 一个简单的xgb.train示例:</br>\n                # 这些函数可以通过传递它们来使用：</br>\n                ## 一个'xgboost'接口示例：</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}