{
  "title": "Perceptron multi-stratificat, folosind pachetul RSNNS",
  "navigation": "Perceptron multi-stratificat",
  "label1": "VĂ RUGĂM SĂ CODIFICAȚI VARIABILELE FACTOR, VEZI DATE > CALCULAȚI VARIABILE DUMMY (PĂSTRAȚI TOATE NIVELURILE A.K.A CODIFICARE ONE HOT). SCALATI ȘI CENTRATI VARIABILELE NUMERICE, VEZI DATE > STANDARDIZAȚI VARIABILELE",
  "model": "Introduceți un nume de model",
  "dependentvar": "Variabila dependentă",
  "independentvars": "Variabila(e) independent(e)",
  "seed": "Setați semințele",
  "iter": "Maxim de iterații pentru învățare",
  "tf": "Funcția de învățare",
  "label2": "Numărul de straturi ascunse și neuronii pe stratul ascuns",
  "layers": "Specificați numărul de neuroni în fiecare strat, de exemplu 1. Pentru 5 neuroni într-un strat, introduceți 5. 2. Pentru 5 neuroni în stratul 1, 6 neuroni în stratul 2, 7 neuroni în stratul 3 introduceți 5,6,7",
  "learnfuncparams": "Parametrii funcției de învățare",
  "help": {
    "title": "Perceptron multi-stratificat, folosind pachetul RSNNS",
    "r_help": "help(mlp, package ='RSNNS')",
    "body": "\n            <b>NOTĂ</b></br>\n            Când specificați o singură variabilă dependentă, aceasta poate fi numerică sau factor. Dacă variabila dependentă specificată este un factor, codificăm automat variabila factor folosind codificarea one-hot folosind funcția decode din pachetul RSNNS.</br></br>\n            În plus, dacă utilizați codificarea one-hot pentru a codifica o variabilă factor, puteți specifica mai mult de o variabilă dependentă în dialog. În acest caz, variabilele dependente trebuie să fie de tip numeric.</br></br>\n            Puteți folosi \"Date > Calculați variabile dummy\", alegeți setarea „Păstrați toate nivelurile” pentru a obține codificarea one-hot.</br></br>\n            Pentru variabilele dependente de tip factor, vom afișa o matrice de confuzie, ROC și statistici de precizie a modelului atunci când evaluăm un set de date folosind modelul construit. Predicțiile generate sunt de tip factor deoarece prezicem clasa. Acestea vor fi salvate în setul de date împreună cu probabilitățile prezise atunci când evaluăm.</br></br>\n            Când există variabile dependente codificate dummy, nu vom afișa o matrice de confuzie, ROC și statistici de precizie a modelului atunci când evaluăm un set de date folosind modelul construit. Cu toate acestea, predicțiile vor fi salvate în setul de date atunci când evaluăm setul de date. Predicțiile sunt probabilitățile asociate cu variabilele dependente codificate dummy.</br></br>\n            Este de obicei cel mai bine să standardizăm variabilele independente (de asemenea, trebuie să fie numerice) Vezi „Date > Standardizați variabilele.”</br></br>\n            Dacă aveți variabile independente categorice, utilizați codificarea one-hot pentru a codifica variabilele factor.</br></br>\n            <b>Descriere</b></br>\n            Această funcție creează un perceptron multi-stratificat (MLP) și îl antrenează. MLP-urile sunt rețele complet conectate feedforward și probabil cea mai comună arhitectură de rețea utilizată. Antrenamentul se realizează de obicei prin retropropagarea erorii sau o procedură similară.</br>\n            Există multe funcții de învățare diferite prezente în SNNS care pot fi utilizate împreună cu această funcție, de exemplu, Std_Backpropagation, BackpropBatch, BackpropChunk, BackpropMomentum, BackpropWeightDecay, Rprop, Quickprop, SCG (gradient conjugat scalat), ...</br>\n            <b>Utilizare</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## Metoda implicită S3:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>Argumente</b><br/>\n            <ul>\n            <li>\n            x: o matrice cu intrările de antrenament pentru rețea\n            </li>\n            <li>\n            ... : parametrii suplimentari ai funcției (în prezent nu sunt utilizați)\n            </li>\n            <li>\n            y: valorile corespunzătoare ale țintelor\n            </li>\n            <li>\n            size: numărul de unități în stratul(e) ascuns(e)\n            </li>\n            <li>\n            maxit: maxim de iterații pentru învățare\n            </li>\n            <li>\n            initFunc: funcția de inițializare de utilizat\n            </li>\n            <li>\n            initFuncParams: parametrii pentru funcția de inițializare\n            </li>\n            <li>\n            learnFunc: funcția de învățare de utilizat\n            </li>\n            <li>\n            learnFuncParams: parametrii pentru funcția de învățare\n            </li>\n            <li>\n            updateFunc: funcția de actualizare de utilizat\n            </li>\n            <li>\n            updateFuncParams: parametrii pentru funcția de actualizare\n            </li>\n            <li>\n            hiddenActFunc: funcția de activare a tuturor unităților ascunse\n            </li>\n            <li>\n            shufflePatterns: ar trebui să fie amestecate tiparele?\n            </li>\n            <li>\n            linOut: setează funcția de activare a unităților de ieșire la liniară sau logistică (ignorată dacă outputActFunc este dat)\n            </li>\n            <li>\n            outputActFunc: funcția de activare a tuturor unităților de ieșire\n            </li>\n            <li>\n            inputsTest: o matrice cu intrările pentru a testa rețeaua\n            </li>\n            <li>\n            targetsTest: țintele corespunzătoare pentru intrarea de test\n            </li>\n            <li>\n            pruneFunc: funcția de tăiere de utilizat\n            </li>\n            <li>\n            pruneFuncParams: parametrii pentru funcția de tăiere. Spre deosebire de celelalte funcții, acestea trebuie date într-o listă numită. Consultați demonstrațiile de tăiere pentru explicații suplimentare.\n            </li>\n            </ul>\n            <b>Detalii</b></br>\n            Std_Backpropagation, BackpropBatch, de exemplu, au doi parametri, rata de învățare și diferența maximă de ieșire. Rata de învățare este de obicei o valoare între 0.1 și 1. Aceasta specifică lățimea pasului de coborâre a gradientului. Diferența maximă definește cât de multă diferență între valoarea de ieșire și cea țintă este tratată ca eroare zero și nu este retropropagată. Acest parametru este utilizat pentru a preveni supraînvățarea. Pentru o listă completă a parametrilor tuturor funcțiilor de învățare, consultați Manualul utilizatorului SNNS, pp. 67.</br>\n            Valorile implicite setate pentru funcțiile de inițializare și actualizare de obicei nu trebuie schimbate.</br>\n            <b>Valoare</b><br/>\n            un obiect rsnns.\n            <b>Referințe</b><br/>\n            Rosenblatt, F. (1958), 'Perceptronul: un model probabilistic pentru stocarea și organizarea informației în creier', Psychological Review 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Group, P. R. (1986), Procesare distribuită în paralel: explorări în microstructura cogniției, Mit, Cambridge, MA etc.<br/>\n            Zell, A. et al. (1998), 'Manualul utilizatorului SNNS Stuttgart Neural Network Simulator, Versiunea 4.2', IPVR, Universitatea din Stuttgart și WSI, Universitatea din Tübingen.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), Simularea rețelelor neuronale, Addison-Wesley. (în germană)<br/>\n            <b>Exemple</b><br/>\n            <code> \n            data(iris)<br/>\n            #amestecați vectorul<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #matrice de confuzie cu metoda 402040<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>Pachet</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>Ajutor</b></br>\n            Pentru ajutor detaliat, faceți clic pe pictograma R din colțul din dreapta sus al acestui overlay de dialog sau rulați următoarea comandă în editorul de sintaxă R</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
  }
}