{
  "title": "Antrenarea rețelelor neuronale, folosind pachetul neuralnet",
  "navigation": "Rețele neuronale",
  "label1": "VĂ RUGĂM SĂ CODIFICAȚI VARIABILELE FACTOR DUMMY, VEZI DATE > CALCULAȚI VARIABILE DUMMY (MENȚINEȚI TOATE NIVELURILE A.K.A CODIFICARE ONE HOT). SCALARE ȘI CENTRARE A VARIABILELOR NUMERICE, VEZI DATE > STANDARDIZARE VARIABILE",
  "model": "Introduceți un nume de model",
  "dependentvar": "Variabila dependentă",
  "independentvars": "Variabila(e) independent(e)",
  "seed": "Setați semința",
  "iter": "Pași maximi pentru învățare",
  "tf": "Algoritm",
  "threshold": "Prag",
  "label2": "Numărul de straturi ascunse și neuronii pe stratul ascuns",
  "layers": "Specificați numărul de neuroni din fiecare strat, de exemplu 1. Pentru 5 neuroni într-un strat, introduceți 5 2. Pentru 5 neuroni în stratul 1, 6 neuroni în stratul 2, 7 neuroni în stratul 3 introduceți 5,6,7",
  "OutActFunc": "Specificați o funcție de activare a ieșirii",
  "rep": "Repetiții pentru antrenarea rețelei neuronale",
  "label3": "Factori de multiplicare pentru rata de învățare superioară și inferioară",
  "minus": "Superior (minus)",
  "upper": "Inferior (plus)",
  "lifesign": "Setarea a cât de mult să imprime în timpul calculului rețelei neuronale",
  "lifesignstep": "Dimensiunea pasului pentru a imprima pragul minim în modul de viață complet",
  "errfct": "Funcția diferențiabilă utilizată pentru calculul erorii",
  "linearoutput": "Funcția de activare nu ar trebui să fie aplicată neuronilor de ieșire",
  "likelihood": "Probabilitate",
  "help": {
    "title": "Antrenarea rețelelor neuronale, folosind pachetul neuralnet",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>NOTĂ</b></br>\n            Atunci când specificați o singură variabilă dependentă, aceasta poate fi numerică sau factor. Dacă variabila dependentă specificată este un factor, codificăm automat variabila factor folosind codificarea one-hot folosind funcția decode din pachetul RSNNS.</br></br>\n            În plus, dacă utilizați codificarea one-hot pentru a codifica o variabilă factor, puteți specifica mai mult de o variabilă dependentă în dialog. În acest caz, variabilele dependente trebuie să fie de tip numeric.</br></br>\n            Puteți folosi \"Date > Calculați variabile dummy\", alegeți setarea „Mențineți toate nivelurile” pentru a obține codificarea one-hot.</br></br>\n            Pentru variabilele dependente de tip factor, vom afișa o matrice de confuzie, ROC și statistici de precizie a modelului atunci când evaluăm un set de date folosind modelul construit. Predicțiile generate sunt de tip factor deoarece prezicem clasa. Acestea vor fi salvate în setul de date împreună cu probabilitățile prezise atunci când evaluăm.</br></br>\n            Când există variabile dependente codificate ca dummy, nu vom afișa o matrice de confuzie, ROC și statistici de precizie a modelului atunci când evaluăm un set de date folosind modelul construit. Cu toate acestea, predicțiile vor fi salvate în setul de date atunci când evaluăm setul de date. Predicțiile sunt probabilitățile asociate cu variabilele dependente codificate ca dummy.</br></br>\n            Este de obicei cel mai bine să standardizăm variabilele independente (de asemenea, trebuie să fie numerice) Vezi „Date > Standardizare variabile.”</br></br>\n            Dacă aveți variabile independente categorice, utilizați codificarea one-hot pentru a codifica variabilele factor.</br></br>\n            <b>Descriere</b></br>\n            Antrenați rețele neuronale folosind retropropagare, retropropagare rezistentă (RPROP) cu (Riedmiller, 1994) sau fără retrocedere a greutății (Riedmiller și Braun, 1993) sau versiunea modificată global convergentă (GRPROP) de Anastasiadis et al. (2005). Funcția permite setări flexibile prin alegerea personalizată a funcției de eroare și a funcției de activare. În plus, se implementează calculul greutăților generalizate (Intrator O. și Intrator N., 1993).\n            <b>Utilizare</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>Argumente</b><br/>\n            <ul>\n            <li>\n            formula: o descriere simbolică a modelului care trebuie ajustat.\n            </li>\n            <li>\n            data: un cadru de date care conține variabilele specificate în formulă.\n            </li>\n            <li>\n            hidden: un vector de întregi care specifică numărul de neuroni ascunși (vârfuri) în fiecare strat.\n            </li>\n            <li>\n            threshold: o valoare numerică care specifică pragul pentru derivatele parțiale ale funcției de eroare ca criteriu de oprire.\n            </li>\n            <li>\n            stepmax: pașii maximali pentru antrenarea rețelei neuronale. Atingerea acestui maxim duce la oprirea procesului de antrenare a rețelei neuronale.\n            </li>\n            <li>\n            rep: numărul de repetiții pentru antrenarea rețelei neuronale.\n            </li>\n            <li>\n            startweights: un vector care conține valori inițiale pentru greutăți. Setați la NULL pentru inițializare aleatorie.\n            </li>\n            <li>\n            learningrate.limit: un vector sau o listă care conține limita minimă și maximă pentru rata de învățare. Folosit doar pentru RPROP și GRPROP.</li>\n            <li>\n            learningrate.factor: un vector sau o listă care conține factorii de multiplicare pentru rata de învățare superioară și inferioară. Folosit doar pentru RPROP și GRPROP.\n            </li>\n            <li>\n            learningrate: o valoare numerică care specifică rata de învățare utilizată de retropropagarea tradițională. Folosit doar pentru retropropagarea tradițională.\n            </li>\n            <li>\n            lifesign: un șir care specifică cât de mult va imprima funcția în timpul calculului rețelei neuronale. 'none', 'minimal' sau 'full'.\n            </li>\n            <li>\n            lifesign.step: un întreg care specifică dimensiunea pasului pentru a imprima pragul minim în modul de viață complet.\n            </li>\n            <li>\n            algorithm: un șir care conține tipul de algoritm pentru a calcula rețeaua neuronală. Următoarele tipuri sunt posibile: 'backprop', 'rprop+', 'rprop-', 'sag' sau 'slr'. 'backprop' se referă la retropropagare, 'rprop+' și 'rprop-' se referă la retropropagarea rezistentă cu și fără retrocedere a greutății, în timp ce 'sag' și 'slr' induc utilizarea algoritmului modificat global convergent (grprop). Vedeți Detalii pentru mai multe informații.\n            </li>\n            <li>\n            err.fct: o funcție diferențiabilă care este utilizată pentru calculul erorii. Alternativ, se pot folosi șirurile 'sse' și 'ce' care reprezintă suma erorilor pătrate și entropia încrucișată.\n            </li>\n            <li>\n            act.fct: o funcție diferențiabilă care este utilizată pentru a netezi rezultatul produsului încrucișat al covariabilei sau neuronilor și greutăților. În plus, șirurile 'logistic' și 'tanh' sunt posibile pentru funcția logistică și tangenta hiperbolică.\n            </li>\n            <li>\n            linear.output: logic. Dacă act.fct nu ar trebui să fie aplicată neuronilor de ieșire, setați ieșirea liniară la TRUE, altfel la FALSE.\n            </li>\n            <li>\n            exclude: un vector sau o matrice care specifică greutățile care sunt excluse din calcul. Dacă este dat ca un vector, trebuie să se cunoască pozițiile exacte ale greutăților. O matrice cu n rânduri și 3 coloane va exclude n greutăți, unde prima coloană reprezintă stratul, a doua coloană neuronul de intrare și a treia coloană neuronul de ieșire al greutății.\n            </li>\n            <li>\n            constant.weights: un vector care specifică valorile greutăților care sunt excluse din procesul de antrenare și tratate ca fixe.\n            </li>\n            <li>\n            likelihood: logic. Dacă funcția de eroare este egală cu funcția de verosimilitate negativă, se vor calcula criteriile de informație AIC și BIC. În plus, utilizarea intervalului de încredere este semnificativă.\n            </li>\n            </ul>\n            <b>Detalii</b><br/>\n            Algoritmul global convergent se bazează pe retropropagarea rezistentă fără retrocedere a greutății și modifică în plus o rată de învățare, fie rata de învățare asociată cu cel mai mic gradient absolut (sag) sau cea mai mică rată de învățare (slr) în sine. Ratele de învățare în algoritmul grprop sunt limitate la limitele definite în learningrate.limit.\n            ​<b>Valoare</b><br/>\n            neuralnet returnează un obiect de clasă nn. Un obiect de clasă nn este o listă care conține cel mult următoarele componente:<br/>\n            call: apelul corespunzător.<br/>\n            response: extras din argumentul de date.<br/>\n            covariate: variabilele extrase din argumentul de date.<br/>\n            model.list: o listă care conține covariabilele și variabilele de răspuns extrase din argumentul de formulă.<br/>\n            err.fct: funcția de eroare.<br/>\n            act.fct: funcția de activare.<br/>\n            data: argumentul de date.<br/>\n            net.result: o listă care conține rezultatul general al rețelei neuronale pentru fiecare repetiție.<br/>\n            weights: o listă care conține greutățile ajustate ale rețelei neuronale pentru fiecare repetiție.<br/>\n            generalized.weights: o listă care conține greutățile generalizate ale rețelei neuronale pentru fiecare repetiție.<br/>\n            result.matrix: o matrice care conține pragul atins, pașii necesari, eroarea, AIC și BIC (dacă este calculat) și greutățile pentru fiecare repetiție. Fiecare coloană reprezintă o repetiție.<br/>\n            startweights: o listă care conține greutățile inițiale ale rețelei neuronale pentru fiecare repetiție.<br/>\n            ​<b>Exemple</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# Clasificare binară\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Nu a fost rulat: print(nn)\n            ## Nu a fost rulat: plot(nn)\n            # Clasificare multiclasă\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Nu a fost rulat: print(nn)\n            ## Nu a fost rulat: plot(nn)\n            # Funcție de activare personalizată\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## Nu a fost rulat: print(nn)\n            ## Nu a fost rulat: plot(nn)\n            </code> <br/>\n            <b>Pachet</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>Ajutor</b></br>\n            Pentru ajutor detaliat, faceți clic pe pictograma R din colțul din dreapta sus al acestui dialog sau rulați următoarea comandă în editorul de sintaxă R</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}