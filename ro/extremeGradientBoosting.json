{
  "title": "Îmbunătățirea Gradientului Extrem",
  "navigation": "Îmbunătățirea Gradientului Extrem",
  "label1": "VARIABLE DE FACTOR DE COD DUMMY, VEZI VARIABLES>CALCULEAZĂ>OPȚIUNEA DE UTILIZARE A CODULUI DUMMY PĂSTREAZĂ TOATE NIVELURILE. PENTRU PREDICȚIA MULTIPLELOR CLASE, CITESTE AJUTORUL (FACI CLIC PE ICONIȚA ? DIN DIALOG).",
  "model": "Introduceți numele modelului",
  "dependentvar": "Variabila dependentă",
  "independentvars": "Variabilă(variabile) independent(e)",
  "objective": "Obiectiv",
  "seed": "Setați semința",
  "nrounds": "Numărul maxim de iterații de îmbunătățire",
  "maxdepth": "Adâncimea maximă",
  "minchildweight": "Greutatea minimă a copilului",
  "maxdeltastep": "Pasul delta maxim",
  "eta": "eta (rata de învățare)",
  "gamma": "gamma",
  "numclasses": "Numărul de clase. **Utilizați doar cu multi:softmax și multi:softprob",
  "basescore": "Scor de bază",
  "Verbose": "Mod detaliat (0=Silențios, 1=info de performanță, 2= alte informații)",
  "printevery": "Imprimați mesajele de evaluare pentru fiecare n-a iterație atunci când verbose > 0",
  "help": {
    "title": "Îmbunătățirea Gradientului Extrem",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>Descriere</b></br>\n                Creați un model de Îmbunătățire a Gradientului Extrem\n                <br/>\n                <b>NOTĂ</b></br>\n                1. Pentru a prezice variabila dependentă de tip factor, trebuie să recodificați variabila dependentă într-un număr cu valori începând de la 0. De exemplu, dacă există 3 niveluri în variabila factor, variabila numerică va conține valori 0,1,2.</br>\n                Vedeți Date > Recodificați Variabile. Alternativ, pur și simplu convertiți variabila factor în numeric, de obicei nivelurile se vor mapa la întregi începând de la 1 și scădeți 1 din variabila rezultată. Acest lucru vă va oferi o variabilă numerică cu valori începând de la 0.</br>\n                De asemenea, trebuie să setați obiectivul la multi:softmax (predice clase) sau multi:softprob (predice probabilități) și să introduceți numărul de clase în caseta de text a numărului de clase.</br>\n                Numărul de clase trebuie introdus doar pentru multi:softmax și multi:softprob. Vor fi generate erori dacă numărul de clase este introdus pentru celelalte obiective.</br>\n                2. Trebuie să codificați variabilele independente de factor, utilizați codificarea 1-Hot vedeți Date > Calculați Variabile Dummy</br>\n                3. O matrice de confuzie și o curbă ROC nu sunt generate atunci când obiectivul selectat este unul dintre reg:squarederror, reg:logistic, binary:logitraw și rank:pairwise deoarece funcția de predicție \n                nu returnează clasa variabilei dependente. Predicțiile numerice (în cazul reg:squarederror), scoruri, ranguri etc. pe care funcția de predicție le returnează sunt stocate în setul de date. Vedeți help(predict.xgb.Booster) pentru mai multe detalii</br>\n                4. O matrice de confuzie nu este generată atunci când obiectivul selectat este multi:softprob deoarece funcția de predicție returnează probabilitățile prezise și nu clasa variabilei dependente. Probabilitățile prezise sunt salvate în setul de date și se generează curba ROC. Pentru a vedea matricea de confuzie, selectați multi:softmax ca obiectiv.</br>\n                5. O curbă ROC nu este generată atunci când obiectivul de multi:softmax este selectat deoarece funcția de predicție returnează clasa și nu probabilitățile prezise. Pentru a vedea curba ROC selectați multi:softprob ca obiectiv .</br>\n                <br/>\n                <b>Utilizare</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Argumente</b><br/>\n                <ul>\n                <li>\n                params: lista de parametrii. Lista completă de parametrii este disponibilă la http://xgboost.readthedocs.io/en/latest/parameter.html. Mai jos este un rezumat mai scurt:<br/>\n                </li>\n                <li>\n                1. Parametrii Generali<br/>\n                Acest dialog folosește gbtree (booster de arbore)<br/>\n                </li>\n                <li>\n                2. Parametrii Booster<br/>\n                2.1. Parametru pentru Booster de Arbore<br/>\n                eta: controlează rata de învățare: scalează contribuția fiecărui arbore printr-un factor de 0 < eta < 1 atunci când este adăugat la aproximația curentă.<br/>\n                Utilizat pentru a preveni supraînvățarea făcând procesul de îmbunătățire mai conservator.<br/>\n                O valoare mai mică pentru eta implică o valoare mai mare pentru nrounds: o valoare mică de eta înseamnă că modelul este mai robust la supraînvățare dar mai lent de calculat. Predeterminat: 0.3<br/>\n                gamma: reducerea minimă a pierderii necesară pentru a face o partiție suplimentară pe un nod frunză al arborelui. cu cât este mai mare, cu atât algoritmul va fi mai conservator.<br/>\n                max_depth: adâncimea maximă a unui arbore. Predeterminat: 6<br/>\n                min_child_weight: suma minimă a greutății instanței (hessian) necesară într-un copil. Dacă pasul de partiție al arborelui rezultă într-un nod frunză cu suma greutății instanței mai mică decât min_child_weight, atunci procesul de construcție va renunța la mai multe partiții. În modul de regresie liniară, aceasta corespunde pur și simplu numărului minim de instanțe necesare în fiecare nod. Cu cât este mai mare, cu atât algoritmul va fi mai conservator. Predeterminat: 1<br/>\n                </li>\n                <li>\n                3. Parametrii Sarcinii<br/>\n                objective: specificați sarcina de învățare și obiectivul de învățare corespunzător, utilizatorii pot trece o funcție definită de ei înșiși. Opțiunile de obiectiv predeterminate sunt mai jos:<br/>\n                reg:squarederror -Regresie cu pierdere pătrată (Predeterminat).<br/>\n                reg:logistic regresie logistică.<br/>\n                binary:logistic -regresie logistică pentru clasificare binară. Probabilitate de ieșire.<br/>\n                binary:logitraw -regresie logistică pentru clasificare binară, ieșire de scor înainte de transformarea logistică.<br/>\n                num_class: setați numărul de clase. Pentru a folosi doar cu obiective multiclasificate.<br/>\n                multi:softmax -setați xgboost pentru a face clasificare multiclasificată folosind obiectivul softmax. Clasa este reprezentată printr-un număr și ar trebui să fie de la 0 la num_class - 1.<br/>\n                multi:softprob -la fel ca softmax, dar predicția produce un vector de elemente ndata * nclass, care poate fi ulterior remodelat într-o matrice ndata, nclass. Rezultatul conține probabilitățile prezise ale fiecărui punct de date aparținând fiecărei clase.<br/>\n                rank:pairwise -setați xgboost pentru a face o sarcină de clasificare minimizând pierderea pe perechi.<br/>\n                base_score: scorul inițial de predicție al tuturor instanțelor, bias global. Predeterminat: 0.5<br/>\n                eval_metric: metrici de evaluare pentru datele de validare. Utilizatorii pot trece o funcție definită de ei înșiși. Predeterminat: metrica va fi atribuită conform obiectivului (rmse pentru regresie,\n                și eroare pentru clasificare, precizie medie pentru clasificare). Lista este furnizată în secțiunea detalii.<br/>\n                data: set de date de antrenament. xgb.train acceptă doar un xgb.DMatrix ca intrare. xgboost, în plus, acceptă și matrice, dgCMatrix sau numele unui fișier de date local.<br/>\n                nrounds: numărul maxim de iterații de îmbunătățire.<br/>\n                verbose: Dacă 0, xgboost va rămâne tăcut. Dacă 1, va imprima informații despre performanță. Dacă 2, se vor imprima informații suplimentare. Rețineți că setarea verbose > 0 activează automat funcția de apel cb.print.evaluation(period=1).<br/>\n                print_every_n: Imprimați mesajele de evaluare pentru fiecare n-a iterație atunci când verbose>0. Valoarea implicită este 1, ceea ce înseamnă că toate mesajele sunt imprimate. Acest parametru este trecut la funcția de apel cb.print.evaluation.<br/>\n                label: vector de valori de răspuns. Nu trebuie furnizat atunci când datele sunt un nume de fișier de date local sau un xgb.DMatrix.<br/>\n                missing: în mod implicit este setat la NA, ceea ce înseamnă că valorile NA ar trebui considerate ca 'lipsă' de algoritm. Uneori, 0 sau altă valoare extremă ar putea fi utilizată pentru a reprezenta valorile lipsă. Acest parametru este utilizat doar atunci când intrarea este o matrice densă.<br/>\n                </li>\n                </ul>\n                <b>Detalii</b></br>\n                Metrica de evaluare este aleasă automat de Xgboost (conform obiectivului) atunci când parametrul eval_metric nu este furnizat.</br>\n                Utilizatorul poate seta unul sau mai mulți parametri eval_metric. Rețineți că atunci când utilizați o metrică personalizată, doar această metrică unică poate fi utilizată. Următoarea este lista de metrici încorporate pentru care Xgboost oferă o implementare optimizată:</br>\n                rmse eroare pătratică medie. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss logaritm negativ al verosimilității. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss logloss multiclasificat. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error Rata de eroare a clasificării binare. Se calculează ca (# cazuri greșite) / (# toate cazurile). În mod implicit, folosește pragul de 0.5 pentru valorile prezise pentru a defini instanțele negative și pozitive.</br>\n                Un prag diferit (de exemplu, 0.) ar putea fi specificat ca \"eroare@0.\"</br>\n                merror Rata de eroare a clasificării multiclasificate. Se calculează ca (# cazuri greșite) / (# toate cazurile).</br>\n                auc Zona sub curbă. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve pentru evaluarea clasificării.</br>\n                aucpr Zona sub curba PR. https://en.wikipedia.org/wiki/Precision_and_recall pentru evaluarea clasificării.</br>\n                ndcg Câștig Cumulat Normalizat (pentru sarcina de clasificare). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                Următoarele apeluri sunt create automat atunci când anumite parametrii sunt setați:</br>\n                cb.print.evaluation este activat atunci când verbose > 0; și parametrul print_every_n este trecut la ea.</br>\n                cb.evaluation.log este activat atunci când lista de observație este prezentă.</br>\n                cb.early.stop: atunci când early_stopping_rounds este setat.</br>\n                cb.save.model: atunci când save_period > 0 este setat.</br></br>\n                <b>Valoare</b></br>\n                Un obiect de clasă xgb.Booster cu următoarele elemente:</br>\n                handle un maner (pointer) la modelul xgboost în memorie.</br>\n                raw un dump de memorie în cache al modelului xgboost salvat ca tip raw de R.</br>\n                niter: numărul de iterații de îmbunătățire.</br>\n                evaluation_log: istoria evaluării stocată ca un data.table cu prima coloană corespunzătoare numărului de iterație și restul corespunzând valorilor metricilor de evaluare. Este creată de apelul cb.evaluation.log.</br>\n                call: o apelare a funcției.</br>\n                params: parametrii care au fost trecuți la biblioteca xgboost. Rețineți că nu capturează parametrii schimbați de apelul cb.reset.parameters.</br>\n                callbacks funcții de apel care au fost fie atribuite automat, fie trecute explicit.</br>\n                best_iteration: numărul de iterație cu cel mai bun valoare a metricii de evaluare (disponibilă doar cu oprirea timpurie).</br>\n                best_ntreelimit: valoarea ntreelimit corespunzătoare celei mai bune iterații, care ar putea fi utilizată ulterior în metoda de predicție (disponibilă doar cu oprirea timpurie).</br>\n                best_score: cea mai bună valoare a metricii de evaluare în timpul opririi timpurii. (disponibilă doar cu oprirea timpurie).</br>\n                feature_names: numele caracteristicilor setului de date de antrenament (doar atunci când numele coloanelor au fost definite în datele de antrenament).</br>\n                nfeatures: numărul de caracteristici în datele de antrenament.</br>\n                <b>Referințe</b></br>\n                Tianqi Chen și Carlos Guestrin, \"XGBoost: Un Sistem de Îmbunătățire a Arborilor Scalabil\", Conferința SIGKDD 22 despre Descoperirea Cunoștințelor și Minarea Datelor, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>Vezi De asemenea</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Exemple</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## Un exemplu simplu de xgb.train:</br>\n                # Aceste funcții ar putea fi utilizate trecându-le fie:</br>\n                ## Un exemplu de interfață 'xgboost':</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}