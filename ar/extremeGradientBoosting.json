{
  "title": "تعزيز التدرج المتطرف",
  "navigation": "تعزيز التدرج المتطرف",
  "label1": "متغيرات عامل الشيفرة الوهمية، راجع المتغيرات>احسب>خيار استخدام الشيفرة الوهمية احتفظ بجميع المستويات. للتنبؤ بفئات متعددة، اقرأ المساعدة (انقر على أيقونة ? في الحوار).",
  "model": "أدخل اسم النموذج",
  "dependentvar": "المتغير التابع",
  "independentvars": "المتغير (المتغيرات) المستقلة",
  "objective": "الهدف",
  "seed": "تعيين البذور",
  "nrounds": "الحد الأقصى لعدد تكرارات التعزيز",
  "maxdepth": "العمق الأقصى",
  "minchildweight": "الحد الأدنى لوزن الطفل",
  "maxdeltastep": "الحد الأقصى لخطوة دلتا",
  "eta": "إيتا (معدل التعلم)",
  "gamma": "غاما",
  "numclasses": "عدد الفئات. **استخدم فقط مع multi:softmax و multi:softprob",
  "basescore": "الدرجة الأساسية",
  "Verbose": "وضع التفاصيل (0=صامت، 1=معلومات الأداء، 2= معلومات أخرى)",
  "printevery": "طباعة رسائل تقييم كل n-ث iteration عندما يكون verbose > 0",
  "OptvarTaskparam": "معلمات المهمة",
  "OptvarAdvDiagnostics": "تشخيصات متقدمة",
  "OptvarTreeBoostparam": "معلمات تعزيز الشجرة",
  "help": {
    "title": "تعزيز التدرج المتطرف",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>الوصف</b></br>\n                إنشاء نموذج تعزيز التدرج المتطرف\n                <br/>\n                <b>ملاحظة</b></br>\n                1. للتنبؤ بالمتغير التابع من نوع العامل، تحتاج إلى إعادة ترميز المتغير التابع إلى قيمة عددية تبدأ من 0. على سبيل المثال، إذا كان هناك 3 مستويات في متغير العامل، فإن المتغير العددي سيحتوي على القيم 0،1،2.</br>\n                راجع البيانات > إعادة ترميز المتغيرات. بدلاً من ذلك، قم بتحويل متغير العامل إلى عدد، عادةً ما يتم تعيين المستويات إلى أعداد صحيحة تبدأ من 1 وطرح 1 من المتغير الناتج. سيعطيك هذا متغيرًا عدديًا بقيم تبدأ من 0.</br>\n                تحتاج أيضًا إلى تعيين الهدف إلى multi:softmax (يتنبأ بالفئات) أو multi:softprob (يتنبأ بالاحتمالات) وإدخال عدد الفئات في مربع نص عدد الفئات.</br>\n                يجب إدخال عدد الفئات فقط لـ multi:softmax و multi:softprob. سيتم إنشاء أخطاء إذا تم إدخال عدد الفئات للأهداف الأخرى.</br>\n                2. تحتاج إلى استخدام الشيفرة الوهمية للمتغيرات المستقلة، استخدم ترميز 1-Hot راجع البيانات > احسب المتغيرات الوهمية</br>\n                3. لا يتم إنشاء مصفوفة الارتباك ومنحنى ROC عندما يكون الهدف المحدد هو أحد reg:squarederror، reg:logistic، binary:logitraw و rank:pairwise حيث أن وظيفة التنبؤ \n                لا تعيد فئة المتغير التابع. يتم تخزين التنبؤات العددية (في حالة reg:squarederror)، الدرجات، الترتيبات وما إلى ذلك التي تعيدها وظيفة التنبؤ في مجموعة البيانات. راجع help(predict.xgb.Booster) لمزيد من التفاصيل</br>\n                4. لا يتم إنشاء مصفوفة الارتباك عندما يكون الهدف المحدد هو multi:softprob حيث أن وظيفة التنبؤ تعيد الاحتمالات المتوقعة وليس فئة المتغير التابع. يتم حفظ الاحتمالات المتوقعة في مجموعة البيانات ويتم إنشاء منحنى ROC. لرؤية مصفوفة الارتباك، اختر multi:softmax كهدف.</br>\n                5. لا يتم إنشاء منحنى ROC عندما يتم اختيار هدف multi:softmax حيث أن وظيفة التنبؤ تعيد الفئة وليس الاحتمالات المتوقعة. لرؤية منحنى ROC، اختر multi:softprob كهدف.</br>\n                <br/>\n                <b>الاستخدام</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>المعلمات</b><br/>\n                <ul>\n                <li>\n                params: قائمة المعلمات. القائمة الكاملة للمعلمات متاحة على http://xgboost.readthedocs.io/en/latest/parameter.html. أدناه ملخص أقصر:<br/>\n                </li>\n                <li>\n                1. المعلمات العامة<br/>\n                يستخدم هذا الحوار gbtree (معزز الشجرة)<br/>\n                </li>\n                <li>\n                2. معلمات المعزز<br/>\n                2.1. معلمة معزز الشجرة<br/>\n                إيتا: التحكم في معدل التعلم: قم بتقليل مساهمة كل شجرة بعامل من 0 < إيتا < 1 عند إضافتها إلى التقريب الحالي.<br/>\n                يستخدم لمنع الإفراط في التخصيص عن طريق جعل عملية التعزيز أكثر تحفظًا.<br/>\n                قيمة أقل لإيتا تعني قيمة أكبر لـ nrounds: قيمة إيتا المنخفضة تعني أن النموذج أكثر قوة ضد الإفراط في التخصيص ولكن أبطأ في الحساب. الافتراضي: 0.3<br/>\n                غاما: الحد الأدنى لتقليل الخسارة المطلوبة لإجراء تقسيم إضافي على عقدة ورقة الشجرة. كلما كانت أكبر، كانت الخوارزمية أكثر تحفظًا.<br/>\n                max_depth: الحد الأقصى لعمق الشجرة. الافتراضي: 6<br/>\n                min_child_weight: الحد الأدنى لمجموع وزن الحالة (هسيان) المطلوب في طفل. إذا كانت خطوة تقسيم الشجرة تؤدي إلى عقدة ورقة بمجموع وزن الحالة أقل من min_child_weight، فإن عملية البناء ستتخلى عن المزيد من التقسيم. في وضع الانحدار الخطي، يتوافق هذا ببساطة مع الحد الأدنى لعدد الحالات المطلوبة في كل عقدة. كلما كانت أكبر، كانت الخوارزمية أكثر تحفظًا. الافتراضي: 1<br/>\n                </li>\n                <li>\n                3. معلمات المهمة<br/>\n                الهدف: تحديد مهمة التعلم والهدف التعليمي المقابل، يمكن للمستخدمين تمرير وظيفة معرفة ذاتيًا إليها. خيارات الهدف الافتراضية أدناه:<br/>\n                reg:squarederror - الانحدار مع خسارة مربعة (افتراضي).<br/>\n                reg:logistic الانحدار اللوجستي.<br/>\n                binary:logistic - الانحدار اللوجستي للتصنيف الثنائي. احتمال الخروج.<br/>\n                binary:logitraw - الانحدار اللوجستي للتصنيف الثنائي، الخروج من الدرجة قبل التحويل اللوجستي.<br/>\n                num_class: تعيين عدد الفئات. للاستخدام فقط مع الأهداف متعددة الفئات.<br/>\n                multi:softmax - تعيين xgboost للقيام بتصنيف متعدد الفئات باستخدام الهدف softmax. يتم تمثيل الفئة برقم ويجب أن تكون من 0 إلى num_class - 1.<br/>\n                multi:softprob - نفس الشيء مثل softmax، ولكن تنبؤات الخروج عبارة عن متجه من عناصر ndata * nclass، والتي يمكن إعادة تشكيلها لاحقًا إلى مصفوفة ndata، nclass. تحتوي النتيجة على الاحتمالات المتوقعة لكل نقطة بيانات تنتمي إلى كل فئة.<br/>\n                rank:pairwise - تعيين xgboost للقيام بمهمة الترتيب عن طريق تقليل الخسارة الزوجية.<br/>\n                base_score: الدرجة الأولية للتنبؤ لجميع الحالات، التحيز العالمي. الافتراضي: 0.5<br/>\n                eval_metric: مقاييس التقييم لبيانات التحقق. يمكن للمستخدمين تمرير وظيفة معرفة ذاتيًا إليها. الافتراضي: سيتم تعيين المقياس وفقًا للهدف (rmse للانحدار، \n                والخطأ للتصنيف، ومتوسط الدقة للتصنيف). يتم توفير القائمة في قسم التفاصيل.<br/>\n                data: مجموعة بيانات التدريب. تقبل xgb.train فقط xgb.DMatrix كمدخل. xgboost، بالإضافة إلى ذلك، يقبل أيضًا المصفوفة، dgCMatrix، أو اسم ملف بيانات محلي.<br/>\n                nrounds: الحد الأقصى لعدد تكرارات التعزيز.<br/>\n                verbose: إذا كانت 0، ستبقى xgboost صامتة. إذا كانت 1، ستطبع معلومات حول الأداء. إذا كانت 2، سيتم طباعة بعض المعلومات الإضافية. لاحظ أن تعيين verbose > 0 يشغل تلقائيًا وظيفة رد الاتصال cb.print.evaluation(period=1).<br/>\n                print_every_n: طباعة رسائل تقييم كل n-ث iteration عندما يكون verbose>0. الافتراضي هو 1 مما يعني أنه يتم طباعة جميع الرسائل. يتم تمرير هذه المعلمة إلى رد الاتصال cb.print.evaluation.<br/>\n                label: متجه من قيم الاستجابة. يجب عدم توفيره عندما تكون البيانات اسم ملف بيانات محلي أو xgb.DMatrix.<br/>\n                missing: بشكل افتراضي يتم تعيينه إلى NA، مما يعني أن القيم NA يجب اعتبارها 'مفقودة' بواسطة الخوارزمية. أحيانًا، قد يتم استخدام 0 أو قيمة متطرفة أخرى لتمثيل القيم المفقودة. يتم استخدام هذه المعلمة فقط عندما تكون المدخلات مصفوفة كثيفة.<br/>\n                </li>\n                </ul>\n                <b>التفاصيل</b></br>\n                يتم اختيار مقياس التقييم تلقائيًا بواسطة Xgboost (وفقًا للهدف) عندما لا يتم توفير معلمة eval_metric.</br>\n                يمكن للمستخدم تعيين واحدة أو عدة معلمات eval_metric. لاحظ أنه عند استخدام مقياس مخصص، يمكن استخدام هذا المقياس الفردي فقط. القائمة التالية هي قائمة بالمقاييس المدمجة التي توفر Xgboost تنفيذًا محسنًا لها:</br>\n                rmse خطأ الجذر التربيعي. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss احتمال السجل السالب. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss احتمال السجل متعدد الفئات. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error معدل خطأ التصنيف الثنائي. يتم حسابه كـ (# حالات خاطئة) / (# جميع الحالات). بشكل افتراضي، يستخدم عتبة 0.5 للقيم المتوقعة لتعريف الحالات السلبية والإيجابية.</br>\n                يمكن تحديد عتبة مختلفة (مثل 0.) كـ \"error@0.\"</br>\n                merror معدل خطأ التصنيف متعدد الفئات. يتم حسابه كـ (# حالات خاطئة) / (# جميع الحالات).</br>\n                auc المنطقة تحت المنحنى. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve لتقييم الترتيب.</br>\n                aucpr المنطقة تحت منحنى PR. https://en.wikipedia.org/wiki/Precision_and_recall لتقييم الترتيب.</br>\n                ndcg مكسب تراكمي مخفض مُعَد (لمهمة الترتيب). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                يتم إنشاء ردود الاتصال التالية تلقائيًا عند تعيين معلمات معينة:</br>\n                cb.print.evaluation يتم تشغيله عندما يكون verbose > 0؛ ويتم تمرير معلمة print_every_n إليه.</br>\n                cb.evaluation.log مفعل عندما تكون قائمة المراقبة موجودة.</br>\n                cb.early.stop: عندما يتم تعيين early_stopping_rounds.</br>\n                cb.save.model: عندما يتم تعيين save_period > 0.</br></br>\n                <b>القيمة</b></br>\n                كائن من فئة xgb.Booster مع العناصر التالية:</br>\n                handle مقبض (مؤشر) لنموذج xgboost في الذاكرة.</br>\n                raw تفريغ ذاكرة مؤقتة لنموذج xgboost محفوظ كنوع raw في R.</br>\n                niter: عدد تكرارات التعزيز.</br>\n                evaluation_log: تاريخ التقييم المخزن كـ data.table مع العمود الأول الذي يتوافق مع رقم التكرار والباقي يتوافق مع قيم مقاييس التقييم. يتم إنشاؤه بواسطة رد الاتصال cb.evaluation.log.</br>\n                call: استدعاء دالة.</br>\n                params: المعلمات التي تم تمريرها إلى مكتبة xgboost. لاحظ أنه لا يلتقط المعلمات التي تم تغييرها بواسطة رد الاتصال cb.reset.parameters.</br>\n                callbacks وظائف رد الاتصال التي تم تعيينها تلقائيًا أو تمريرها صراحة.</br>\n                best_iteration: رقم التكرار مع أفضل قيمة لمقياس التقييم (متاح فقط مع التوقف المبكر).</br>\n                best_ntreelimit: قيمة ntreelimit المقابلة لأفضل تكرار، والتي يمكن استخدامها لاحقًا في طريقة التنبؤ (متاحة فقط مع التوقف المبكر).</br>\n                best_score: أفضل قيمة لمقياس التقييم خلال التوقف المبكر. (متاحة فقط مع التوقف المبكر).</br>\n                feature_names: أسماء ميزات مجموعة بيانات التدريب (فقط عندما تم تعريف أسماء الأعمدة في بيانات التدريب).</br>\n                nfeatures: عدد الميزات في بيانات التدريب.</br>\n                <b>المراجع</b></br>\n                تيانكي تشين وكارلوس غيستين، \"XGBoost: نظام تعزيز الشجرة القابل للتوسع\"، المؤتمر الثاني والعشرون لاكتشاف المعرفة واستخراج البيانات، 2016، https://arxiv.org/abs/1603.02754</br>\n                <b>انظر أيضًا</b></br>\n                ردود الاتصال، predict.xgb.Booster، xgb.cv</br>\n                <b>أمثلة</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## مثال بسيط على xgb.train:</br>\n                # يمكن استخدام هذه الوظائف عن طريق تمريرها إما:</br>\n                ## مثال واجهة 'xgboost':</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}