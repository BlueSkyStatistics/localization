{
  "title": "Antrenarea rețelelor neuronale, folosind pachetul neuralnet",
  "navigation": "Rețele neuronale",
  "label1": "VĂ RUGĂM SĂ CODIFICAȚI VARIABILELE FACTOR, VEZI DATE > CALCULAȚI VARIABILE DUMMY (PĂSTRAȚI TOATE NIVELURILE A.K.A CODIFICARE ONE HOT). SCALATI ȘI CENTRATI VARIABILELE NUMERICE, VEZI DATE > STANDARDIZAȚI VARIABILELE",
  "model": "Introduceți un nume de model",
  "dependentvar": "Variabila dependentă",
  "independentvars": "Variabila(e) independent(e)",
  "seed": "Setați semințele",
  "iter": "Maxim pași pentru învățare",
  "tf": "Algoritm",
  "threshold": "Prag",
  "label2": "Numărul de straturi ascunse și neuronii pe stratul ascuns",
  "layers": "Specificați numărul de neuroni în fiecare strat, de exemplu 1. Pentru 5 neuroni într-un strat, introduceți 5 2. Pentru 5 neuroni în stratul 1, 6 neuroni în stratul 2, 7 neuroni în stratul 3 introduceți 5,6,7",
  "OutActFunc": "Specificați o funcție de activare a ieșirii",
  "rep": "Repetiții pentru antrenarea rețelei neuronale",
  "label3": "Factori de multiplicare pentru rata de învățare superioară și inferioară",
  "minus": "Superioară (minus)",
  "upper": "Inferioară (plus)",
  "lifesign": "Setarea a cât de mult să se imprime în timpul calculului rețelei neuronale",
  "lifesignstep": "Dimensiunea pasului pentru a imprima pragul minim în modul de semn de viață complet",
  "errfct": "Funcția derivabilă utilizată pentru calculul erorii",
  "linearoutput": "Funcția de activare nu ar trebui să fie aplicată neuronilor de ieșire",
  "likelihood": "Probabilitate",
  "advanced_lbl" : "Avansat",
  "help": {
    "title": "Antrenarea rețelelor neuronale, folosind pachetul neuralnet",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>NOTĂ</b></br>\n            Atunci când specificați o singură variabilă dependentă, aceasta poate fi numerică sau factor. Dacă variabila dependentă specificată este un factor, codificăm automat variabila factor folosind codificarea one-hot utilizând funcția decode din pachetul RSNNS.</br></br>\n            În plus, dacă utilizați codificarea one-hot pentru a codifica o variabilă factor, puteți specifica mai mult de o variabilă dependentă în dialog. În acest caz, variabilele dependente trebuie să fie de tip numeric.</br></br>\n            Puteți folosi \"Date > Calculați variabile dummy\", alegeți setarea „Păstrați toate nivelurile” pentru a obține codificarea one-hot.</br></br>\n            Pentru variabilele dependente de tip factor, vom afișa o matrice de confuzie, ROC și statistici de acuratețe a modelului atunci când evaluăm un set de date folosind modelul construit. Predicțiile generate sunt de tip factor deoarece prezicem clasa. Acestea vor fi salvate în setul de date împreună cu probabilitățile prezise atunci când evaluăm.</br></br>\n            Când există variabile dependente codificate dummy, nu vom afișa o matrice de confuzie, ROC și statistici de acuratețe a modelului atunci când evaluăm un set de date folosind modelul construit. Cu toate acestea, predicțiile vor fi salvate în setul de date atunci când evaluăm setul de date. Predicțiile sunt probabilitățile asociate cu variabilele dependente codificate dummy.</br></br>\n            Este de obicei cel mai bine să standardizăm variabilele independente (de asemenea, trebuie să fie numerice) Vezi „Date > Standardizați variabilele.”</br></br>\n            Dacă aveți variabile independente categorice, utilizați codificarea one-hot pentru a codifica variabilele factor.</br></br>\n            <b>Descriere</b></br>\n            Antrenați rețele neuronale folosind backpropagation, backpropagation rezistent (RPROP) cu (Riedmiller, 1994) sau fără revenirea greutății (Riedmiller și Braun, 1993) sau versiunea modificată global convergentă (GRPROP) de Anastasiadis et al. (2005). Funcția permite setări flexibile prin alegerea personalizată a funcției de eroare și a funcției de activare. În plus, calculul greutăților generalizate (Intrator O. și Intrator N., 1993) este implementat.\n            <b>Utilizare</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>Argumente</b><br/>\n            <ul>\n            <li>\n            formula: o descriere simbolică a modelului care trebuie ajustat.\n            </li>\n            <li>\n            data: un cadru de date care conține variabilele specificate în formulă.\n            </li>\n            <li>\n            hidden: un vector de întregi specificând numărul de neuroni ascunși (vârfuri) în fiecare strat.\n            </li>\n            <li>\n            threshold: o valoare numerică specificând pragul pentru derivatele parțiale ale funcției de eroare ca criteriu de oprire.\n            </li>\n            <li>\n            stepmax: pașii maximali pentru antrenarea rețelei neuronale. Atingerea acestui maxim duce la oprirea procesului de antrenare a rețelei neuronale.\n            </li>\n            <li>\n            rep: numărul de repetări pentru antrenarea rețelei neuronale.\n            </li>\n            <li>\n            startweights: un vector care conține valori de început pentru greutăți. Setați la NULL pentru inițializare aleatorie.\n            </li>\n            <li>\n            learningrate.limit: un vector sau o listă care conține limita minimă și maximă pentru rata de învățare. Utilizat doar pentru RPROP și GRPROP.</li>\n            <li>\n            learningrate.factor: un vector sau o listă care conține factorii de multiplicare pentru rata de învățare superioară și inferioară. Utilizat doar pentru RPROP și GRPROP.\n            </li>\n            <li>\n            learningrate: o valoare numerică specificând rata de învățare utilizată de backpropagation tradițional. Utilizat doar pentru backpropagation tradițional.\n            </li>\n            <li>\n            lifesign: un șir specificând cât de mult va imprima funcția în timpul calculului rețelei neuronale. 'none', 'minimal' sau 'full'.\n            </li>\n            <li>\n            lifesign.step: un întreg specificând dimensiunea pasului pentru a imprima pragul minim în modul de semn de viață complet.\n            </li>\n            <li>\n            algorithm: un șir care conține tipul de algoritm pentru a calcula rețeaua neuronală. Următoarele tipuri sunt posibile: 'backprop', 'rprop+', 'rprop-', 'sag' sau 'slr'. 'backprop' se referă la backpropagation, 'rprop+' și 'rprop-' se referă la backpropagation rezistent cu și fără revenirea greutății, în timp ce 'sag' și 'slr' induc utilizarea algoritmului modificat global convergent (grprop). Vezi Detalii pentru mai multe informații.\n            </li>\n            <li>\n            err.fct: o funcție derivabilă care este utilizată pentru calculul erorii. Alternativ, șirurile 'sse' și 'ce' care reprezintă suma pătratelor erorilor și entropia încrucișată pot fi utilizate.\n            </li>\n            <li>\n            act.fct: o funcție derivabilă care este utilizată pentru netezirea rezultatului produsului încrucișat al covariatelor sau neuronilor și greutăților. În plus, șirurile, 'logistic' și 'tanh' sunt posibile pentru funcția logistică și tangentul hiperbolic.\n            </li>\n            <li>\n            linear.output: logic. Dacă act.fct nu ar trebui să fie aplicată neuronilor de ieșire, setați ieșirea liniară la TRUE, altfel la FALSE.\n            </li>\n            <li>\n            exclude: un vector sau o matrice specificând greutățile, care sunt excluse din calcul. Dacă este dat ca un vector, pozițiile exacte ale greutăților trebuie să fie cunoscute. O matrice cu n-rânduri și 3 coloane va exclude n greutăți, unde prima coloană reprezintă stratul, a doua coloană neuronul de intrare și a treia coloană neuronul de ieșire al greutății.\n            </li>\n            <li>\n            constant.weights: un vector specificând valorile greutăților care sunt excluse din procesul de antrenare și tratate ca fixe.\n            </li>\n            <li>\n            likelihood: logic. Dacă funcția de eroare este egală cu funcția de log-verosimilitate negativă, criteriile de informație AIC și BIC vor fi calculate. În plus, utilizarea intervalului de încredere este semnificativă.\n            </li>\n            </ul>\n            <b>Detalii</b><br/>\n            Algoritmul global convergent se bazează pe backpropagation rezistent fără revenirea greutății și modifică suplimentar o rată de învățare, fie rata de învățare asociată cu cel mai mic gradient absolut (sag) sau cea mai mică rată de învățare (slr) în sine. Ratelor de învățare în algoritmul grprop sunt limitate la limitele definite în learningrate.limit.\n            ​<b>Valoare</b><br/>\n            neuralnet returnează un obiect de clasă nn. Un obiect de clasă nn este o listă care conține cel mult următoarele componente:<br/>\n            call: apelul potrivit.<br/>\n            response: extras din argumentul de date.<br/>\n            covariate: variabilele extrase din argumentul de date.<br/>\n            model.list: o listă care conține covariatele și variabilele de răspuns extrase din argumentul formulă.<br/>\n            err.fct: funcția de eroare.<br/>\n            act.fct: funcția de activare.<br/>\n            data: argumentul de date.<br/>\n            net.result: o listă care conține rezultatul general al rețelei neuronale pentru fiecare repetare.<br/>\n            weights: o listă care conține greutățile ajustate ale rețelei neuronale pentru fiecare repetare.<br/>\n            generalized.weights: o listă care conține greutățile generalizate ale rețelei neuronale pentru fiecare repetare.<br/>\n            result.matrix: o matrice care conține pragul atins, pașii necesari, eroarea, AIC și BIC (dacă sunt calculate) și greutățile pentru fiecare repetare. Fiecare coloană reprezintă o repetare.<br/>\n            startweights: o listă care conține greutățile de început ale rețelei neuronale pentru fiecare repetare.<br/>\n            ​<b>Exemple</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# Clasificare binară\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Nu a fost rulat: print(nn)\n            ## Nu a fost rulat: plot(nn)\n            # Clasificare multiclass\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Nu a fost rulat: print(nn)\n            ## Nu a fost rulat: plot(nn)\n            # Funcție de activare personalizată\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## Nu a fost rulat: print(nn)\n            ## Nu a fost rulat: plot(nn)\n            </code> <br/>\n            <b>Pachet</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>Ajutor</b></br>\n            Pentru ajutor detaliat, faceți clic pe pictograma R din colțul din dreapta sus al acestui overlay de dialog sau rulați următoarea comandă în editorul de sintaxă R</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}