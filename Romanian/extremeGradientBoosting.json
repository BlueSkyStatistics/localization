{
  "title": "Boosting Extrem de Gradient",
  "navigation": "Boosting Extrem de Gradient",
  "label1": "VARIABLES DE FACTOR DE COD DUMMY, VEZI VARIABLES>COMPUTE>DUMMY CODE USE OPTION PĂSTREAZĂ TOATE NIVELURILE. PENTRU PREDICȚIA CLASELOR MULTIPLE, CITESTE AJUTORUL (DA CLICK PE ICONIȚA ? DIN DIALOG).",
  "model": "Introdu numele modelului",
  "dependentvar": "Variabila dependentă",
  "independentvars": "Variabila(e) independent(e)",
  "objective": "Obiectiv",
  "seed": "Setează semințe",
  "nrounds": "Numărul maxim de iterații de boosting",
  "maxdepth": "Adâncimea maximă",
  "minchildweight": "Greutatea minimă a copilului",
  "maxdeltastep": "Pasul maxim delta",
  "eta": "eta (rata de învățare)",
  "gamma": "gamma",
  "numclasses": "Numărul de clase. **Folosește doar cu multi:softmax și multi:softprob",
  "basescore": "Scor de bază",
  "Verbose": "Mod verbose (0=Silenzios, 1=info de performanță, 2=alte informații)",
  "printevery": "Afișează mesajele de evaluare la fiecare n-a iterație când verbose > 0",
  "OptvarTaskparam": "Parametrii sarcinii",
  "OptvarAdvDiagnostics": "Diagnostic avansat",
  "OptvarTreeBoostparam": "Parametrii pentru îmbunătățirea arborilor",  
  "help": {
    "title": "Boosting Extrem de Gradient",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>Descriere</b></br>\n                Creează un model de Boosting Extrem de Gradient\n                <br/>\n                <b>NOTĂ</b></br>\n                1. Pentru a prezice variabila dependentă de tip factor, trebuie să recodifici variabila dependentă într-o variabilă numerică cu valori începând de la 0. De exemplu, dacă există 3 niveluri în variabila factor, variabila numerică va conține valorile 0,1,2.</br>\n                Vezi Date > Recodifică Variabile. Alternativ, convertește pur și simplu variabila factor în numeric, de obicei nivelurile vor fi mapate la întregi începând de la 1 și scade 1 din variabila rezultată. Aceasta îți va oferi o variabilă numerică cu valori începând de la 0.</br>\n                De asemenea, trebuie să setezi obiectivul la multi:softmax (predice clase) sau multi:softprob (predice probabilități) și să introduci numărul de clase în caseta de text pentru numărul de clase.</br>\n                Numărul de clase trebuie introdus doar pentru multi:softmax și multi:softprob. Vor fi generate erori dacă numărul de clase este introdus pentru celelalte obiective.</br>\n                2. Trebuie să codifici dummy variabilele independente de tip factor, folosește codificarea 1-Hot vezi Date > Compute Dummy Variables</br>\n                3. O matrice de confuzie și curba ROC nu sunt generate atunci când obiectivul selectat este unul dintre reg:squarederror, reg:logistic, binary:logitraw și rank:pairwise deoarece funcția predict nu returnează clasa variabilei dependente. Predicțiile numerice (în cazul reg:squarederror), scorurile, rangurile etc. pe care funcția predict le returnează sunt stocate în setul de date. Vezi ajutor(predict.xgb.Booster) pentru mai multe detalii</br>\n                4. O matrice de confuzie nu este generată atunci când obiectivul selectat este multi:softprob deoarece funcția predict returnează probabilitățile prezise și nu clasa variabilei dependente. Probabilitățile prezise sunt salvate în setul de date și curba ROC este generată. Pentru a vedea matricea de confuzie, selectează multi:softmax ca obiectiv.</br>\n                5. O curba ROC nu este generată atunci când obiectivul multi:softmax este selectat deoarece funcția predict returnează clasa și nu probabilitățile prezise. Pentru a vedea curba ROC selectează multi:softprob ca obiectiv.</br>\n                <br/>\n                <b>Utilizare</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Argumente</b><br/>\n                <ul>\n                <li>\n                params: lista de parametrii. Lista completă de parametrii este disponibilă la http://xgboost.readthedocs.io/en/latest/parameter.html. Mai jos este un rezumat mai scurt:<br/>\n                </li>\n                <li>\n                1. Parametrii Generali<br/>\n                Acest dialog folosește gbtree (booster de arbori)<br/>\n                </li>\n                <li>\n                2. Parametrii Booster<br/>\n                2.1. Parametru pentru Booster de Arbori<br/>\n                eta: controlează rata de învățare: scalează contribuția fiecărui arbore printr-un factor de 0 < eta < 1 atunci când este adăugat la aproximația curentă.<br/>\n                Folosit pentru a preveni supraînvățarea prin a face procesul de boosting mai conservator.<br/>\n                O valoare mai mică pentru eta implică o valoare mai mare pentru nrounds: o valoare mică a eta înseamnă că modelul este mai robust la supraînvățare, dar mai lent de calculat. Implicit: 0.3<br/>\n                gamma: reducerea minimă a pierderii necesară pentru a face o partitionare suplimentară pe un nod frunză al arborelui. cu cât este mai mare, cu atât algoritmul va fi mai conservator.<br/>\n                max_depth: adâncimea maximă a unui arbore. Implicit: 6<br/>\n                min_child_weight: suma minimă a greutății instanței (hessian) necesară într-un copil. Dacă pasul de partitionare a arborelui rezultă într-un nod frunză cu suma greutății instanței mai mică decât min_child_weight, atunci procesul de construcție va renunța la partitionarea suplimentară. În modul de regresie liniară, aceasta corespunde pur și simplu numărului minim de instanțe necesare pentru a fi în fiecare nod. Cu cât este mai mare, cu atât algoritmul va fi mai conservator. Implicit: 1<br/>\n                </li>\n                <li>\n                3. Parametrii Sarcinii<br/>\n                obiectiv: specifică sarcina de învățare și obiectivul de învățare corespunzător, utilizatorii pot trece o funcție definită de sine. Opțiunile implicite pentru obiectiv sunt mai jos:<br/>\n                reg:squarederror -Regresie cu pierdere pătratică (Implicit).<br/>\n                reg:logistic regresie logistică.<br/>\n                binary:logistic -regresie logistică pentru clasificarea binară. Probabilitate de ieșire.<br/>\n                binary:logitraw -regresie logistică pentru clasificarea binară, ieșire scor înainte de transformarea logistică.<br/>\n                num_class: setează numărul de clase. Folosește doar cu obiective multiclass.<br/>\n                multi:softmax -setează xgboost pentru a face clasificare multiclass folosind obiectivul softmax. Clasa este reprezentată printr-un număr și ar trebui să fie de la 0 la num_class - 1.<br/>\n                multi:softprob -la fel ca softmax, dar predicția returnează un vector de elemente ndata * nclass, care poate fi ulterior remodelat în matrice ndata, nclass. Rezultatul conține probabilitățile prezise ale fiecărui punct de date aparținând fiecărei clase.<br/>\n                rank:pairwise -setează xgboost pentru a face sarcina de clasificare prin minimizarea pierderii pairwise.<br/>\n                base_score: scorul inițial de predicție al tuturor instanțelor, bias global. Implicit: 0.5<br/>\n                eval_metric: metrici de evaluare pentru datele de validare. Utilizatorii pot trece o funcție definită de sine pentru aceasta. Implicit: metrica va fi atribuită conform obiectivului (rmse pentru regresie,\n                și eroare pentru clasificare, precizia medie pentru clasificare). Lista este furnizată în secțiunea detaliată.<br/>\n                data: set de date de antrenament. xgb.train acceptă doar un xgb.DMatrix ca intrare. xgboost, în plus, acceptă de asemenea matrice, dgCMatrix sau numele unui fișier de date local.<br/>\n                nrounds: numărul maxim de iterații de boosting.<br/>\n                verbose: Dacă 0, xgboost va rămâne tăcut. Dacă 1, va imprima informații despre performanță. Dacă 2, vor fi imprimate unele informații suplimentare. Reține că setarea verbose > 0 angajează automat funcția de callback cb.print.evaluation(period=1).<br/>\n                print_every_n: Afișează mesajele de evaluare la fiecare n-a iterație când verbose>0. Implicit este 1, ceea ce înseamnă că toate mesajele sunt afișate. Acest parametru este trecut funcției de callback cb.print.evaluation.<br/>\n                label: vector de valori de răspuns. Nu ar trebui să fie furnizat atunci când datele sunt un nume de fișier de date local sau un xgb.DMatrix.<br/>\n                missing: implicit este setat la NA, ceea ce înseamnă că valorile NA ar trebui considerate ca 'lipsă' de algoritm. Uneori, 0 sau altă valoare extremă ar putea fi folosită pentru a reprezenta valorile lipsă. Acest parametru este folosit doar atunci când intrarea este o matrice densă.<br/>\n                </li>\n                </ul>\n                <b>Detalii</b></br>\n                Metrica de evaluare este aleasă automat de Xgboost (conform obiectivului) atunci când parametrul eval_metric nu este furnizat.</br>\n                Utilizatorul poate seta unul sau mai multe parametrii eval_metric. Reține că atunci când folosești o metrică personalizată, doar această metrică unică poate fi folosită. Următoarea este lista de metrici încorporate pentru care Xgboost oferă o implementare optimizată:</br>\n                rmse eroarea pătrată medie. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss log-verosimilitatea negativă. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss log-verosimilitatea multiclass. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error Rata de eroare a clasificării binare. Este calculată ca (# cazuri greșite) / (# toate cazurile). Implicit, folosește pragul de 0.5 pentru valorile prezise pentru a defini instanțele negative și pozitive.</br>\n                Un prag diferit (de exemplu, 0.) ar putea fi specificat ca \"eroare@0.\"</br>\n                merror Rata de eroare a clasificării multiclass. Este calculată ca (# cazuri greșite) / (# toate cazurile).</br>\n                auc Zona sub curbă. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve pentru evaluarea clasificării.</br>\n                aucpr Zona sub curba PR. https://en.wikipedia.org/wiki/Precision_and_recall pentru evaluarea clasificării.</br>\n                ndcg Câștig Cumulat Normalizat Discountat (pentru sarcina de clasificare). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                Următoarele callback-uri sunt create automat atunci când anumite parametrii sunt setați:</br>\n                cb.print.evaluation este activat când verbose > 0; și parametrul print_every_n este trecut la acesta.</br>\n                cb.evaluation.log este activat când watchlist este prezent.</br>\n                cb.early.stop: când early_stopping_rounds este setat.</br>\n                cb.save.model: când save_period > 0 este setat.</br></br>\n                <b>Valoare</b></br>\n                Un obiect de clasă xgb.Booster cu următoarele elemente:</br>\n                handle un handle (pointer) către modelul xgboost în memorie.</br>\n                raw o copie a memoriei cache a modelului xgboost salvat ca tip raw al R.</br>\n                niter: numărul de iterații de boosting.</br>\n                evaluation_log: istoricul evaluării stocat ca un data.table cu prima coloană corespunzătoare numărului de iterație și restul corespunzând valorilor metricilor de evaluare. Este creat de callback-ul cb.evaluation.log.</br>\n                call: o apelare a funcției.</br>\n                params: parametrii care au fost trecuți bibliotecii xgboost. Reține că nu capturează parametrii schimbați de callback-ul cb.reset.parameters.</br>\n                callbacks funcții de callback care au fost fie atribuite automat, fie trecute explicit.</br>\n                best_iteration: numărul de iterație cu cea mai bună valoare a metricii de evaluare (disponibilă doar cu oprirea timpurie).</br>\n                best_ntreelimit: valoarea ntreelimit corespunzătoare celei mai bune iterații, care ar putea fi folosită ulterior în metoda predict (disponibilă doar cu oprirea timpurie).</br>\n                best_score: cea mai bună valoare a metricii de evaluare în timpul opririi timpurii. (disponibilă doar cu oprirea timpurie).</br>\n                feature_names: numele caracteristicilor setului de date de antrenament (doar când numele coloanelor au fost definite în datele de antrenament).</br>\n                nfeatures: numărul de caracteristici în datele de antrenament.</br>\n                <b>Referințe</b></br>\n                Tianqi Chen și Carlos Guestrin, \"XGBoost: Un sistem de Boosting de Arbori Scalabil\", Conferința SIGKDD 22 despre Descoperirea Cunoștințelor și Minarea Datelor, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>Vezi De asemenea</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Exemple</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## Un exemplu simplu xgb.train:</br>\n                # Aceste funcții ar putea fi folosite trecându-le fie:</br>\n                ## Un exemplu de interfață 'xgboost':</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}