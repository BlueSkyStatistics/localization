{
  "title": "다층 퍼셉트론, RSNNS 패키지 사용",
  "navigation": "다층 퍼셉트론",
  "label1": "더미 코드 팩터 변수를 입력하세요. 데이터 > 더미 변수 계산(모든 레벨 유지, 즉 1 핫 인코딩). 숫자 변수를 스케일링하고 중심화하세요. 데이터 > 변수 표준화 참조",
  "model": "모델 이름을 입력하세요",
  "dependentvar": "종속 변수",
  "independentvars": "독립 변수(들)",
  "seed": "시드 설정",
  "iter": "학습을 위한 최대 반복 횟수",
  "tf": "학습 함수",
  "label2": "숨겨진 층의 수와 각 숨겨진 층의 뉴런 수",
  "layers": "각 층의 뉴런 수를 지정하세요. 예: 1. 1층에 5개의 뉴런을 입력하려면 5를 입력하세요. 1층에 5개, 2층에 6개, 3층에 7개의 뉴런을 입력하려면 5,6,7을 입력하세요.",
  "learnfuncparams": "학습 함수 매개변수",
  "help": {
    "title": "다층 퍼셉트론, RSNNS 패키지 사용",
    "r_help": "help(mlp, package ='RSNNS')",
    "body": "\n            <b>참고</b></br>\n            단일 종속 변수를 지정할 때, 숫자형 또는 팩터형일 수 있습니다. 지정된 종속 변수가 팩터인 경우, RSNNS 패키지의 decode 함수를 사용하여 더미 코딩을 자동으로 수행합니다.</br></br>\n            또한, 팩터 변수를 더미 코드화하기 위해 원-핫 인코딩을 사용하는 경우, 대화 상자에서 둘 이상의 종속 변수를 지정할 수 있습니다. 이 경우, 종속 변수는 숫자형이어야 합니다.</br></br>\n            \"데이터 > 더미 변수 계산\"을 사용하여 \"모든 레벨 유지\" 설정을 선택하여 원-핫 인코딩을 얻을 수 있습니다.</br></br>\n            팩터형 종속 변수의 경우, 모델을 사용하여 데이터 세트를 점수화할 때 혼동 행렬, ROC 및 모델 정확도 통계를 표시합니다. 생성된 예측은 클래스 예측이므로 팩터형입니다. 이들은 점수화할 때 예측 확률과 함께 데이터 세트에 저장됩니다.</br></br>\n            더미 코드화된 종속 변수가 있는 경우, 모델을 사용하여 데이터 세트를 점수화할 때 혼동 행렬, ROC 및 모델 정확도 통계를 표시하지 않습니다. 그러나 점수화할 때 예측은 데이터 세트에 저장됩니다. 예측은 더미 코드화된 종속 변수와 관련된 확률입니다.</br></br>\n            독립 변수를 표준화하는 것이 일반적으로 가장 좋습니다(숫자형이어야 함). \"데이터 > 변수 표준화\"를 참조하세요.</br></br>\n            범주형 독립 변수가 있는 경우, 팩터 변수를 더미 코드화하기 위해 원-핫 인코딩을 사용하세요.</br></br>\n            <b>설명</b></br>\n            이 함수는 다층 퍼셉트론(MLP)을 생성하고 훈련합니다. MLP는 완전히 연결된 피드포워드 네트워크이며, 아마도 가장 일반적인 네트워크 아키텍처입니다. 훈련은 일반적으로 오류 역전파 또는 관련 절차에 의해 수행됩니다.</br>\n            SNNS에는 이 함수와 함께 사용할 수 있는 다양한 학습 함수가 있습니다. 예: Std_Backpropagation, BackpropBatch, BackpropChunk, BackpropMomentum, BackpropWeightDecay, Rprop, Quickprop, SCG(스케일된 공액 기울기), ...</br>\n            <b>사용법</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## 기본 S3 메서드:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>인수</b><br/>\n            <ul>\n            <li>\n            x: 네트워크의 훈련 입력을 위한 행렬\n            </li>\n            <li>\n            ... : 추가 함수 매개변수(현재 사용되지 않음)\n            </li>\n            <li>\n            y: 해당하는 목표 값\n            </li>\n            <li>\n            size: 숨겨진 층의 유닛 수\n            </li>\n            <li>\n            maxit: 학습을 위한 최대 반복 횟수\n            </li>\n            <li>\n            initFunc: 사용할 초기화 함수\n            </li>\n            <li>\n            initFuncParams: 초기화 함수의 매개변수\n            </li>\n            <li>\n            learnFunc: 사용할 학습 함수\n            </li>\n            <li>\n            learnFuncParams: 학습 함수의 매개변수\n            </li>\n            <li>\n            updateFunc: 사용할 업데이트 함수\n            </li>\n            <li>\n            updateFuncParams: 업데이트 함수의 매개변수\n            </li>\n            <li>\n            hiddenActFunc: 모든 숨겨진 유닛의 활성화 함수\n            </li>\n            <li>\n            shufflePatterns: 패턴을 섞어야 합니까?\n            </li>\n            <li>\n            linOut: 출력 유닛의 활성화 함수를 선형 또는 로지스틱으로 설정합니다(출력ActFunc가 주어지면 무시됨)\n            </li>\n            <li>\n            outputActFunc: 모든 출력 유닛의 활성화 함수\n            </li>\n            <li>\n            inputsTest: 네트워크를 테스트할 입력 행렬\n            </li>\n            <li>\n            targetsTest: 테스트 입력에 대한 해당 목표\n            </li>\n            <li>\n            pruneFunc: 사용할 가지치기 함수\n            </li>\n            <li>\n            pruneFuncParams: 가지치기 함수의 매개변수. 다른 함수와 달리, 이들은 명명된 목록으로 제공되어야 합니다. 추가 설명은 가지치기 데모를 참조하세요.\n            </li>\n            </ul>\n            <b>세부사항</b></br>\n            Std_Backpropagation, BackpropBatch 등은 두 개의 매개변수, 즉 학습률과 최대 출력 차이를 가집니다. 학습률은 일반적으로 0.1과 1 사이의 값입니다. 이는 경량 하강 단계 너비를 지정합니다. 최대 차이는 출력과 목표 값 간의 차이가 제로 오류로 처리되는 정도를 정의하며, 역전파되지 않습니다. 이 매개변수는 과훈련을 방지하는 데 사용됩니다. 모든 학습 함수의 매개변수에 대한 전체 목록은 SNNS 사용자 매뉴얼, pp. 67을 참조하세요.</br>\n            초기화 및 업데이트 함수에 대해 설정된 기본값은 일반적으로 변경할 필요가 없습니다.</br>\n            <b>값</b><br/>\n            rsnns 객체입니다.\n            <b>참고문헌</b><br/>\n            Rosenblatt, F. (1958), '퍼셉트론: 뇌에서 정보 저장 및 조직을 위한 확률적 모델', Psychological Review 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Group, P. R. (1986), Parallel distributed processing :explorations in the microstructure of cognition, Mit, Cambridge, MA 등.<br/>\n            Zell, A. et al. (1998), 'SNNS 슈투트가르트 신경망 시뮬레이터 사용자 매뉴얼, 버전 4.2', IPVR, 슈투트가르트 대학교 및 튀빙겐 대학교 WSI.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (독일어)<br/>\n            <b>예제</b><br/>\n            <code> \n            data(iris)<br/>\n            #벡터 섞기<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #402040 방법으로 혼동 행렬<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>패키지</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>도움말</b></br>\n            자세한 도움말은 이 대화 상자의 오른쪽 상단에 있는 R 아이콘을 클릭하거나 R 구문 편집기에서 다음 명령을 실행하세요.</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
  }
}