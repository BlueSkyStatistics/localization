{
  "title": "신경망 훈련, neuralnet 패키지 사용",
  "navigation": "신경망",
  "label1": "더미 코드 팩터 변수를 작성하십시오. 데이터 > 더미 변수 계산(모든 레벨 유지, 즉 1 핫 인코딩). 숫자 변수를 스케일링하고 중심화하십시오. 데이터 > 변수 표준화 참조",
  "model": "모델 이름 입력",
  "dependentvar": "종속 변수",
  "independentvars": "독립 변수(들)",
  "seed": "시드 설정",
  "iter": "학습을 위한 최대 단계",
  "tf": "알고리즘",
  "threshold": "임계값",
  "label2": "은닉층 수 및 각 은닉층의 뉴런 수",
  "layers": "각 층의 뉴런 수를 지정하십시오. 예: 1. 1층에 5개의 뉴런을 원하면 5를 입력하십시오. 1층에 5개, 2층에 6개, 3층에 7개 뉴런을 원하면 5,6,7을 입력하십시오.",
  "OutActFunc": "출력 활성화 함수를 지정하십시오.",
  "rep": "신경망 훈련을 위한 반복 횟수",
  "label3": "상위 및 하위 학습률에 대한 곱셈 계수",
  "minus": "상위(마이너스)",
  "upper": "하위(플러스)",
  "lifesign": "신경망 계산 중 출력할 양 설정",
  "lifesignstep": "전체 생명 신호 모드에서 최소 임계값을 인쇄할 단계 크기",
  "errfct": "오류 계산에 사용되는 미분 가능 함수",
  "linearoutput": "출력 뉴런에 활성화 함수를 적용하지 않아야 합니다.",
  "likelihood": "가능성",
  "advanced_lbl": "고급",
  "help": {
    "title": "신경망 훈련, neuralnet 패키지 사용",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>참고</b></br>\n            단일 종속 변수를 지정할 때 숫자 또는 팩터일 수 있습니다. 지정된 종속 변수가 팩터인 경우, RSNNS 패키지의 decode 함수를 사용하여 더미 코드를 자동으로 생성합니다.</br></br>\n            또한, 팩터 변수를 더미 코드화하기 위해 원-핫 인코딩을 사용하는 경우, 대화 상자에서 둘 이상의 종속 변수를 지정할 수 있습니다. 이 경우 종속 변수는 숫자형이어야 합니다.</br></br>\n            \"데이터 > 더미 변수 계산\"을 사용하여 \"모든 레벨 유지\" 설정을 선택하여 원-핫 인코딩을 얻을 수 있습니다.</br></br>\n            팩터 유형의 종속 변수에 대해, 모델을 사용하여 데이터 세트를 점수화할 때 혼동 행렬, ROC 및 모델 정확도 통계를 표시합니다. 생성된 예측은 클래스 예측이므로 팩터 유형입니다. 이들은 점수화할 때 예측된 확률과 함께 데이터 세트에 저장됩니다.</br></br>\n            더미 코드화된 종속 변수가 있는 경우, 모델을 사용하여 데이터 세트를 점수화할 때 혼동 행렬, ROC 및 모델 정확도 통계를 표시하지 않습니다. 그러나 점수화할 때 예측은 데이터 세트에 저장됩니다. 예측은 더미 코드화된 종속 변수와 관련된 확률입니다.</br></br>\n            독립 변수를 표준화하는 것이 일반적으로 가장 좋습니다(숫자형이어야 함). \"데이터 > 변수 표준화\"를 참조하십시오.</br></br>\n            범주형 독립 변수가 있는 경우, 팩터 변수를 더미 코드화하기 위해 원-핫 인코딩을 사용하십시오.</br></br>\n            <b>설명</b></br>\n            역전파, 탄력적 역전파(RPROP) 및 수정된 전역 수렴 버전(GRPROP)을 사용하여 신경망을 훈련합니다. 이 함수는 오류 및 활성화 함수의 사용자 정의 선택을 통해 유연한 설정을 허용합니다. 또한 일반화된 가중치 계산이 구현되어 있습니다.\n            <b>사용법</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>인수</b><br/>\n            <ul>\n            <li>\n            formula: 적합할 모델의 기호적 설명.\n            </li>\n            <li>\n            data: formula에 지정된 변수를 포함하는 데이터 프레임.\n            </li>\n            <li>\n            hidden: 각 층의 은닉 뉴런(정점) 수를 지정하는 정수 벡터.\n            </li>\n            <li>\n            threshold: 부분 도함수의 임계값을 지정하는 숫자 값.\n            </li>\n            <li>\n            stepmax: 신경망 훈련을 위한 최대 단계 수. 이 최대 단계에 도달하면 신경망 훈련 프로세스가 중지됩니다.\n            </li>\n            <li>\n            rep: 신경망 훈련을 위한 반복 횟수.\n            </li>\n            <li>\n            startweights: 가중치의 시작 값을 포함하는 벡터. 무작위 초기화를 위해 NULL로 설정하십시오.\n            </li>\n            <li>\n            learningrate.limit: 학습률의 최저 및 최고 한계를 포함하는 벡터 또는 리스트. RPROP 및 GRPROP에만 사용됩니다.</li>\n            <li>\n            learningrate.factor: 상위 및 하위 학습률에 대한 곱셈 계수를 포함하는 벡터 또는 리스트. RPROP 및 GRPROP에만 사용됩니다.\n            </li>\n            <li>\n            learningrate: 전통적인 역전파에 의해 사용되는 학습률을 지정하는 숫자 값. 전통적인 역전파에만 사용됩니다.\n            </li>\n            <li>\n            lifesign: 신경망 계산 중 얼마나 출력할지를 지정하는 문자열. 'none', 'minimal' 또는 'full'.\n            </li>\n            <li>\n            lifesign.step: 전체 생명 신호 모드에서 최소 임계값을 인쇄할 단계 크기를 지정하는 정수.\n            </li>\n            <li>\n            algorithm: 신경망을 계산하기 위한 알고리즘 유형을 포함하는 문자열. 가능한 유형은 'backprop', 'rprop+', 'rprop-', 'sag', 또는 'slr'입니다. 'backprop'은 역전파를 의미하고, 'rprop+' 및 'rprop-'는 가중치 백트래킹이 있는 탄력적 역전파를 의미하며, 'sag' 및 'slr'은 수정된 전역 수렴 알고리즘(grprop)의 사용을 유도합니다. 자세한 내용은 세부정보를 참조하십시오.\n            </li>\n            <li>\n            err.fct: 오류 계산에 사용되는 미분 가능 함수. 대안으로, 'sse' 및 'ce' 문자열을 사용할 수 있습니다.\n            </li>\n            <li>\n            act.fct: 공변량 또는 뉴런과 가중치의 교차 곱의 결과를 부드럽게 하는 데 사용되는 미분 가능 함수. 또한 'logistic' 및 'tanh' 문자열이 가능하여 로지스틱 함수 및 탄젠트 하이퍼볼릭을 사용할 수 있습니다.\n            </li>\n            <li>\n            linear.output: 논리적. act.fct가 출력 뉴런에 적용되지 않도록 하려면 linear output을 TRUE로 설정하고, 그렇지 않으면 FALSE로 설정하십시오.\n            </li>\n            <li>\n            exclude: 계산에서 제외할 가중치를 지정하는 벡터 또는 행렬. 벡터로 제공되는 경우, 가중치의 정확한 위치를 알아야 합니다. n-행과 3열의 행렬은 n개의 가중치를 제외하며, 첫 번째 열은 층, 두 번째 열은 입력 뉴런, 세 번째 열은 가중치의 출력 뉴런을 나타냅니다.\n            </li>\n            <li>\n            constant.weights: 훈련 과정에서 제외되고 고정으로 처리되는 가중치의 값을 지정하는 벡터.\n            </li>\n            <li>\n            likelihood: 논리적. 오류 함수가 음의 로그 가능성 함수와 같으면 정보 기준 AIC 및 BIC가 계산됩니다. 또한 신뢰 구간의 사용이 의미가 있습니다.\n            </li>\n            </ul>\n            <b>세부정보</b><br/>\n            전역 수렴 알고리즘은 가중치 백트래킹이 없는 탄력적 역전파를 기반으로 하며, 추가로 가장 작은 절대 기울기(sag) 또는 가장 작은 학습률(sl)과 관련된 학습률을 수정합니다. grprop 알고리즘의 학습률은 learningrate.limit에 정의된 경계로 제한됩니다.\n            ​<b>값</b><br/>\n            neuralnet는 nn 클래스의 객체를 반환합니다. nn 클래스의 객체는 다음 구성 요소를 포함하는 리스트입니다:<br/>\n            call: 일치하는 호출.<br/>\n            response: 데이터 인수에서 추출됨.<br/>\n            covariate: 데이터 인수에서 추출된 변수.<br/>\n            model.list: formula 인수에서 추출된 공변량 및 응답 변수를 포함하는 리스트.<br/>\n            err.fct: 오류 함수.<br/>\n            act.fct: 활성화 함수.<br/>\n            data: 데이터 인수.<br/>\n            net.result: 각 반복에 대한 신경망의 전체 결과를 포함하는 리스트.<br/>\n            weights: 각 반복에 대한 신경망의 적합된 가중치를 포함하는 리스트.<br/>\n            generalized.weights: 각 반복에 대한 신경망의 일반화된 가중치를 포함하는 리스트.<br/>\n            result.matrix: 각 반복에 대한 도달한 임계값, 필요한 단계, 오류, AIC 및 BIC(계산된 경우) 및 가중치를 포함하는 행렬. 각 열은 하나의 반복을 나타냅니다.<br/>\n            startweights: 각 반복에 대한 신경망의 시작 가중치를 포함하는 리스트.<br/>\n            ​<b>예제</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# 이진 분류\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## 실행되지 않음: print(nn)\n            ## 실행되지 않음: plot(nn)\n            # 다중 클래스 분류\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## 실행되지 않음: print(nn)\n            ## 실행되지 않음: plot(nn)\n            # 사용자 정의 활성화 함수\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## 실행되지 않음: print(nn)\n            ## 실행되지 않음: plot(nn)\n            </code> <br/>\n            <b>패키지</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>도움말</b></br>\n            자세한 도움말은 이 대화 상자의 오른쪽 상단에 있는 R 아이콘을 클릭하거나 R 구문 편집기에서 다음 명령을 실행하십시오.</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}