{
  "title": "극단적 경량 부스팅",
  "navigation": "극단적 경량 부스팅",
  "label1": "더미 코드 요인 변수, 변수>계산>더미 코드 사용 옵션에서 모든 수준 유지. 여러 클래스를 예측하려면 도움말을 읽으십시오 (대화 상자에서 ? 아이콘 클릭).",
  "model": "모델 이름 입력",
  "dependentvar": "종속 변수",
  "independentvars": "독립 변수(들)",
  "objective": "목표",
  "seed": "시드 설정",
  "nrounds": "최대 부스팅 반복 횟수",
  "maxdepth": "최대 깊이",
  "minchildweight": "최소 자식 가중치",
  "maxdeltastep": "최대 델타 단계",
  "eta": "eta (학습률)",
  "gamma": "감마",
  "numclasses": "클래스 수. **multi:softmax 및 multi:softprob와만 사용하십시오.",
  "basescore": "기본 점수",
  "Verbose": "상세 모드 (0=침묵, 1=성능 정보, 2=기타 정보)",
  "printevery": "상세 모드 > 0일 때 n번째 반복 평가 메시지 인쇄",
  "OptvarTaskparam": "작업 매개변수",
  "OptvarAdvDiagnostics": "고급 진단",
  "OptvarTreeBoostparam": "트리 부스팅 매개변수",
  "help": {
    "title": "극단적 경량 부스팅",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>설명</b></br>\n                극단적 경량 부스팅 모델 생성\n                <br/>\n                <b>참고</b></br>\n                1. 요인 유형의 종속 변수를 예측하려면 종속 변수를 0부터 시작하는 숫자로 다시 인코딩해야 합니다. 예를 들어, 요인 변수에 3개의 수준이 있는 경우 숫자 변수는 0,1,2의 값을 포함합니다.</br>\n                데이터 > 변수 재코드 참조. 또는 요인 변수를 숫자로 변환하면 일반적으로 수준이 1부터 시작하는 정수로 매핑되고 결과 변수에서 1을 빼야 합니다. 이렇게 하면 0부터 시작하는 숫자 변수를 얻을 수 있습니다.</br>\n                또한 목표를 multi:softmax (클래스 예측) 또는 multi:softprob (확률 예측)으로 설정하고 클래스 수를 클래스 수 텍스트 상자에 입력해야 합니다.</br>\n                클래스 수는 multi:softmax 및 multi:softprob에 대해서만 입력해야 합니다. 다른 목표에 대해 클래스 수를 입력하면 오류가 발생합니다.</br>\n                2. 독립 요인 변수를 더미 코드화해야 하며, 1-Hot 인코딩을 사용하여 데이터 > 더미 변수 계산을 참조하십시오.</br>\n                3. 선택한 목표가 reg:squarederror, reg:logistic, binary:logitraw 및 rank:pairwise 중 하나일 때 혼동 행렬 및 ROC 곡선이 생성되지 않습니다. 예측 함수는 종속 변수의 클래스를 반환하지 않기 때문입니다. 예측 함수가 반환하는 숫자 예측(예: reg:squarederror), 점수, 순위 등은 데이터 세트에 저장됩니다. 자세한 내용은 help(predict.xgb.Booster)를 참조하십시오.</br>\n                4. 선택한 목표가 multi:softprob일 때 혼동 행렬이 생성되지 않습니다. 예측 함수는 예측된 확률을 반환하고 종속 변수의 클래스를 반환하지 않기 때문입니다. 예측된 확률은 데이터 세트에 저장되며 ROC 곡선이 생성됩니다. 혼동 행렬을 보려면 목표로 multi:softmax를 선택하십시오.</br>\n                5. 선택한 목표가 multi:softmax일 때 ROC 곡선이 생성되지 않습니다. 예측 함수는 클래스를 반환하고 예측된 확률을 반환하지 않기 때문입니다. ROC 곡선을 보려면 목표로 multi:softprob를 선택하십시오.</br>\n                <br/>\n                <b>사용법</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>인수</b><br/>\n                <ul>\n                <li>\n                params: 매개변수 목록. 매개변수의 전체 목록은 http://xgboost.readthedocs.io/en/latest/parameter.html에서 확인할 수 있습니다. 아래는 간단한 요약입니다:<br/>\n                </li>\n                <li>\n                1. 일반 매개변수<br/>\n                이 대화 상자는 gbtree (트리 부스터)를 사용합니다.<br/>\n                </li>\n                <li>\n                2. 부스터 매개변수<br/>\n                2.1. 트리 부스터 매개변수<br/>\n                eta: 학습률을 제어합니다: 현재 근사치에 추가될 때 각 트리의 기여도를 0 < eta < 1의 계수로 조정합니다.<br/>\n                과적합을 방지하기 위해 부스팅 프로세스를 보다 보수적으로 만듭니다.<br/>\n                eta 값이 낮을수록 nrounds 값이 커집니다: 낮은 eta 값은 과적합에 더 강한 모델을 의미하지만 계산 속도가 느려집니다. 기본값: 0.3<br/>\n                gamma: 리프 노드에서 추가 분할을 수행하기 위해 필요한 최소 손실 감소. 값이 클수록 알고리즘이 더 보수적입니다.<br/>\n                max_depth: 트리의 최대 깊이. 기본값: 6<br/>\n                min_child_weight: 자식에서 필요한 인스턴스 가중치(헤시안)의 최소 합계. 트리 분할 단계가 min_child_weight보다 작은 인스턴스 가중치의 합계를 가진 리프 노드를 생성하면 빌딩 프로세스는 추가 분할을 포기합니다. 선형 회귀 모드에서는 각 노드에 있어야 하는 최소 인스턴스 수에 해당합니다. 값이 클수록 알고리즘이 더 보수적입니다. 기본값: 1<br/>\n                </li>\n                <li>\n                3. 작업 매개변수<br/>\n                objective: 학습 작업 및 해당 학습 목표를 지정합니다. 사용자는 사용자 정의 함수를 전달할 수 있습니다. 기본 목표 옵션은 아래와 같습니다:<br/>\n                reg:squarederror - 제곱 손실을 가진 회귀 (기본값).<br/>\n                reg:logistic 로지스틱 회귀.<br/>\n                binary:logistic - 이진 분류를 위한 로지스틱 회귀. 출력 확률.<br/>\n                binary:logitraw - 이진 분류를 위한 로지스틱 회귀, 로지스틱 변환 이전의 점수 출력.<br/>\n                num_class: 클래스 수를 설정합니다. 다중 클래스 목표와만 사용하십시오.<br/>\n                multi:softmax - xgboost를 다중 클래스 분류를 수행하도록 설정합니다. 클래스는 숫자로 표현되며 0에서 num_class - 1까지의 값이어야 합니다.<br/>\n                multi:softprob - softmax와 동일하지만 예측은 ndata * nclass 요소의 벡터를 출력하며, 이는 ndata, nclass 행렬로 다시 형성될 수 있습니다. 결과는 각 데이터 포인트가 각 클래스에 속할 확률을 포함합니다.<br/>\n                rank:pairwise - xgboost를 쌍별 손실을 최소화하여 순위 작업을 수행하도록 설정합니다.<br/>\n                base_score: 모든 인스턴스의 초기 예측 점수, 전역 편향. 기본값: 0.5<br/>\n                eval_metric: 검증 데이터에 대한 평가 메트릭. 사용자는 사용자 정의 함수를 전달할 수 있습니다. 기본값: 메트릭은 목표에 따라 할당됩니다(회귀의 경우 rmse,\n                분류의 경우 오류, 순위의 경우 평균 정밀도). 자세한 목록은 세부 섹션에 제공됩니다.<br/>\n                data: 훈련 데이터 세트. xgb.train은 입력으로 xgb.DMatrix만 허용합니다. xgboost는 추가로 행렬, dgCMatrix 또는 로컬 데이터 파일의 이름도 허용합니다.<br/>\n                nrounds: 최대 부스팅 반복 횟수.<br/>\n                verbose: 0이면 xgboost는 침묵합니다. 1이면 성능에 대한 정보를 인쇄합니다. 2이면 추가 정보가 인쇄됩니다. verbose > 0으로 설정하면 자동으로 cb.print.evaluation(period=1) 콜백 함수가 활성화됩니다.<br/>\n                print_every_n: verbose > 0일 때 n번째 반복 평가 메시지를 인쇄합니다. 기본값은 1로 모든 메시지가 인쇄됩니다. 이 매개변수는 cb.print.evaluation 콜백에 전달됩니다.<br/>\n                label: 응답 값의 벡터. 데이터가 로컬 데이터 파일 이름 또는 xgb.DMatrix인 경우 제공하지 않아야 합니다.<br/>\n                missing: 기본적으로 NA로 설정되어 있으며, 이는 알고리즘에서 NA 값을 '누락'으로 간주해야 함을 의미합니다. 때때로 0 또는 다른 극단적인 값이 누락된 값을 나타내는 데 사용될 수 있습니다. 이 매개변수는 입력이 밀집 행렬일 때만 사용됩니다.<br/>\n                </li>\n                </ul>\n                <b>세부정보</b></br>\n                평가 메트릭은 eval_metric 매개변수가 제공되지 않을 때 Xgboost에 의해 자동으로 선택됩니다.</br>\n                사용자는 하나 이상의 eval_metric 매개변수를 설정할 수 있습니다. 사용자 정의 메트릭을 사용할 때는 이 단일 메트릭만 사용할 수 있습니다. Xgboost가 최적화된 구현을 제공하는 내장 메트릭 목록은 다음과 같습니다:</br>\n                rmse 평균 제곱근 오차. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss 음의 로그 가능도. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss 다중 클래스 로그 손실. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error 이진 분류 오류율. 이는 (# 잘못된 사례) / (# 모든 사례)로 계산됩니다. 기본적으로 예측 값에 대해 0.5 임계값을 사용하여 부정 및 긍정 인스턴스를 정의합니다.</br>\n                다른 임계값(예: 0.)은 \"error@0.\"으로 지정할 수 있습니다.</br>\n                merror 다중 클래스 분류 오류율. 이는 (# 잘못된 사례) / (# 모든 사례)로 계산됩니다.</br>\n                auc 곡선 아래 면적. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve 순위 평가를 위한.</br>\n                aucpr PR 곡선 아래 면적. https://en.wikipedia.org/wiki/Precision_and_recall 순위 평가를 위한.</br>\n                ndcg 정규화된 할인 누적 이득 (순위 작업을 위한). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                특정 매개변수가 설정되면 자동으로 생성되는 콜백은 다음과 같습니다:</br>\n                cb.print.evaluation은 verbose > 0일 때 활성화됩니다. print_every_n 매개변수가 전달됩니다.</br>\n                cb.evaluation.log은 watchlist가 있을 때 활성화됩니다.</br>\n                cb.early.stop: early_stopping_rounds가 설정될 때.</br>\n                cb.save.model: save_period > 0이 설정될 때.</br></br>\n                <b>값</b></br>\n                다음 요소가 포함된 xgb.Booster 클래스의 객체:</br>\n                handle 메모리의 xgboost 모델에 대한 핸들(포인터).</br>\n                raw R의 원시 유형으로 저장된 xgboost 모델의 캐시된 메모리 덤프.</br>\n                niter: 부스팅 반복 횟수.</br>\n                evaluation_log: 첫 번째 열은 반복 번호에 해당하고 나머지는 평가 메트릭 값에 해당하는 데이터 테이블로 저장된 평가 기록. cb.evaluation.log 콜백에 의해 생성됩니다.</br>\n                call: 함수 호출.</br>\n                params: xgboost 라이브러리에 전달된 매개변수. cb.reset.parameters 콜백에 의해 변경된 매개변수는 캡처하지 않습니다.</br>\n                callbacks 자동으로 할당되거나 명시적으로 전달된 콜백 함수.</br>\n                best_iteration: 최상의 평가 메트릭 값을 가진 반복 번호 (조기 중지 시에만 사용 가능).</br>\n                best_ntreelimit: 최상의 반복에 해당하는 ntreelimit 값, 이는 예측 메서드에서 추가로 사용될 수 있습니다 (조기 중지 시에만 사용 가능).</br>\n                best_score: 조기 중지 동안 최상의 평가 메트릭 값. (조기 중지 시에만 사용 가능).</br>\n                feature_names: 훈련 데이터 세트의 특징 이름 (훈련 데이터에서 열 이름이 정의된 경우에만).</br>\n                nfeatures: 훈련 데이터의 특징 수.</br>\n                <b>참조</b></br>\n                Tianqi Chen과 Carlos Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 제22회 SIGKDD 지식 발견 및 데이터 마이닝 회의, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>참고 사항</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>예제</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## 간단한 xgb.train 예제:</br>\n                # 이러한 함수는 다음과 같이 전달하여 사용할 수 있습니다:</br>\n                ## 'xgboost' 인터페이스 예:</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}