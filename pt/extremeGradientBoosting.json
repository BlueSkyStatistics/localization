{
  "title": "Aumento de Gradiente Extremo",
  "navigation": "Aumento de Gradiente Extremo",
  "label1": "VARIÁVEIS DE FATOR DE CÓDIGO DUMMY, VEJA VARIÁVEIS>COMPUTAR>OPÇÃO DE USO DE CÓDIGO DUMMY MANTER TODOS OS NÍVEIS. PARA PREDIZER MÚLTIPLAS CLASSES, LEIA AJUDA (CLIQUE NO ÍCONE ? NO DIÁLOGO).",
  "model": "Insira o nome do modelo",
  "dependentvar": "Variável dependente",
  "independentvars": "Variável(is) independente(s)",
  "objective": "Objetivo",
  "seed": "Definir semente",
  "nrounds": "Número máximo de iterações de aumento",
  "maxdepth": "Profundidade máxima",
  "minchildweight": "Peso mínimo da criança",
  "maxdeltastep": "Máximo passo delta",
  "eta": "eta (taxa de aprendizado)",
  "gamma": "gamma",
  "numclasses": "Número de classes. **Use apenas com multi:softmax e multi:softprob",
  "basescore": "Pontuação base",
  "Verbose": "Modo Verbose (0=Silencioso, 1=informações de desempenho, 2=outras informações)",
  "printevery": "Imprimir mensagens de avaliação de cada n-ésima iteração quando verbose > 0",
  "OptvarTaskparam": "Parâmetros da Tarefa",
  "OptvarAdvDiagnostics": "Diagnósticos Avançados",
  "OptvarTreeBoostparam": "Parâmetros para Aumento de Árvore",
  "help": {
    "title": "Aumento de Gradiente Extremo",
    "r_help": "ajuda(xgboost, pacote ='xgboost')",
    "body": "\n                <b>Descrição</b></br>\n                Crie um modelo de Aumento de Gradiente Extremo\n                <br/>\n                <b>NOTA</b></br>\n                1. Para prever a variável dependente do tipo fator, você precisa recodificar a variável dependente para um numérico com valores começando de 0. Por exemplo, se houver 3 níveis na variável fator, a variável numérica conterá valores 0,1,2.</br>\n                Veja Dados > Recode Variáveis. Alternativamente, basta converter a variável fator em numérica, tipicamente os níveis serão mapeados para inteiros começando de 1 e subtrair 1 da variável resultante. Isso lhe dará uma variável numérica com valores começando de 0.</br>\n                Você também precisa definir o objetivo como multi:softmax (prediz classes) ou multi:softprob (prediz probabilidades) e inserir o número de classes na caixa de texto do número de classes.</br>\n                O número de classes deve ser inserido apenas para multi:softmax e multi:softprob. Erros serão gerados se o número de classes for inserido para os outros objetivos.</br>\n                2. Você precisa codificar variáveis fator independentes, use codificação 1-Hot veja Dados > Computar Variáveis Dummy</br>\n                3. Uma matriz de confusão e curva ROC não são geradas quando o objetivo selecionado é um dos reg:squarederror, reg:logistic, binary:logitraw e rank:pairwise, pois a função de previsão \n                não retorna a classe da variável dependente. As previsões numéricas (no caso de reg:squarederror), pontuações, classificações etc que a função de previsão retorna são armazenadas no conjunto de dados. Veja ajuda(predict.xgb.Booster) para mais detalhes</br>\n                4. Uma matriz de confusão não é gerada quando o objetivo selecionado é multi:softprob, pois a função de previsão retorna as probabilidades previstas e não a classe da variável dependente. As probabilidades previstas são salvas no conjunto de dados e a curva ROC é gerada. Para ver a matriz de confusão, selecione multi:softmax como o objetivo.</br>\n                5. Uma curva ROC não é gerada quando o objetivo de multi:softmax é selecionado, pois a função de previsão retorna a classe e não as probabilidades previstas. Para ver a curva ROC, selecione multi:softprob como o objetivo.</br>\n                <br/>\n                <b>Uso</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Argumentos</b><br/>\n                <ul>\n                <li>\n                params: a lista de parâmetros. A lista completa de parâmetros está disponível em http://xgboost.readthedocs.io/en/latest/parameter.html. Abaixo está um resumo mais curto:<br/>\n                </li>\n                <li>\n                1. Parâmetros Gerais<br/>\n                Este diálogo usa gbtree (aumento de árvore)<br/>\n                </li>\n                <li>\n                2. Parâmetros do Aumento<br/>\n                2.1. Parâmetro para Aumento de Árvore<br/>\n                eta: controla a taxa de aprendizado: escala a contribuição de cada árvore por um fator de 0 < eta < 1 quando é adicionada à aproximação atual.<br/>\n                Usado para prevenir overfitting tornando o processo de aumento mais conservador.<br/>\n                Um valor mais baixo para eta implica um valor maior para nrounds: um valor baixo de eta significa que o modelo é mais robusto ao overfitting, mas mais lento para computar. Padrão: 0.3<br/>\n                gamma: redução mínima de perda necessária para fazer uma nova partição em um nó folha da árvore. quanto maior, mais conservador será o algoritmo.<br/>\n                max_depth: profundidade máxima de uma árvore. Padrão: 6<br/>\n                min_child_weight: soma mínima do peso da instância (hessiano) necessária em uma criança. Se o passo de partição da árvore resultar em um nó folha com a soma do peso da instância menor que min_child_weight, então o processo de construção desistirá de mais partições. No modo de regressão linear, isso simplesmente corresponde ao número mínimo de instâncias necessárias para estar em cada nó. Quanto maior, mais conservador será o algoritmo. Padrão: 1<br/>\n                </li>\n                <li>\n                3. Parâmetros da Tarefa<br/>\n                objective: especifique a tarefa de aprendizado e o objetivo de aprendizado correspondente, os usuários podem passar uma função definida pelo usuário para isso. As opções de objetivo padrão estão abaixo:<br/>\n                reg:squarederror - Regressão com perda quadrada (Padrão).<br/>\n                reg:logistic regressão logística.<br/>\n                binary:logistic - regressão logística para classificação binária. Probabilidade de saída.<br/>\n                binary:logitraw - regressão logística para classificação binária, saída de pontuação antes da transformação logística.<br/>\n                num_class: defina o número de classes. Para usar apenas com objetivos multiclasses.<br/>\n                multi:softmax - defina o xgboost para fazer classificação multiclass usando o objetivo softmax. A classe é representada por um número e deve ser de 0 a num_class - 1.<br/>\n                multi:softprob - igual a softmax, mas a previsão produz um vetor de elementos ndata * nclass, que pode ser remodelado para matriz ndata, nclass. O resultado contém as probabilidades previstas de cada ponto de dados pertencendo a cada classe.<br/>\n                rank:pairwise - defina o xgboost para fazer a tarefa de classificação minimizando a perda par a par.<br/>\n                base_score: a pontuação de previsão inicial de todas as instâncias, viés global. Padrão: 0.5<br/>\n                eval_metric: métricas de avaliação para dados de validação. Os usuários podem passar uma função definida pelo usuário para isso. Padrão: a métrica será atribuída de acordo com o objetivo (rmse para regressão,\n                e erro para classificação, média de precisão para classificação). A lista é fornecida na seção de detalhes.<br/>\n                data: conjunto de dados de treinamento. xgb.train aceita apenas um xgb.DMatrix como entrada. xgboost, além disso, também aceita matriz, dgCMatrix ou nome de um arquivo de dados local.<br/>\n                nrounds: número máximo de iterações de aumento.<br/>\n                verbose: Se 0, xgboost ficará em silêncio. Se 1, imprimirá informações sobre desempenho. Se 2, algumas informações adicionais serão impressas. Note que definir verbose > 0 ativa automaticamente a função de callback cb.print.evaluation(period=1).<br/>\n                print_every_n: Imprimir mensagens de avaliação de cada n-ésima iteração quando verbose>0. O padrão é 1, o que significa que todas as mensagens são impressas. Este parâmetro é passado para o callback cb.print.evaluation.<br/>\n                label: vetor de valores de resposta. Não deve ser fornecido quando os dados são um nome de arquivo de dados local ou um xgb.DMatrix.<br/>\n                missing: por padrão é definido como NA, o que significa que os valores NA devem ser considerados como 'faltando' pelo algoritmo. Às vezes, 0 ou outro valor extremo pode ser usado para representar valores faltantes. Este parâmetro é usado apenas quando a entrada é uma matriz densa.<br/>\n                </li>\n                </ul>\n                <b>Detalhes</b></br>\n                A métrica de avaliação é escolhida automaticamente pelo Xgboost (de acordo com o objetivo) quando o parâmetro eval_metric não é fornecido.</br>\n                O usuário pode definir um ou vários parâmetros eval_metric. Note que ao usar uma métrica personalizada, apenas esta única métrica pode ser usada. A seguir está a lista de métricas internas para as quais o Xgboost fornece implementação otimizada:</br>\n                rmse erro quadrático médio. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss log-verossimilhança negativa. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss logloss multiclass. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error Taxa de erro de classificação binária. É calculada como (# casos errados) / (# todos os casos). Por padrão, usa o limiar de 0.5 para valores previstos para definir instâncias negativas e positivas.</br>\n                Um limiar diferente (por exemplo, 0.) poderia ser especificado como \"error@0.\"</br>\n                merror Taxa de erro de classificação multiclass. É calculada como (# casos errados) / (# todos os casos).</br>\n                auc Área sob a curva. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve para avaliação de classificação.</br>\n                aucpr Área sob a curva PR. https://en.wikipedia.org/wiki/Precision_and_recall para avaliação de classificação.</br>\n                ndcg Ganho Cumulativo Normalizado Descontado (para tarefa de classificação). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                Os seguintes callbacks são criados automaticamente quando certos parâmetros são definidos:</br>\n                cb.print.evaluation é ativado quando verbose > 0; e o parâmetro print_every_n é passado para ele.</br>\n                cb.evaluation.log está ativado quando a watchlist está presente.</br>\n                cb.early.stop: quando early_stopping_rounds é definido.</br>\n                cb.save.model: quando save_period > 0 é definido.</br></br>\n                <b>Valor</b></br>\n                Um objeto da classe xgb.Booster com os seguintes elementos:</br>\n                handle um identificador (ponteiro) para o modelo xgboost na memória.</br>\n                raw um dump de memória em cache do modelo xgboost salvo como tipo raw do R.</br>\n                niter: número de iterações de aumento.</br>\n                evaluation_log: histórico de avaliação armazenado como um data.table com a primeira coluna correspondente ao número da iteração e o resto correspondente aos valores das métricas de avaliação. É criado pelo callback cb.evaluation.log.</br>\n                call: uma chamada de função.</br>\n                params: parâmetros que foram passados para a biblioteca xgboost. Note que não captura parâmetros alterados pelo callback cb.reset.parameters.</br>\n                callbacks funções de callback que foram automaticamente atribuídas ou explicitamente passadas.</br>\n                best_iteration: número da iteração com o melhor valor da métrica de avaliação (disponível apenas com parada antecipada).</br>\n                best_ntreelimit: o valor de ntreelimit correspondente à melhor iteração, que pode ser usado posteriormente no método de previsão (disponível apenas com parada antecipada).</br>\n                best_score: o melhor valor da métrica de avaliação durante a parada antecipada. (disponível apenas com parada antecipada).</br>\n                feature_names: nomes das características do conjunto de dados de treinamento (apenas quando os nomes das colunas foram definidos nos dados de treinamento).</br>\n                nfeatures: número de características nos dados de treinamento.</br>\n                <b>Referências</b></br>\n                Tianqi Chen e Carlos Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 22ª Conferência SIGKDD sobre Descoberta de Conhecimento e Mineração de Dados, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>Veja Também</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Exemplos</b></br>\n                data(agaricus.train, pacote='xgboost')</br>\n                data(agaricus.test, pacote='xgboost')</br>\n                ## Um exemplo simples de xgb.train:</br>\n                # Essas funções poderiam ser usadas passando-as de qualquer forma:</br>\n                ## Um exemplo de interface 'xgboost':</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}