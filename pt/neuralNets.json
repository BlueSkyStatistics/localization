{
  "title": "Treinamento de Redes Neurais, usando o pacote neuralnet",
  "navigation": "Redes Neurais",
  "label1": "POR FAVOR, CODIFIQUE VARIÁVEIS DUMMY, VEJA DADOS > COMPUTAR VARIÁVEIS DUMMY (MANTENHA TODOS OS NÍVEIS A.K.A CODIFICAÇÃO ONE HOT). ESCALONE E CENTRALIZE AS VARIÁVEIS NUMÉRICAS, VEJA DADOS > PADRONIZAR VARIÁVEIS",
  "model": "Insira um nome de modelo",
  "dependentvar": "Variável dependente",
  "independentvars": "Variável(is) independente(s)",
  "seed": "Definir semente",
  "iter": "Máximo de passos para aprendizado",
  "tf": "Algoritmo",
  "threshold": "Limite",
  "label2": "Número de camadas ocultas e os neurônios por camada oculta",
  "layers": "Especifique o número de neurônios em cada camada, por exemplo 1. Para 5 neurônios em 1 camada, insira 5 2. Para 5 neurônios na camada 1, 6 neurônios na camada 2, 7 neurônios na camada 3 insira 5,6,7",
  "OutActFunc": "Especifique uma função de ativação de saída",
  "rep": "Repetições para treinamento da rede neural",
  "label3": "Fatores de multiplicação para a taxa de aprendizado superior e inferior",
  "minus": "Superior (menos)",
  "upper": "Inferior (mais)",
  "lifesign": "Configuração de quanto imprimir durante o cálculo da rede neural",
  "lifesignstep": "Tamanho do passo para imprimir o limite mínimo em modo de sinal de vida completo",
  "errfct": "Função diferenciável usada para o cálculo do erro",
  "linearoutput": "A função de ativação não deve ser aplicada aos neurônios de saída",
  "likelihood": "Verossimilhança",
  "advanced_lbl": "Avançado",
  "help": {
    "title": "Treinamento de Redes Neurais, usando o pacote neuralnet",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>NOTA</b></br>\n            Ao especificar uma única variável dependente, ela pode ser numérica ou fator. Se a variável dependente especificada for um fator, codificamos automaticamente a variável fator usando codificação one-hot com a função decode no pacote RSNNS.</br></br>\n            Além disso, se você estiver usando codificação one-hot para codificar uma variável fator, pode especificar mais de uma variável dependente no diálogo. Nesse caso, as variáveis dependentes devem ser do tipo numérico.</br></br>\n            Você pode usar \"Dados > Computar variáveis dummy\", escolher a configuração “Manter todos os níveis” para obter codificação one-hot.</br></br>\n            Para variáveis dependentes do tipo fator, exibiremos uma matriz de confusão, ROC e estatísticas de precisão do modelo ao pontuar um conjunto de dados usando o modelo construído. As previsões geradas são do tipo fator, uma vez que prevemos a classe. Estas serão salvas no conjunto de dados junto com as probabilidades previstas ao pontuar.</br></br>\n            Quando há variáveis dependentes codificadas como dummy, não exibiremos uma matriz de confusão, ROC e estatísticas de precisão do modelo ao pontuar um conjunto de dados usando o modelo construído. No entanto, as previsões serão salvas no conjunto de dados ao pontuar o conjunto de dados. As previsões são as probabilidades associadas às variáveis dependentes codificadas como dummy.</br></br>\n            Geralmente, é melhor padronizar as variáveis independentes (elas também devem ser numéricas). Veja “Dados > Padronizar Variáveis.”</br></br>\n            Se você tiver variáveis independentes categóricas, use codificação one-hot para codificar as variáveis fator.</br></br>\n            <b>Descrição</b></br>\n            Treine redes neurais usando retropropagação, retropropagação resiliente (RPROP) com (Riedmiller, 1994) ou sem retrocesso de peso (Riedmiller e Braun, 1993) ou a versão modificada globalmente convergente (GRPROP) de Anastasiadis et al. (2005). A função permite configurações flexíveis através da escolha personalizada da função de erro e da função de ativação. Além disso, o cálculo de pesos generalizados (Intrator O. e Intrator N., 1993) é implementado.\n            <b>Uso</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>Argumentos</b><br/>\n            <ul>\n            <li>\n            formula: uma descrição simbólica do modelo a ser ajustado.\n            </li>\n            <li>\n            data: um data frame contendo as variáveis especificadas na fórmula.\n            </li>\n            <li>\n            hidden: um vetor de inteiros especificando o número de neurônios ocultos (vértices) em cada camada.\n            </li>\n            <li>\n            threshold: um valor numérico especificando o limite para as derivadas parciais da função de erro como critério de parada.\n            </li>\n            <li>\n            stepmax: o número máximo de passos para o treinamento da rede neural. Atingir esse máximo leva a uma parada do processo de treinamento da rede neural.\n            </li>\n            <li>\n            rep: o número de repetições para o treinamento da rede neural.\n            </li>\n            <li>\n            startweights: um vetor contendo valores iniciais para os pesos. Defina como NULL para inicialização aleatória.\n            </li>\n            <li>\n            learningrate.limit: um vetor ou uma lista contendo o limite mais baixo e mais alto para a taxa de aprendizado. Usado apenas para RPROP e GRPROP.</li>\n            <li>\n            learningrate.factor: um vetor ou uma lista contendo os fatores de multiplicação para a taxa de aprendizado superior e inferior. Usado apenas para RPROP e GRPROP.\n            </li>\n            <li>\n            learningrate: um valor numérico especificando a taxa de aprendizado usada pela retropropagação tradicional. Usado apenas para retropropagação tradicional.\n            </li>\n            <li>\n            lifesign: uma string especificando quanto a função imprimirá durante o cálculo da rede neural. 'nenhum', 'mínimo' ou 'completo'.\n            </li>\n            <li>\n            lifesign.step: um inteiro especificando o tamanho do passo para imprimir o limite mínimo em modo de sinal de vida completo.\n            </li>\n            <li>\n            algorithm: uma string contendo o tipo de algoritmo para calcular a rede neural. Os seguintes tipos são possíveis: 'backprop', 'rprop+', 'rprop-', 'sag' ou 'slr'. 'backprop' refere-se à retropropagação, 'rprop+' e 'rprop-' referem-se à retropropagação resiliente com e sem retrocesso de peso, enquanto 'sag' e 'slr' induzem o uso do algoritmo globalmente convergente modificado (grprop). Veja Detalhes para mais informações.\n            </li>\n            <li>\n            err.fct: uma função diferenciável que é usada para o cálculo do erro. Alternativamente, as strings 'sse' e 'ce' que representam a soma dos erros quadrados e a entropia cruzada podem ser usadas.\n            </li>\n            <li>\n            act.fct: uma função diferenciável que é usada para suavizar o resultado do produto cruzado da covariável ou neurônios e os pesos. Além disso, as strings 'logistic' e 'tanh' são possíveis para a função logística e tangente hiperbólica.\n            </li>\n            <li>\n            linear.output: lógico. Se act.fct não deve ser aplicada aos neurônios de saída, defina a saída linear como TRUE, caso contrário, defina como FALSE.\n            </li>\n            <li>\n            exclude: um vetor ou uma matriz especificando os pesos que são excluídos do cálculo. Se dado como um vetor, as posições exatas dos pesos devem ser conhecidas. Uma matriz com n-linhas e 3 colunas excluirá n pesos, onde a primeira coluna representa a camada, a segunda coluna representa o neurônio de entrada e a terceira coluna representa o neurônio de saída do peso.\n            </li>\n            <li>\n            constant.weights: um vetor especificando os valores dos pesos que são excluídos do processo de treinamento e tratados como fixos.\n            </li>\n            <li>\n            likelihood: lógico. Se a função de erro for igual à função de verossimilhança negativa, os critérios de informação AIC e BIC serão calculados. Além disso, o uso de confidence.interval é significativo.\n            </li>\n            </ul>\n            <b>Detalhes</b><br/>\n            O algoritmo globalmente convergente é baseado na retropropagação resiliente sem retrocesso de peso e modifica adicionalmente uma taxa de aprendizado, seja a taxa de aprendizado associada ao menor gradiente absoluto (sag) ou a menor taxa de aprendizado (slr) em si. As taxas de aprendizado no algoritmo grprop são limitadas aos limites definidos em learningrate.limit.\n            ​<b>Valor</b><br/>\n            neuralnet retorna um objeto da classe nn. Um objeto da classe nn é uma lista contendo no máximo os seguintes componentes:<br/>\n            call: a chamada correspondente.<br/>\n            response: extraído do argumento de dados.<br/>\n            covariate: as variáveis extraídas do argumento de dados.<br/>\n            model.list: uma lista contendo as covariáveis e as variáveis de resposta extraídas do argumento de fórmula.<br/>\n            err.fct: a função de erro.<br/>\n            act.fct: a função de ativação.<br/>\n            data: o argumento de dados.<br/>\n            net.result: uma lista contendo o resultado geral da rede neural para cada repetição.<br/>\n            weights: uma lista contendo os pesos ajustados da rede neural para cada repetição.<br/>\n            generalized.weights: uma lista contendo os pesos generalizados da rede neural para cada repetição.<br/>\n            result.matrix: uma matriz contendo o limite alcançado, passos necessários, erro, AIC e BIC (se computados) e pesos para cada repetição. Cada coluna representa uma repetição.<br/>\n            startweights: uma lista contendo os pesos iniciais da rede neural para cada repetição.<br/>\n            ​<b>Exemplos</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# Classificação binária\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Não executado: print(nn)\n            ## Não executado: plot(nn)\n            # Classificação multiclasse\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Não executado: print(nn)\n            ## Não executado: plot(nn)\n            # Função de ativação personalizada\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## Não executado: print(nn)\n            ## Não executado: plot(nn)\n            </code> <br/>\n            <b>Pacote</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>Ajuda</b></br>\n            Para ajuda detalhada, clique no ícone R no canto superior direito deste diálogo ou execute o seguinte comando no editor de sintaxe R</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}