{
  "title": "Perceptron de Múltiplas Camadas, usando o pacote RSNNS",
  "navigation": "Perceptron de Múltiplas Camadas",
  "label1": "POR FAVOR, CODIFIQUE AS VARIÁVEIS FATORIAIS, VEJA DADOS > COMPUTAR VARIÁVEIS DUMMY (MANTENHA TODOS OS NÍVEIS A.K.A CODIFICAÇÃO ONE HOT). ESCALONE E CENTRALIZE AS VARIÁVEIS NUMÉRICAS, VEJA DADOS > PADRONIZAR VARIÁVEIS",
  "model": "Insira um nome de modelo",
  "dependentvar": "Variável dependente",
  "independentvars": "Variável(is) independente(s)",
  "seed": "Definir semente",
  "iter": "Máximo de iterações para aprender",
  "tf": "Função de aprendizado",
  "label2": "Número de camadas ocultas e os neurônios por camada oculta",
  "layers": "Especifique o número de neurônios em cada camada, por exemplo 1. Para 5 neurônios em 1 camada, insira 5 2. Para 5 neurônios na camada 1, 6 neurônios na camada 2, 7 neurônios na camada 3 insira 5,6,7",
  "learnfuncparams": "Parâmetros da função de aprendizado",
  "help": {
    "title": "Perceptron de Múltiplas Camadas, usando o pacote RSNNS",
    "r_help": "help(mlp, package ='RSNNS')",
    "body": "\n            <b>NOTA</b></br>\n            Quando você especifica uma única variável dependente, ela pode ser numérica ou fator. Se a variável dependente especificada for um fator, nós automaticamente codificamos a variável fator usando codificação one-hot com a função decode no pacote RSNNS.</br></br>\n            Além disso, se você estiver usando codificação one-hot para codificar uma variável fator, pode especificar mais de uma variável dependente no diálogo. Nesse caso, as variáveis dependentes devem ser do tipo numérico.</br></br>\n            Você pode usar \"Dados > Computar variáveis dummy\", escolha a configuração \"Manter todos os níveis\" para obter codificação one-hot.</br></br>\n            Para variáveis dependentes do tipo fator, exibiremos uma matriz de confusão, ROC e estatísticas de precisão do modelo ao pontuar um conjunto de dados usando o modelo construído. As previsões geradas são do tipo fator, uma vez que prevemos a classe. Estas serão salvas no conjunto de dados junto com as probabilidades previstas ao pontuar.</br></br>\n            Quando há variáveis dependentes codificadas como dummy, não exibiremos uma matriz de confusão, ROC e estatísticas de precisão do modelo ao pontuar um conjunto de dados usando o modelo construído. No entanto, as previsões serão salvas no conjunto de dados ao pontuar o conjunto de dados. As previsões são as probabilidades associadas às variáveis dependentes codificadas como dummy.</br></br>\n            Geralmente é melhor padronizar as variáveis independentes (elas também devem ser numéricas). Veja \"Dados > Padronizar Variáveis.\"</br></br>\n            Se você tiver variáveis independentes categóricas, use codificação one-hot para codificar as variáveis fatoriais.</br></br>\n            <b>Descrição</b></br>\n            Esta função cria um perceptron de múltiplas camadas (MLP) e o treina. MLPs são redes totalmente conectadas de avanço, e provavelmente a arquitetura de rede mais comum em uso. O treinamento é geralmente realizado por retropropagação de erro ou um procedimento relacionado.</br>\n            Existem muitas funções de aprendizado diferentes presentes no SNNS que podem ser usadas junto com esta função, por exemplo, Std_Backpropagation, BackpropBatch, BackpropChunk, BackpropMomentum, BackpropWeightDecay, Rprop, Quickprop, SCG (gradiente conjugado escalado), ...</br>\n            <b>Uso</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## Método padrão S3:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>Argumentos</b><br/>\n            <ul>\n            <li>\n            x: uma matriz com entradas de treinamento para a rede\n            </li>\n            <li>\n            ... : parâmetros adicionais da função (atualmente não utilizados)\n            </li>\n            <li>\n            y: os valores correspondentes dos alvos\n            </li>\n            <li>\n            size: número de unidades nas camadas ocultas\n            </li>\n            <li>\n            maxit: máximo de iterações para aprender\n            </li>\n            <li>\n            initFunc: a função de inicialização a ser usada\n            </li>\n            <li>\n            initFuncParams: os parâmetros para a função de inicialização\n            </li>\n            <li>\n            learnFunc: a função de aprendizado a ser usada\n            </li>\n            <li>\n            learnFuncParams: os parâmetros para a função de aprendizado\n            </li>\n            <li>\n            updateFunc: a função de atualização a ser usada\n            </li>\n            <li>\n            updateFuncParams: os parâmetros para a função de atualização\n            </li>\n            <li>\n            hiddenActFunc: a função de ativação de todas as unidades ocultas\n            </li>\n            <li>\n            shufflePatterns: os padrões devem ser embaralhados?\n            </li>\n            <li>\n            linOut: define a função de ativação das unidades de saída como linear ou logística (ignorada se outputActFunc for fornecida)\n            </li>\n            <li>\n            outputActFunc: a função de ativação de todas as unidades de saída\n            </li>\n            <li>\n            inputsTest: uma matriz com entradas para testar a rede\n            </li>\n            <li>\n            targetsTest: os alvos correspondentes para a entrada de teste\n            </li>\n            <li>\n            pruneFunc: a função de poda a ser usada\n            </li>\n            <li>\n            pruneFuncParams: os parâmetros para a função de poda. Ao contrário das outras funções, estes devem ser fornecidos em uma lista nomeada. Veja os demos de poda para mais explicações.\n            </li>\n            </ul>\n            <b>Detalhes</b></br>\n            Std_Backpropagation, BackpropBatch, por exemplo, têm dois parâmetros, a taxa de aprendizado e a diferença máxima de saída. A taxa de aprendizado é geralmente um valor entre 0.1 e 1. Ela especifica a largura do passo de descida do gradiente. A diferença máxima define quanto de diferença entre o valor de saída e o valor alvo é tratado como erro zero, e não retropropagado. Este parâmetro é usado para evitar o sobreajuste. Para uma lista completa dos parâmetros de todas as funções de aprendizado, veja o Manual do Usuário do SNNS, pp. 67.</br>\n            Os padrões que são definidos para funções de inicialização e atualização geralmente não precisam ser alterados.</br>\n            <b>Valor</b><br/>\n            um objeto rsnns.\n            <b>Referências</b><br/>\n            Rosenblatt, F. (1958), 'O perceptron: Um modelo probabilístico para armazenamento e organização de informações no cérebro', Psychological Review 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Group, P. R. (1986), Processamento distribuído paralelo: explorações na microestrutura da cognição, Mit, Cambridge, MA etc.<br/>\n            Zell, A. et al. (1998), 'Manual do Usuário do Simulador de Rede Neural SNNS Stuttgart, Versão 4.2', IPVR, Universidade de Stuttgart e WSI, Universidade de Tübingen.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), Simulação de Redes Neurais, Addison-Wesley. (em Alemão)<br/>\n            <b>Exemplos</b><br/>\n            <code> \n            data(iris)<br/>\n            #embaralhar o vetor<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #matriz de confusão com o método 402040<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>Pacote</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>Ajuda</b></br>\n            Para ajuda detalhada clique no ícone R no canto superior direito deste diálogo ou execute o seguinte comando no editor de sintaxe R</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
  }
}