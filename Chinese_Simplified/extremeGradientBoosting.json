{
  "title": "极端梯度提升",
  "navigation": "极端梯度提升",
  "label1": "虚拟编码因子变量，请参见变量>计算>虚拟编码使用选项以保留所有级别。对于预测多个类别，请阅读帮助（点击对话框上的 ? 图标）。",
  "model": "输入模型名称",
  "dependentvar": "因变量",
  "independentvars": "自变量",
  "objective": "目标",
  "seed": "设置种子",
  "nrounds": "最大提升迭代次数",
  "maxdepth": "最大深度",
  "minchildweight": "最小子权重",
  "maxdeltastep": "最大增量步长",
  "eta": "eta（学习率）",
  "gamma": "gamma",
  "numclasses": "类别数量。**仅与multi:softmax和multi:softprob一起使用",
  "basescore": "基础分数",
  "Verbose": "详细模式（0=静默，1=性能信息，2=其他信息）",
  "printevery": "当详细信息 > 0 时，打印每 n 次迭代评估消息",
  "help": {
    "title": "极端梯度提升",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>描述</b></br>\n                创建一个极端梯度提升模型\n                <br/>\n                <b>注意</b></br>\n                1. 要预测因子类型的因变量，您需要将因变量重新编码为从 0 开始的数值。例如，如果因子变量中有 3 个级别，则数值变量将包含值 0,1,2。</br>\n                请参见数据 > 重新编码变量。或者只需将因子变量转换为数值，通常级别将映射到从 1 开始的整数，并从结果变量中减去 1。这将为您提供一个从 0 开始的数值变量。</br>\n                您还需要将目标设置为 multi:softmax（预测类别）或 multi:softprob（预测概率），并在类别数量文本框中输入类别数量。</br>\n                类别数量必须仅为 multi:softmax 和 multi:softprob 输入。对于其他目标，如果输入类别数量，将会生成错误。</br>\n                2. 您需要对独立因子变量进行虚拟编码，使用 1-Hot 编码，请参见数据 > 计算虚拟变量</br>\n                3. 当选择的目标是 reg:squarederror、reg:logistic、binary:logitraw 和 rank:pairwise 时，不会生成混淆矩阵和 ROC 曲线，因为预测函数\n                不返回因变量的类别。预测函数返回的数值预测（在 reg:squarederror 的情况下）、分数、排名等存储在数据集中。有关更多详细信息，请参见 help(predict.xgb.Booster)</br>\n                4. 当选择的目标是 multi:softprob 时，不会生成混淆矩阵，因为预测函数返回预测概率而不是因变量的类别。预测概率保存在数据集中，并生成 ROC 曲线。要查看混淆矩阵，请选择 multi:softmax 作为目标。</br>\n                5. 当选择 multi:softmax 作为目标时，不会生成 ROC 曲线，因为预测函数返回类别而不是预测概率。要查看 ROC 曲线，请选择 multi:softprob 作为目标。</br>\n                <br/>\n                <b>用法</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>参数</b><br/>\n                <ul>\n                <li>\n                params: 参数列表。完整的参数列表可在 http://xgboost.readthedocs.io/en/latest/parameter.html 中找到。以下是简短的摘要:<br/>\n                </li>\n                <li>\n                1. 一般参数<br/>\n                此对话框使用 gbtree（树提升器）<br/>\n                </li>\n                <li>\n                2. 提升器参数<br/>\n                2.1. 树提升器的参数<br/>\n                eta: 控制学习率：在添加到当前近似值时按因子 0 < eta < 1 缩放每棵树的贡献。<br/>\n                用于通过使提升过程更加保守来防止过拟合。<br/>\n                eta 的较低值意味着 nrounds 的较大值：低 eta 值意味着模型对过拟合更具鲁棒性，但计算速度较慢。默认值：0.3<br/>\n                gamma: 进一步划分树叶节点所需的最小损失减少。越大，算法越保守。<br/>\n                max_depth: 树的最大深度。默认值：6<br/>\n                min_child_weight: 子节点中所需的实例权重（海森矩阵）最小总和。如果树划分步骤导致的叶节点的实例权重总和小于 min_child_weight，则构建过程将放弃进一步划分。在线性回归模式下，这简单对应于每个节点中所需的最小实例数量。越大，算法越保守。默认值：1<br/>\n                </li>\n                <li>\n                3. 任务参数<br/>\n                objective: 指定学习任务和相应的学习目标，用户可以传递自定义函数。默认目标选项如下:<br/>\n                reg:squarederror -平方损失回归（默认）。<br/>\n                reg:logistic 逻辑回归。<br/>\n                binary:logistic -二元分类的逻辑回归。输出概率。<br/>\n                binary:logitraw -二元分类的逻辑回归，输出逻辑变换前的分数。<br/>\n                num_class: 设置类别数量。仅与多类目标一起使用。<br/>\n                multi:softmax -设置 xgboost 进行多类分类，使用 softmax 目标。类别由数字表示，应该从 0 到 num_class - 1。<br/>\n                multi:softprob -与 softmax 相同，但预测输出一个 ndata * nclass 元素的向量，可以进一步重塑为 ndata, nclass 矩阵。结果包含每个数据点属于每个类别的预测概率。<br/>\n                rank:pairwise -设置 xgboost 通过最小化成对损失进行排名任务。<br/>\n                base_score: 所有实例的初始预测分数，全局偏差。默认值：0.5<br/>\n                eval_metric: 验证数据的评估指标。用户可以传递自定义函数。默认情况下，指标将根据目标分配（回归的 rmse，分类的错误，排名的平均精度）。详细部分提供了列表。<br/>\n                data: 训练数据集。xgb.train 仅接受 xgb.DMatrix 作为输入。xgboost 还接受矩阵、dgCMatrix 或本地数据文件的名称。<br/>\n                nrounds: 最大提升迭代次数。<br/>\n                verbose: 如果为 0，xgboost 将保持静默。如果为 1，它将打印有关性能的信息。如果为 2，将打印一些额外信息。请注意，设置 verbose > 0 会自动启用 cb.print.evaluation(period=1) 回调函数。<br/>\n                print_every_n: 当 verbose>0 时，打印每 n 次迭代评估消息。默认值为 1，这意味着打印所有消息。此参数传递给 cb.print.evaluation 回调。<br/>\n                label: 响应值的向量。当数据是本地数据文件名或 xgb.DMatrix 时，不应提供。<br/>\n                missing: 默认设置为 NA，这意味着算法应将 NA 值视为“缺失”。有时，0 或其他极端值可能用于表示缺失值。此参数仅在输入为稠密矩阵时使用。<br/>\n                </li>\n                </ul>\n                <b>详细信息</b></br>\n                当未提供 eval_metric 参数时，评估指标由 Xgboost 自动选择（根据目标）。</br>\n                用户可以设置一个或多个 eval_metric 参数。请注意，当使用自定义指标时，仅可以使用此单个指标。以下是 Xgboost 提供优化实现的内置指标列表：</br>\n                rmse 均方根误差。http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss 负对数似然。http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss 多类 logloss。http://wiki.fast.ai/index.php/Log_Loss</br>\n                error 二元分类错误率。计算为（# 错误案例）/（# 所有案例）。默认情况下，它使用 0.5 阈值来定义负实例和正实例。</br>\n                可以指定不同的阈值（例如，0.）作为 \"error@0.\"</br>\n                merror 多类分类错误率。计算为（# 错误案例）/（# 所有案例）。</br>\n                auc 曲线下面积。http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve 用于排名评估。</br>\n                aucpr PR 曲线下面积。https://en.wikipedia.org/wiki/Precision_and_recall 用于排名评估。</br>\n                ndcg 标准化折扣累积增益（用于排名任务）。http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                当设置某些参数时，以下回调会自动创建：</br>\n                cb.print.evaluation 在 verbose > 0 时启用；并且将 print_every_n 参数传递给它。</br>\n                cb.evaluation.log 在 watchlist 存在时启用。</br>\n                cb.early.stop: 当设置 early_stopping_rounds 时。</br>\n                cb.save.model: 当设置 save_period > 0 时。</br></br>\n                <b>值</b></br>\n                一个类 xgb.Booster 的对象，具有以下元素：</br>\n                handle 指向内存中 xgboost 模型的句柄（指针）。</br>\n                raw xgboost 模型的缓存内存转储，保存为 R 的原始类型。</br>\n                niter: 提升迭代次数。</br>\n                evaluation_log: 评估历史记录存储为数据表，第一列对应于迭代次数，其余列对应于评估指标的值。由 cb.evaluation.log 回调创建。</br>\n                call: 函数调用。</br>\n                params: 传递给 xgboost 库的参数。请注意，它不捕获通过 cb.reset.parameters 回调更改的参数。</br>\n                callbacks 自动分配或显式传递的回调函数。</br>\n                best_iteration: 具有最佳评估指标值的迭代次数（仅在早期停止时可用）。</br>\n                best_ntreelimit: 对应于最佳迭代的 ntreelimit 值，可以在预测方法中进一步使用（仅在早期停止时可用）。</br>\n                best_score: 在早期停止期间最佳评估指标值（仅在早期停止时可用）。</br>\n                feature_names: 训练数据集特征的名称（仅在训练数据中定义了列名时）。</br>\n                nfeatures: 训练数据中的特征数量。</br>\n                <b>参考文献</b></br>\n                Tianqi Chen 和 Carlos Guestrin，\"XGBoost: A Scalable Tree Boosting System\"，第 22 届 SIGKDD 知识发现与数据挖掘会议，2016，https://arxiv.org/abs/1603.02754</br>\n                <b>另请参见</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>示例</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## 一个简单的 xgb.train 示例:</br>\n                # 这些函数可以通过传递它们来使用：</br>\n                ## 一个 'xgboost' 接口示例：</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}