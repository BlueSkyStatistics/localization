{
  "title": "使用neuralnet包训练神经网络",
  "navigation": "神经网络",
  "label1": "请对因子变量进行虚拟编码，参见数据 > 计算虚拟变量（保留所有级别即1热编码）。缩放和中心化数值变量，参见数据 > 标准化变量",
  "model": "输入模型名称",
  "dependentvar": "因变量",
  "independentvars": "自变量",
  "seed": "设置种子",
  "iter": "学习的最大步骤",
  "tf": "算法",
  "threshold": "阈值",
  "label2": "隐藏层的数量和每个隐藏层的神经元数量",
  "layers": "指定每层中的神经元数量，例如1。对于1层中的5个神经元，输入5；对于第1层5个神经元，第2层6个神经元，第3层7个神经元，输入5,6,7",
  "OutActFunc": "指定输出激活函数",
  "rep": "神经网络训练的重复次数",
  "label3": "上限和下限学习率的乘法因子",
  "minus": "上限（减）",
  "upper": "下限（加）",
  "lifesign": "在计算神经网络期间打印的程度设置",
  "lifesignstep": "在完全生命迹象模式下打印最小阈值的步长",
  "errfct": "用于计算误差的可微函数",
  "linearoutput": "不应将激活函数应用于输出神经元",
  "likelihood": "似然",
  "help": {
    "title": "使用neuralnet包训练神经网络",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>注意</b></br>\n            当指定单个因变量时，它可以是数值型或因子型。如果指定的因变量是因子型，我们会使用RSNNS包中的decode函数自动对因子变量进行虚拟编码，使用一热编码。</br></br>\n            此外，如果您使用一热编码对因子变量进行虚拟编码，您可以在对话框中指定多个因变量。在这种情况下，因变量必须是数值型。</br></br>\n            您可以使用“数据 > 计算虚拟变量”，选择“保留所有级别”设置以获得一热编码。</br></br>\n            对于因子型因变量，当使用构建的模型对数据集进行评分时，我们将显示混淆矩阵、ROC和模型准确性统计信息。生成的预测为因子型，因为我们预测类别。这些将与预测概率一起保存在数据集中。</br></br>\n            当存在虚拟编码的因变量时，我们在使用构建的模型对数据集进行评分时不会显示混淆矩阵、ROC和模型准确性统计信息。然而，预测将在评分数据集时保存在数据集中。预测是与虚拟编码因变量相关的概率。</br></br>\n            通常最好对自变量进行标准化（它们也必须是数值型），请参见“数据 > 标准化变量”。</br></br>\n            如果您有分类自变量，请使用一热编码对因子变量进行虚拟编码。</br></br>\n            <b>描述</b></br>\n            使用反向传播、弹性反向传播（RPROP）与（Riedmiller，1994）或不带权重回溯（Riedmiller和Braun，1993）或由Anastasiadis等（2005）修改的全局收敛版本（GRPROP）训练神经网络。该函数通过自定义选择误差和激活函数允许灵活设置。此外，实现了广义权重的计算（Intrator O.和Intrator N.，1993）。\n            <b>用法</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>参数</b><br/>\n            <ul>\n            <li>\n            formula: 要拟合的模型的符号描述。\n            </li>\n            <li>\n            data: 包含公式中指定的变量的数据框。\n            </li>\n            <li>\n            hidden: 一个整数向量，指定每层中隐藏神经元（顶点）的数量。\n            </li>\n            <li>\n            threshold: 一个数值，指定误差函数的偏导数的阈值作为停止标准。\n            </li>\n            <li>\n            stepmax: 神经网络训练的最大步骤。达到此最大值将导致神经网络的训练过程停止。\n            </li>\n            <li>\n            rep: 神经网络训练的重复次数。\n            </li>\n            <li>\n            startweights: 一个包含权重起始值的向量。设置为NULL以进行随机初始化。\n            </li>\n            <li>\n            learningrate.limit: 一个向量或列表，包含学习率的最低和最高限制。仅用于RPROP和GRPROP。</li>\n            <li>\n            learningrate.factor: 一个向量或列表，包含上限和下限学习率的乘法因子。仅用于RPROP和GRPROP。\n            </li>\n            <li>\n            learningrate: 一个数值，指定传统反向传播使用的学习率。仅用于传统反向传播。\n            </li>\n            <li>\n            lifesign: 一个字符串，指定在计算神经网络期间函数将打印多少。'none'，'minimal'或'full'。\n            </li>\n            <li>\n            lifesign.step: 一个整数，指定在完全生命迹象模式下打印最小阈值的步长。\n            </li>\n            <li>\n            algorithm: 一个字符串，包含计算神经网络的算法类型。可能的类型有：'backprop'，'rprop+'，'rprop-'，'sag'或'slr'。'backprop'指反向传播，'rprop+'和'rprop-'指带和不带权重回溯的弹性反向传播，而'sag'和'slr'引入使用修改的全局收敛算法（grprop）。有关更多信息，请参见详细信息。\n            </li>\n            <li>\n            err.fct: 用于计算误差的可微函数。或者，可以使用字符串'sse'和'ce'，分别表示平方和误差和交叉熵。\n            </li>\n            <li>\n            act.fct: 用于平滑协变量或神经元与权重的乘积结果的可微函数。此外，'logistic'和'tanh'的字符串也适用于逻辑函数和双曲正切。\n            </li>\n            <li>\n            linear.output: 逻辑。如果不应将act.fct应用于输出神经元，则将线性输出设置为TRUE，否则设置为FALSE。\n            </li>\n            <li>\n            exclude: 一个向量或矩阵，指定在计算中排除的权重。如果作为向量给出，则必须知道权重的确切位置。具有n行和3列的矩阵将排除n个权重，其中第一列表示层，第二列表示输入神经元，第三列表示权重的输出神经元。\n            </li>\n            <li>\n            constant.weights: 一个向量，指定在训练过程中排除的权重值，并视为固定。\n            </li>\n            <li>\n            likelihood: 逻辑。如果误差函数等于负对数似然函数，则将计算信息标准AIC和BIC。此外，使用confidence.interval是有意义的。\n            </li>\n            </ul>\n            <b>详细信息</b><br/>\n            全局收敛算法基于不带权重回溯的弹性反向传播，并额外修改一个学习率，即与最小绝对梯度（sag）或最小学习率（slr）相关的学习率。grprop算法中的学习率限制在learningrate.limit中定义的边界内。\n            ​<b>值</b><br/>\n            neuralnet返回一个nn类的对象。nn类的对象是一个包含最多以下组件的列表：<br/>\n            call: 匹配的调用。<br/>\n            response: 从数据参数中提取。<br/>\n            covariate: 从数据参数中提取的变量。<br/>\n            model.list: 一个包含从公式参数中提取的协变量和响应变量的列表。<br/>\n            err.fct: 误差函数。<br/>\n            act.fct: 激活函数。<br/>\n            data: 数据参数。<br/>\n            net.result: 一个列表，包含每次重复的神经网络的整体结果。<br/>\n            weights: 一个列表，包含每次重复的神经网络的拟合权重。<br/>\n            generalized.weights: 一个列表，包含每次重复的神经网络的广义权重。<br/>\n            result.matrix: 一个矩阵，包含每次重复达到的阈值、所需步骤、误差、AIC和BIC（如果计算）和权重。每列代表一次重复。<br/>\n            startweights: 一个列表，包含每次重复的神经网络的起始权重。<br/>\n            ​<b>示例</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# 二元分类\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## 不运行：print(nn)\n            ## 不运行：plot(nn)\n            # 多类分类\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## 不运行：print(nn)\n            ## 不运行：plot(nn)\n            # 自定义激活函数\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## 不运行：print(nn)\n            ## 不运行：plot(nn)\n            </code> <br/>\n            <b>包</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>帮助</b></br>\n            要详细帮助，请单击此对话框覆盖右上角的R图标，或在R语法编辑器中运行以下命令</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}