{
  "title": "Sinir Ağlarını Eğitme, neuralnet paketini kullanarak",
  "navigation": "Sinir Ağları",
  "label1": "LÜTFEN DUMMY KODU FAKTÖR DEĞİŞKENLERİ, VERİ > DUMMY DEĞİŞKENLERİ HESAPLA'YA GİDİN (TÜM SEVİYELERİ KORUYUN A.K.A 1 Sıcak Kodlama). SAYISAL DEĞİŞKENLERİ ÖLÇEKLENDİRİN VE MERKEZİYETİNİ SAĞLAYIN, VERİ > DEĞİŞKENLERİ STANDARDİZE ET'YE GİDİN",
  "model": "Bir model adı girin",
  "dependentvar": "Bağımlı değişken",
  "independentvars": "Bağımsız değişken(ler)",
  "seed": "Tohum ayarla",
  "iter": "Öğrenme için maksimum adım",
  "tf": "Algoritma",
  "threshold": "Eşik",
  "label2": "Gizli katman sayısı ve her gizli katmandaki nöron sayısı",
  "layers": "Her katmandaki nöron sayısını belirtin, örneğin 1. 1 katmanda 5 nöron için 5 girin. 1. katmanda 5 nöron, 2. katmanda 6 nöron, 3. katmanda 7 nöron için 5,6,7 girin",
  "OutActFunc": "Bir çıkış aktivasyon fonksiyonu belirtin",
  "rep": "Sinir ağı eğitimi için tekrar sayısı",
  "label3": "Üst ve alt öğrenme oranı için çarpanlar",
  "minus": "Üst (eksi)",
  "upper": "Alt (artı)",
  "lifesign": "Sinir ağının hesaplanması sırasında ne kadar yazdırılacağını ayarlama",
  "lifesignstep": "Tam yaşam işareti modunda minimum eşiği yazdırmak için adım boyutu",
  "errfct": "Hatanın hesaplanması için kullanılan türevlenebilir fonksiyon",
  "linearoutput": "Aktivasyon fonksiyonu çıkış nöronlarına uygulanmamalıdır",
  "likelihood": "Olasılık",
  "advanced_lbl": "Gelişmiş",
  "help": {
    "title": "Sinir Ağlarını Eğitme, neuralnet paketini kullanarak",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>NOT</b></br>\n            Tek bir bağımlı değişken belirtirken, bu sayısal veya faktör olabilir. Belirtilen bağımlı değişken bir faktörse, faktör değişkenini otomatik olarak bir sıcak kodlama ile dummy kodluyoruz, RSNNS paketindeki decode fonksiyonunu kullanarak.</br></br>\n            Ayrıca, bir faktör değişkenini dummy kodlamak için bir sıcak kodlama kullanıyorsanız, diyalogda birden fazla bağımlı değişken belirtebilirsiniz. Bu durumda, bağımlı değişkenler sayısal türde olmalıdır.</br></br>\n            \"Veri > Dummy Değişkenleri Hesapla\" seçeneğini kullanarak, \"Tüm seviyeleri koru\" ayarını seçerek bir sıcak kodlama elde edebilirsiniz.</br></br>\n            Faktör türündeki bağımlı değişkenler için, oluşturulan model kullanılarak bir veri kümesini puanlarken bir karışıklık matrisini, ROC ve model doğruluk istatistiklerini görüntüleyeceğiz. Üretilen tahminler faktör türündedir çünkü sınıfı tahmin ediyoruz. Bu tahminler, puanlama sırasında tahmin edilen olasılıklarla birlikte veri kümesine kaydedilecektir.</br></br>\n            Dummy kodlanmış bağımlı değişkenler olduğunda, oluşturulan model kullanılarak bir veri kümesini puanlarken bir karışıklık matrisini, ROC ve model doğruluk istatistiklerini görüntülemeyeceğiz. Ancak, tahminler veri kümesini puanlarken veri kümesine kaydedilecektir. Tahminler, dummy kodlanmış bağımlı değişkenlerle ilişkili olasılıklardır.</br></br>\n            Bağımsız değişkenleri standartlaştırmak genellikle en iyisidir (bunlar da sayısal olmalıdır). \"Veri > Değişkenleri Standardize Et\" seçeneğine bakın.</br></br>\n            Kategorik bağımsız değişkenleriniz varsa, faktör değişkenlerini dummy kodlamak için bir sıcak kodlama kullanın.</br></br>\n            <b>Açıklama</b></br>\n            Sinir ağlarını geri yayılım, dirençli geri yayılım (RPROP) ile (Riedmiller, 1994) veya ağırlık geri izleme olmadan (Riedmiller ve Braun, 1993) veya Anastasiadis ve ark. (2005) tarafından modifiye edilmiş küresel olarak yakınsayan versiyon (GRPROP) kullanarak eğitin. Fonksiyon, hata ve aktivasyon fonksiyonu için özel seçimler aracılığıyla esnek ayarlar sağlar. Ayrıca, genelleştirilmiş ağırlıkların hesaplanması (Intrator O. ve Intrator N., 1993) uygulanmıştır.\n            <b>Kullanım</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>Argümanlar</b><br/>\n            <ul>\n            <li>\n            formula: uyum sağlanacak modelin sembolik tanımı.\n            </li>\n            <li>\n            data: formülde belirtilen değişkenleri içeren bir veri çerçevesi.\n            </li>\n            <li>\n            hidden: her katmandaki gizli nöronların (düğüm) sayısını belirten bir tam sayı vektörü.\n            </li>\n            <li>\n            threshold: durdurma kriteri olarak hata fonksiyonunun kısmi türevleri için belirli bir sayısal değer.\n            </li>\n            <li>\n            stepmax: sinir ağının eğitimi için maksimum adımlar. Bu maksimuma ulaşmak, sinir ağının eğitim sürecinin durmasına neden olur.\n            </li>\n            <li>\n            rep: sinir ağının eğitimi için tekrar sayısı.\n            </li>\n            <li>\n            startweights: ağırlıklar için başlangıç değerlerini içeren bir vektör. Rastgele başlatma için NULL olarak ayarlayın.\n            </li>\n            <li>\n            learningrate.limit: öğrenme oranı için en düşük ve en yüksek sınırı içeren bir vektör veya liste. Sadece RPROP ve GRPROP için kullanılır.</li>\n            <li>\n            learningrate.factor: üst ve alt öğrenme oranı için çarpanları içeren bir vektör veya liste. Sadece RPROP ve GRPROP için kullanılır.\n            </li>\n            <li>\n            learningrate: geleneksel geri yayılım tarafından kullanılan öğrenme oranını belirten bir sayısal değer. Sadece geleneksel geri yayılım için kullanılır.\n            </li>\n            <li>\n            lifesign: sinir ağının hesaplanması sırasında ne kadar yazdırılacağını belirten bir dize. 'none', 'minimal' veya 'full'.\n            </li>\n            <li>\n            lifesign.step: tam yaşam işareti modunda minimum eşiği yazdırmak için bir tam sayı.\n            </li>\n            <li>\n            algorithm: sinir ağını hesaplamak için kullanılan algoritma türünü içeren bir dize. Aşağıdaki türler mümkündür: 'backprop', 'rprop+', 'rprop-', 'sag' veya 'slr'. 'backprop' geri yayılımı ifade eder, 'rprop+' ve 'rprop-' ağırlık geri izleme ile ve olmadan dirençli geri yayılımı ifade ederken, 'sag' ve 'slr' modifiye edilmiş küresel yakınsayan algoritmanın (grprop) kullanımını teşvik eder. Daha fazla bilgi için Ayrıntılara bakın.\n            </li>\n            <li>\n            err.fct: hatanın hesaplanması için kullanılan bir türevlenebilir fonksiyon. Alternatif olarak, 'sse' ve 'ce' dizeleri, kare hataların toplamı ve çapraz entropi için kullanılabilir.\n            </li>\n            <li>\n            act.fct: kovaryantın veya nöronların ve ağırlıkların çarpımının sonucunu düzleştirmek için kullanılan bir türevlenebilir fonksiyon. Ayrıca, 'logistic' ve 'tanh' dizeleri, lojistik fonksiyon ve hiperbolik tanjant için mümkündür.\n            </li>\n            <li>\n            linear.output: mantıksal. Eğer act.fct çıkış nöronlarına uygulanmamalıysa, lineer çıktıyı TRUE olarak ayarlayın, aksi takdirde FALSE olarak ayarlayın.\n            </li>\n            <li>\n            exclude: hesaplamadan hariç tutulan ağırlıkları belirten bir vektör veya matris. Bir vektör olarak verilirse, ağırlıkların kesin konumları bilinmelidir. n-satır ve 3 sütunlu bir matris, n ağırlığı hariç tutacaktır; burada ilk sütun katmanı, ikinci sütun giriş nöronunu ve üçüncü sütun ağırlığın çıkış nöronunu temsil eder.\n            </li>\n            <li>\n            constant.weights: eğitim sürecinden hariç tutulan ve sabit olarak muamele edilen ağırlıkların değerlerini belirten bir vektör.\n            </li>\n            <li>\n            likelihood: mantıksal. Hata fonksiyonu negatif log-olasılık fonksiyonuna eşitse, bilgi kriterleri AIC ve BIC hesaplanacaktır. Ayrıca, güven aralığının kullanımı anlamlıdır.\n            </li>\n            </ul>\n            <b>Ayrıntılar</b><br/>\n            Küresel olarak yakınsayan algoritma, ağırlık geri izleme olmadan dirençli geri yayılım temelinde oluşturulmuştur ve ayrıca bir öğrenme oranını, ya en küçük mutlak gradyana (sag) ya da en küçük öğrenme oranına (slr) bağlı olarak değiştirir. grprop algoritmasındaki öğrenme oranları, learningrate.limit'te tanımlanan sınırlara göre sınırlıdır.\n            ​<b>Değer</b><br/>\n            neuralnet, nn sınıfının bir nesnesini döndürür. nn sınıfının bir nesnesi, en fazla aşağıdaki bileşenleri içeren bir listedir:<br/>\n            çağrı: eşleşen çağrı.<br/>\n            yanıt: veri argümanından çıkarılmıştır.<br/>\n            kovaryant: veri argümanından çıkarılan değişkenler.<br/>\n            model.list: formül argümanından çıkarılan kovaryantlar ve yanıt değişkenlerini içeren bir liste.<br/>\n            err.fct: hata fonksiyonu.<br/>\n            act.fct: aktivasyon fonksiyonu.<br/>\n            data: veri argümanı.<br/>\n            net.result: her tekrar için sinir ağının genel sonucunu içeren bir liste.<br/>\n            weights: her tekrar için sinir ağının uyumlu ağırlıklarını içeren bir liste.<br/>\n            generalized.weights: her tekrar için sinir ağının genelleştirilmiş ağırlıklarını içeren bir liste.<br/>\n            result.matrix: her tekrar için ulaşılan eşik, gereken adımlar, hata, AIC ve BIC (hesaplanmışsa) ve ağırlıkları içeren bir matris. Her sütun bir tekrarı temsil eder.<br/>\n            startweights: her tekrar için sinir ağının başlangıç ağırlıklarını içeren bir liste.<br/>\n            ​<b>Örnekler</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# İkili sınıflandırma\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Çalıştırılmadı: print(nn)\n            ## Çalıştırılmadı: plot(nn)\n            # Çoklu sınıflandırma\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Çalıştırılmadı: print(nn)\n            ## Çalıştırılmadı: plot(nn)\n            # Özel aktivasyon fonksiyonu\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## Çalıştırılmadı: print(nn)\n            ## Çalıştırılmadı: plot(nn)\n            </code> <br/>\n            <b>Paket</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>Yardım</b></br>\n            Ayrıntılı yardım için bu diyalog üstündeki sağ üst köşedeki R simgesine tıklayın veya R sözdizimi editöründe aşağıdaki komutu çalıştırın</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}