{
  "title": "Aşırı Gradient Artırma",
  "navigation": "Aşırı Gradient Artırma",
  "label1": "DUMMY KOD FAKTÖR DEĞİŞKENLERİ, DEĞİŞKENLER>HESAPLA>DUMMY KOD KULLANIM SEÇENEĞİNE GİDİN VE TÜM SEVİYELERİ KORUYUN. BİRKAÇ SINIFI TAHMİN ETMEK İÇİN YARDIMI OKUYUN (DİALOGDA ? İKONUNA TIKLAYIN).",
  "model": "Model adını girin",
  "dependentvar": "Bağımlı değişken",
  "independentvars": "Bağımsız değişken(ler)",
  "objective": "Amaç",
  "seed": "Tohum ayarla",
  "nrounds": "Maksimum artırma iterasyon sayısı",
  "maxdepth": "Maksimum derinlik",
  "minchildweight": "Minimum çocuk ağırlığı",
  "maxdeltastep": "Maksimum delta adımı",
  "eta": "eta (öğrenme oranı)",
  "gamma": "gamma",
  "numclasses": "Sınıf sayısı. **Yalnızca multi:softmax ve multi:softprob ile kullanın",
  "basescore": "Temel puan",
  "Verbose": "Ayrıntılı mod (0=Sessiz, 1=performans bilgisi, 2= diğer bilgiler)",
  "printevery": "Ayrıntılı > 0 olduğunda her n'inci iterasyon değerlendirme mesajını yazdır",
  "OptvarTaskparam": "Görev Parametreleri",
  "OptvarAdvDiagnostics": "Gelişmiş Tanılama",
  "OptvarTreeBoostparam": "Ağaç Artırma için Parametreler",
  "help": {
    "title": "Aşırı Gradient Artırma",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>Açıklama</b></br>\n                Aşırı Gradient Artırma modeli oluşturun\n                <br/>\n                <b>NOT</b></br>\n                1. Bağımlı değişkenin faktör türünde tahmin edilmesi için, bağımlı değişkeni 0'dan başlayan sayısal değerlere yeniden kodlamanız gerekir. Örneğin, faktör değişkeninde 3 seviye varsa, sayısal değişken 0,1,2 değerlerini içerecektir.</br>\n                Veriler > Değişkenleri Yeniden Kodla'ya bakın. Alternatif olarak, faktör değişkenini sayısala dönüştürün, genellikle seviyeler 1'den başlayarak tam sayılara eşlenir ve sonuç değişkeninden 1 çıkarın. Bu, 0'dan başlayan sayısal bir değişken verir.</br>\n                Ayrıca, amacı multi:softmax (sınıfları tahmin eder) veya multi:softprob (olasılıkları tahmin eder) olarak ayarlamanız ve sınıf sayısını sınıf sayısı metin kutusuna girmeniz gerekir.</br>\n                Sınıf sayısı yalnızca multi:softmax ve multi:softprob için girilmelidir. Diğer amaçlar için sınıf sayısı girildiğinde hatalar oluşacaktır.</br>\n                2. Bağımsız faktör değişkenlerini dummy kodlamanız gerekir, 1-Sıcak kodlama kullanın, veriler > Dummy Değişkenleri Hesapla'ya bakın.</br>\n                3. Seçilen amaç reg:squarederror, reg:logistic, binary:logitraw ve rank:pairwise olduğunda bir karışıklık matrisi ve ROC eğrisi oluşturulmaz çünkü tahmin fonksiyonu bağımlı değişkenin sınıfını döndürmez. Tahmin fonksiyonunun döndürdüğü sayısal tahminler (reg:squarederror durumunda), puanlar, sıralar vb. veri kümesinde saklanır. Daha fazla ayrıntı için help(predict.xgb.Booster) 'a bakın.</br>\n                4. Seçilen amaç multi:softprob olduğunda bir karışıklık matrisi oluşturulmaz çünkü tahmin fonksiyonu tahmin edilen olasılıkları döndürür ve bağımlı değişkenin sınıfını döndürmez. Tahmin edilen olasılıklar veri kümesinde saklanır ve ROC eğrisi oluşturulur. Karışıklık matrisini görmek için, amacı multi:softmax olarak seçin.</br>\n                5. Seçilen amaç multi:softmax olduğunda bir ROC eğrisi oluşturulmaz çünkü tahmin fonksiyonu sınıfı döndürür ve tahmin edilen olasılıkları döndürmez. ROC eğrisini görmek için amacı multi:softprob olarak seçin.</br>\n                <br/>\n                <b>Kullanım</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Argümanlar</b><br/>\n                <ul>\n                <li>\n                params: parametreler listesi. Tüm parametreler listesi http://xgboost.readthedocs.io/en/latest/parameter.html adresinde mevcuttur. Aşağıda daha kısa bir özet verilmiştir:<br/>\n                </li>\n                <li>\n                1. Genel Parametreler<br/>\n                Bu diyalog gbtree (ağaç artırıcı) kullanır<br/>\n                </li>\n                <li>\n                2. Artırıcı Parametreleri<br/>\n                2.1. Ağaç Artırıcı için Parametre<br/>\n                eta: öğrenme oranını kontrol edin: her ağacın katkısını mevcut yaklaşık değere eklenirken 0 < eta < 1 faktörü ile ölçeklendirin.<br/>\n                Aşırı uyumu önlemek için artırma sürecini daha temkinli hale getirir.<br/>\n                Düşük bir eta değeri, nrounds için daha büyük bir değer anlamına gelir: düşük eta değeri, modelin aşırı uyuma karşı daha dayanıklı ancak hesaplaması daha yavaş olmasını sağlar. Varsayılan: 0.3<br/>\n                gamma: bir yaprak düğümünde daha fazla bölme yapmak için gereken minimum kayıp azaltma. Daha büyükse, algoritma daha temkinli olacaktır.<br/>\n                max_depth: bir ağacın maksimum derinliği. Varsayılan: 6<br/>\n                min_child_weight: bir çocukta gerekli olan örnek ağırlığının (hessian) minimum toplamı. Ağaç bölme adımı, min_child_weight'ten daha az örnek ağırlığına sahip bir yaprak düğümüne neden olursa, o zaman inşa süreci daha fazla bölme yapmaktan vazgeçer. Doğrusal regresyon modunda, bu basitçe her düğümde bulunması gereken minimum örnek sayısına karşılık gelir. Daha büyükse, algoritma daha temkinli olacaktır. Varsayılan: 1<br/>\n                </li>\n                <li>\n                3. Görev Parametreleri<br/>\n                objective: öğrenme görevini ve ilgili öğrenme amacını belirtin, kullanıcılar buna kendi tanımladıkları bir fonksiyonu geçirebilir. Varsayılan amaç seçenekleri aşağıdadır:<br/>\n                reg:squarederror -Kare kaybı ile regresyon (Varsayılan).<br/>\n                reg:logistic lojistik regresyon.<br/>\n                binary:logistic -ikili sınıflandırma için lojistik regresyon. Çıktı olasılığı.<br/>\n                binary:logitraw -ikili sınıflandırma için lojistik regresyon, lojistik dönüşümden önceki puanı çıktı olarak verir.<br/>\n                num_class: sınıf sayısını ayarlayın. Sadece çoklu sınıf amaçları ile kullanmak için.<br/>\n                multi:softmax -xgboost'u softmax amacını kullanarak çoklu sınıf sınıflandırması yapması için ayarlayın. Sınıf bir sayı ile temsil edilir ve 0'dan num_class - 1'e kadar olmalıdır.<br/>\n                multi:softprob -softmax ile aynı, ancak tahmin çıktısı ndata * nclass elemanlarının bir vektörüdür, bu da daha sonra ndata, nclass matrisine yeniden şekillendirilebilir. Sonuç, her veri noktasının her sınıfa ait olma olasılıklarını içerir.<br/>\n                rank:pairwise -xgboost'u çift sıralama kaybını minimize ederek sıralama görevi yapması için ayarlayın.<br/>\n                base_score: tüm örneklerin başlangıç tahmin puanı, küresel yanlılık. Varsayılan: 0.5<br/>\n                eval_metric: doğrulama verileri için değerlendirme metrikleri. Kullanıcılar buna kendi tanımladıkları bir fonksiyonu geçirebilir. Varsayılan: metrik, amaca göre atanacaktır (regresyon için rmse,\n                ve sınıflandırma için hata, sıralama için ortalama ortalama doğruluk). Liste detay bölümünde sağlanmıştır.<br/>\n                data: eğitim veri seti. xgb.train yalnızca bir xgb.DMatrix'ı girdi olarak kabul eder. xgboost, ayrıca matris, dgCMatrix veya yerel bir veri dosyasının adını da kabul eder.<br/>\n                nrounds: maksimum artırma iterasyon sayısı.<br/>\n                verbose: Eğer 0 ise, xgboost sessiz kalır. Eğer 1 ise, performans hakkında bilgi yazdırır. Eğer 2 ise, bazı ek bilgiler yazdırılır. Not: verbose > 0 ayarlandığında otomatik olarak cb.print.evaluation(period=1) geri çağırma fonksiyonu etkinleştirilir.<br/>\n                print_every_n: ayrıntılı > 0 olduğunda her n'inci iterasyon değerlendirme mesajını yazdır. Varsayılan 1'dir, bu da tüm mesajların yazdırıldığı anlamına gelir. Bu parametre cb.print.evaluation geri çağırmasına geçilir.<br/>\n                label: yanıt değerleri vektörü. Veri bir yerel veri dosyası adı veya bir xgb.DMatrix olduğunda sağlanmamalıdır.<br/>\n                missing: varsayılan olarak NA olarak ayarlanmıştır, bu da NA değerlerinin algoritma tarafından 'eksik' olarak kabul edilmesi gerektiği anlamına gelir. Bazen, 0 veya diğer aşırı değerler eksik değerleri temsil etmek için kullanılabilir. Bu parametre yalnızca girdi yoğun bir matris olduğunda kullanılır.<br/>\n                </li>\n                </ul>\n                <b>Ayrıntılar</b></br>\n                Değerlendirme metriği, eval_metric parametresi sağlanmadığında Xgboost tarafından otomatik olarak seçilir.</br>\n                Kullanıcı bir veya birkaç eval_metric parametresi ayarlayabilir. Not: özelleştirilmiş bir metrik kullanırken, yalnızca bu tek metrik kullanılabilir. Aşağıda, Xgboost'un optimize edilmiş uygulamasını sağladığı yerleşik metriklerin listesi verilmiştir:</br>\n                rmse kök ortalama kare hatası. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss negatif log-olasılık. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss çoklu sınıf logloss. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error İkili sınıflandırma hata oranı. (# yanlış durum) / (# tüm durumlar) olarak hesaplanır. Varsayılan olarak, tahmin edilen değerler için negatif ve pozitif örnekleri tanımlamak için 0.5 eşiği kullanılır.</br>\n                Farklı bir eşik (örneğin, 0.) \"error@0.\" olarak belirtilebilir.</br>\n                merror Çoklu sınıf sınıflandırma hata oranı. (# yanlış durum) / (# tüm durumlar) olarak hesaplanır.</br>\n                auc Eğri altındaki alan. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve sıralama değerlendirmesi için.</br>\n                aucpr PR eğrisi altındaki alan. https://en.wikipedia.org/wiki/Precision_and_recall sıralama değerlendirmesi için.</br>\n                ndcg Normalleştirilmiş İndirilmiş Kümülatif Kazanç (sıralama görevi için). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                Belirli parametreler ayarlandığında otomatik olarak oluşturulan geri çağırmalar:</br>\n                cb.print.evaluation, verbose > 0 olduğunda etkinleştirilir; ve print_every_n parametresi buna geçilir.</br>\n                cb.evaluation.log, izleme listesi mevcut olduğunda açıktır.</br>\n                cb.early.stop: early_stopping_rounds ayarlandığında.</br>\n                cb.save.model: save_period > 0 ayarlandığında.</br></br>\n                <b>Değer</b></br>\n                Aşağıdaki öğelere sahip bir xgb.Booster sınıfı nesnesi:</br>\n                handle bellekteki xgboost modeline bir işaretçi (pointer).</br>\n                raw R'nin ham türü olarak kaydedilmiş xgboost modelinin önbellek bellek dökümü.</br>\n                niter: artırma iterasyon sayısı.</br>\n                evaluation_log: ilk sütunu iterasyon numarasına karşılık gelen bir data.table olarak saklanan değerlendirme geçmişi. cb.evaluation.log geri çağırması tarafından oluşturulur.</br>\n                call: bir fonksiyon çağrısı.</br>\n                params: xgboost kütüphanesine geçirilen parametreler. Not: cb.reset.parameters geri çağırması tarafından değiştirilen parametreleri yakalamaz.</br>\n                callbacks otomatik olarak atanan veya açıkça geçirilen geri çağırma fonksiyonları.</br>\n                best_iteration: en iyi değerlendirme metriği değerine sahip iterasyon numarası (yalnızca erken durdurma ile mevcuttur).</br>\n                best_ntreelimit: en iyi iterasyona karşılık gelen ntreelimit değeri, bu daha sonra tahmin yönteminde kullanılabilir (yalnızca erken durdurma ile mevcuttur).</br>\n                best_score: erken durdurma sırasında en iyi değerlendirme metriği değeri. (yalnızca erken durdurma ile mevcuttur).</br>\n                feature_names: eğitim veri seti özelliklerinin adları (yalnızca eğitim verilerinde sütun adları tanımlandığında).</br>\n                nfeatures: eğitim verilerindeki özellik sayısı.</br>\n                <b>Referanslar</b></br>\n                Tianqi Chen ve Carlos Guestrin, \"XGBoost: Ölçeklenebilir Bir Ağaç Artırma Sistemi\", 22. SIGKDD Bilgi Keşfi ve Veri Madenciliği Konferansı, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>Ayrıca Bakınız</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Örnekler</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## Basit bir xgb.train örneği:</br>\n                # Bu fonksiyonlar, bunları geçerek kullanılabilir:</br>\n                ## Bir 'xgboost' arayüzü örneği:</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}