{
    "title": "Multi-layer Perceptron, using RSNNS package",
    "navigation": "Multi-layer Perceptron",
    "label1": "PLEASE DUMMY CODE FACTOR VARIABLES, SEE DATA > COMPUTE DUMMY VARIABLES (KEEP ALL LEVELS A.K.A 1 HOT ENCODING). SCALE AND CENTER NUMERIC VARIABLES, SEE DATA > STANDARDIZE VARIABLES",
    "model": "Enter a model name",
    "dependentvar": "Dependent variable",
    "independentvars": "Independent variable(s)",
    "seed": "Set seed",
    "iter": "Max iterations to learn",
    "tf": "Learning function",
    "label2": "No of hidden layers and the neurons per hidden layer",
    "layers": "Specify the number of neurons in each layer, for example 1. For 5 neurons in 1 layer, enter 5 2. For 5 neurons in layer 1, 6 neurons in layer 2, 7 neurons in layer 3 enter 5,6,7",
    "learnfuncparams": "Learning function parameters",
    "help": {
        "title": "Multi-layer Perceptron, using RSNNS package",
        "r_help": "help(mlp, package ='RSNNS')",
        "body": "\n            <b>NOTE</b></br>\n            When you specify a single dependent variable, it can be numeric or factor. If the dependent variable specified is a factor, we automatically dummy code the factor variable using one-hot Encoding using the decode function in the RSNNS package.</br></br>\n            Additionally, if you are using one-hot encoding to dummy code a factor variable, you can specify more than one dependent variable in the dialog. In this case, the dependent variables must be of type numeric.</br></br>\n            You can use \"Data > Compute dummy variables\", choose the “Keep all levels” setting to get one-hot encoding.</br></br>\n            For dependent variables of type factor, we will display a confusion matrix, ROC and model accuracy statistics when scoring a dataset using the model built. The predictions generated are of type factor since we predict the class. These will be saved in the dataset along with the predicted probabilities when scoring.</br></br>\n            When there are dummy coded dependent variables, we will not display a confusion matrix, ROC and model accuracy statistics when scoring a dataset using the model built. However, the predictions will be saved in the dataset when scoring the dataset. The predictions are the probabilities associated with the dummy coded dependent variables.</br></br>\n            It usually best to standardize independent variables (they must be numeric, too) See “Data > Standardize Variables.”</br></br>\n            If you have categorical independent variables, use one-hot encoding to dummy code the factor variables.</br></br>\n            <b>Description</b></br>\n            This function creates a multilayer perceptron (MLP) and trains it. MLPs are fully connected feedforward networks, and probably the most common network architecture in use. Training is usually performed by error backpropagation or a related procedure.</br>\n            There are a lot of different learning functions present in SNNS that can be used together with this function, e.g., Std_Backpropagation, BackpropBatch, BackpropChunk, BackpropMomentum, BackpropWeightDecay, Rprop, Quickprop, SCG (scaled conjugate gradient), ...</br>\n            <b>Usage</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## Default S3 method:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>Arguments</b><br/>\n            <ul>\n            <li>\n            x: a matrix with training inputs for the network\n            </li>\n            <li>\n            ... : additional function parameters (currently not used)\n            </li>\n            <li>\n            y: the corresponding targets values\n            </li>\n            <li>\n            size: number of units in the hidden layer(s)\n            </li>\n            <li>\n            maxit: maximum of iterations to learn\n            </li>\n            <li>\n            initFunc: the initialization function to use\n            </li>\n            <li>\n            initFuncParams: the parameters for the initialization function\n            </li>\n            <li>\n            learnFunc: the learning function to use\n            </li>\n            <li>\n            learnFuncParams: the parameters for the learning function\n            </li>\n            <li>\n            updateFunc: the update function to use\n            </li>\n            <li>\n            updateFuncParams: the parameters for the update function\n            </li>\n            <li>\n            hiddenActFunc: the activation function of all hidden units\n            </li>\n            <li>\n            shufflePatterns: should the patterns be shuffled?\n            </li>\n            <li>\n            linOut: sets the activation function of the output units to linear or logistic (ignored if outputActFunc is given)\n            </li>\n            <li>\n            outputActFunc: the activation function of all output units\n            </li>\n            <li>\n            inputsTest: a matrix with inputs to test the network\n            </li>\n            <li>\n            targetsTest: the corresponding targets for the test input\n            </li>\n            <li>\n            pruneFunc: the pruning function to use\n            </li>\n            <li>\n            pruneFuncParams: the parameters for the pruning function. Unlike the other functions, these have to be given in a named list. See the pruning demos for further explanation.\n            </li>\n            </ul>\n            <b>Details</b></br>\n            Std_Backpropagation, BackpropBatch, e.g., have two parameters, the learning rate and the maximum output difference. The learning rate is usually a value between 0.1 and 1. It specifies the gradient descent step width. The maximum difference defines, how much difference between output and target value is treated as zero error, and not backpropagated. This parameter is used to prevent overtraining. For a complete list of the parameters of all the learning functions, see the SNNS User Manual, pp. 67.</br>\n            The defaults that are set for initialization and update functions usually don't have to be changed.</br>\n            <b>Value</b><br/>\n            an rsnns object.\n            <b>References</b><br/>\n            Rosenblatt, F. (1958), 'The perceptron: A probabilistic model for information storage and organization in the brain', Psychological Review 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Group, P. R. (1986), Parallel distributed processing :explorations in the microstructure of cognition, Mit, Cambridge, MA etc.<br/>\n            Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of Tübingen.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)<br/>\n            <b>Examples</b><br/>\n            <code> \n            data(iris)<br/>\n            #shuffle the vector<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #confusion matrix with 402040-method<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>Package</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>Help</b></br>\n            For detailed help click on the R icon on the top right hand side of this dialog overlay or run the following command in the R syntax editor</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
    }
}