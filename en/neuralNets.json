{
    "title": "Training Neuralnets, using neuralnet package",
    "navigation": "Neuralnets",
    "label1": "PLEASE DUMMY CODE FACTOR VARIABLES, SEE DATA > COMPUTE DUMMY VARIABLES (KEEP ALL LEVELS A.K.A 1 HOT ENCODING). SCALE AND CENTER NUMERIC VARIABLES, SEE DATA > STANDARDIZE VARIABLES",
    "model": "Enter a model name",
    "dependentvar": "Dependent variable",
    "independentvars": "Independent variable(s)",
    "seed": "Set seed",
    "iter": "Max steps for learning",
    "tf": "Algorithm",
    "threshold": "Threshold",
    "label2": "No of hiddden layers and the neurons per hidden layer",
    "layers": "Specify the number of neurons in each layer, for example 1. For 5 neurons in 1 layer, enter 5 2. For 5 neurons in layer 1, 6 neurons in layer 2, 7 neurons in layer 3 enter 5,6,7",
    "OutActFunc": "Specify an output activation function",
    "rep": "Repetitions for neural network training",
    "label3": "Multiplication factors for upper and lower learning rate",
    "minus": "Upper (minus)",
    "upper": "Lower (plus)",
    "lifesign": "Setting of how much to print during calculation of the neural net",
    "lifesignstep": "Stepsize to print the minimal threshold in full lifesign mode",
    "errfct": "Differentiable function used for calculation of the error",
    "linearoutput": "Activation function should not be applied to the output neurons",
    "likelihood": "Likelihood",
	"advanced_lbl" : "Advanced",
    "help": {
        "title": "Training Neuralnets, using neuralnet package",
        "r_help": "help(neuralnet, package='neuralnet')",
        "body": "\n            <b>NOTE</b></br>\n            When specifying a singe dependent variable, it  can be numeric or factor. If the dependent variable specified is a factor, we automatically dummy code the factor variable using one-hot Encoding using the decode function in the RSNNS package.</br></br>\n            Additionally, if you are using one-hot encoding to dummy code a factor variable, you can specify more than one dependent variable in the dialog. In this case, the dependent variables must be of type numeric.</br></br>\n            You can use \"Data > Compute dummy variables\", choose the “Keep all levels” setting to get one-hot encoding.</br></br>\n            For dependent variables of type factor, we will display a confusion matrix, ROC and model accuracy statistics when scoring a dataset using the model built. The predictions generated are of type factor since we predict the class. These will be saved in the dataset along with the predicted probabilities when scoring.</br></br>\n            When there are dummy coded dependent variables, we will not display a confusion matrix, ROC and model accuracy statistics when scoring a dataset using the model built. However, the predictions will be saved in the dataset when scoring the dataset. The predictions are the probabilities associated with the dummy coded dependent variables.</br></br>\n            It usually best to standardize independent variables (they must be numeric, too) See “Data > Standardize Variables.”</br></br>\n            If you have categorical independent variables, use one-hot encoding to dummy code the factor variables.</br></br>\n            <b>Description</b></br>\n            Train neural networks using backpropagation, resilient backpropagation (RPROP) with (Riedmiller, 1994) or without weight backtracking (Riedmiller and Braun, 1993) or the modified globally convergent version (GRPROP) by Anastasiadis et al. (2005). The function allows flexible settings through custom-choice of error and activation function. Furthermore, the calculation of generalized weights (Intrator O. and Intrator N., 1993) is implemented.\n            <b>Usage</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>Arguments</b><br/>\n            <ul>\n            <li>\n            formula: a symbolic description of the model to be fitted.\n            </li>\n            <li>\n            data: a data frame containing the variables specified in formula.\n            </li>\n            <li>\n            hidden: a vector of integers specifying the number of hidden neurons (vertices) in each layer.\n            </li>\n            <li>\n            threshold: a numeric value specifying the threshold for the partial derivatives of the error function as stopping criteria.\n            </li>\n            <li>\n            stepmax: the maximum steps for the training of the neural network. Reaching this maximum leads to a stop of the neural network's training process.\n            </li>\n            <li>\n            rep: the number of repetitions for the neural network's training.\n            </li>\n            <li>\n            startweights: a vector containing starting values for the weights. Set to NULL for random initialization.\n            </li>\n            <li>\n            learningrate.limit: a vector or a list containing the lowest and highest limit for the learning rate. Used only for RPROP and GRPROP.</li>\n            <li>\n            learningrate.factor: a vector or a list containing the multiplication factors for the upper and lower learning rate. Used only for RPROP and GRPROP.\n            </li>\n            <li>\n            learningrate: a numeric value specifying the learning rate used by traditional backpropagation. Used only for traditional backpropagation.\n            </li>\n            <li>\n            lifesign: a string specifying how much the function will print during the calculation of the neural network. 'none', 'minimal' or 'full'.\n            </li>\n            <li>\n            lifesign.step: an integer specifying the stepsize to print the minimal threshold in full lifesign mode.\n            </li>\n            <li>\n            algorithm: a string containing the algorithm type to calculate the neural network. The following types are possible: 'backprop', 'rprop+', 'rprop-', 'sag', or 'slr'. 'backprop' refers to backpropagation, 'rprop+' and 'rprop-' refer to the resilient backpropagation with and without weight backtracking, while 'sag' and 'slr' induce the usage of the modified globally convergent algorithm (grprop). See Details for more information.\n            </li>\n            <li>\n            err.fct: a differentiable function that is used for the calculation of the error. Alternatively, the strings 'sse' and 'ce' which stand for the sum of squared errors and the cross-entropy can be used.\n            </li>\n            <li>\n            act.fct: a differentiable function that is used for smoothing the result of the cross product of the covariate or neurons and the weights. Additionally the strings, 'logistic' and 'tanh' are possible for the logistic function and tangent hyperbolicus.\n            </li>\n            <li>\n            linear.output: logical. If act.fct should not be applied to the output neurons set linear output to TRUE, otherwise to FALSE.\n            </li>\n            <li>\n            exclude: a vector or a matrix specifying the weights, that are excluded from the calculation. If given as a vector, the exact positions of the weights must be known. A matrix with n-rows and 3 columns will exclude n weights, where the first column stands for the layer, the second column for the input neuron and the third column for the output neuron of the weight.\n            </li>\n            <li>\n            constant.weights: a vector specifying the values of the weights that are excluded from the training process and treated as fix.\n            </li>\n            <li>\n            likelihood: logical. If the error function is equal to the negative log-likelihood function, the information criteria AIC and BIC will be calculated. Furthermore the usage of confidence.interval is meaningful.\n            </li>\n            </ul>\n            <b>Details</b><br/>\n            The globally convergent algorithm is based on the resilient backpropagation without weight backtracking and additionally modifies one learning rate, either the learningrate associated with the smallest absolute gradient (sag) or the smallest learningrate (slr) itself. The learning rates in the grprop algorithm are limited to the boundaries defined in learningrate.limit.\n            ​<b>Value</b><br/>\n            neuralnet returns an object of class nn. An object of class nn is a list containing at most the following components:<br/>\n            call: the matched call.<br/>\n            response: extracted from the data argument.<br/>\n            covariate: the variables extracted from the data argument.<br/>\n            model.list: a list containing the covariates and the response variables extracted from the formula argument.<br/>\n            err.fct: the error function.<br/>\n            act.fct: the activation function.<br/>\n            data: the data argument.<br/>\n            net.result: a list containing the overall result of the neural network for every repetition.<br/>\n            weights: a list containing the fitted weights of the neural network for every repetition.<br/>\n            generalized.weights: a list containing the generalized weights of the neural network for every repetition.<br/>\n            result.matrix: a matrix containing the reached threshold, needed steps, error, AIC and BIC (if computed) and weights for every repetition. Each column represents one repetition.<br/>\n            startweights: a list containing the startweights of the neural network for every repetition.<br/>\n            ​<b>Examples</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# Binary classification\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Not run: print(nn)\n            ## Not run: plot(nn)\n            # Multiclass classification\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## Not run: print(nn)\n            ## Not run: plot(nn)\n            # Custom activation function\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## Not run: print(nn)\n            ## Not run: plot(nn)\n            </code> <br/>\n            <b>Package</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>Help</b></br>\n            For detailed help click on the R icon on the top right hand side of this dialog overlay or run the following command in the R syntax editor</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
    }
}