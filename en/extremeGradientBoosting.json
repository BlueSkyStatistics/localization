{
    "title": "Extreme Gradient Boosting",
    "navigation": "Extreme Gradient Boosting",
    "label1": "DUMMY CODE FACTOR VARIABLES, SEE VARIABLES>COMPUTE>DUMMY CODE USE OPTION KEEP ALL LEVELS. FOR PREDICTING MULTIPLE CLASSES, READ HELP (CLICK  ? ICON ON DIALOG).",
    "model": "Enter model name",
    "dependentvar": "Dependent variable",
    "independentvars": "Independent variable(s)",
    "objective": "Objective",
    "seed": "Set seed",
    "nrounds": "Max no of boosting iterations",
    "maxdepth": "Maximum depth",
    "minchildweight": "Minimum child weight",
    "maxdeltastep": "Maximum delta step",
    "eta": "eta (learning rate)",
    "gamma": "gamma",
    "numclasses": "Number of classes. **Use only with multi:softmax and multi:softprob",
    "basescore": "Base score",
    "Verbose": "Verbose mode (0=Silent, 1=performance info, 2= other info)",
    "printevery": "Print each n-th iteration evaluation messages when verbose > 0",
    "help": {
        "title": "Extreme Gradient Boosting",
        "r_help": "help(xgboost, package ='xgboost')",
        "body": "\n                <b>Description</b></br>\n                Create an eXtreme Gradient Boosting model\n                <br/>\n                <b>NOTE</b></br>\n                1.For predicting dependent variable of type factor, you need to recode the dependent variable to a numeric with values starting from 0. For example, if there are 3 levels in the factor variable, the numeric variable will contain values 0,1,2.</br>\n                See Data > Recode Variables. Alternately just convert the factor variable to numeric, typically the levels will get mapped to integers starting from 1 and subtract 1 from the resulting variable. This will give you a numeric variable with values starting from 0.</br>\n                You also need to set the objective to multi:softmax (predicts classes) or multi:softprob (predicts probabilities) and enter the  number of classes in the number of classes textbox.</br>\n                The number of classes must be entered only for multi:softmax and multi:softprob. Errors will be generated if number of classes is entered for the other objectives.</br>\n                2. You need to dummy code independent factor variables, use 1-Hot encoding see Data > Compute Dummy Variables</br>\n                3. A confusion matrix and ROC curve is not generated when the objective selected is one of reg:squarederror, reg:logistic, binary:logitraw and rank:pairwise as the predict function \n                does not return the class of the dependent variable. The numeric predictions (in the case of reg:squarederror), scores, ranks etc that the predict function returns are stored in the dataset. See help(predict.xgb.Booster) for more details</br>\n                4. A confusion matrix is not generated when the objective selected is multi:softprob as the predict function returns the predicted probabilities and not the class of the dependent variable. The predicted probabilities are saved in the dataset and the ROC curve is generated. To see the confusion matrix, select multi:softmax as the objective.</br>\n                5. A ROC curve is not generated when the objective of multi:softmax is selected as the predict function returns the class and not the predicted probabilities. To see the ROC curve select multi:softprob as the objective .</br>\n                <br/>\n                <b>Usage</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Arguments</b><br/>\n                <ul>\n                <li>\n                params: the list of parameters. The complete list of parameters is available at http://xgboost.readthedocs.io/en/latest/parameter.html. Below is a shorter summary:<br/>\n                </li>\n                <li>\n                1. General Parameters<br/>\n                This dialog uses gbtree (tree booster)<br/>\n                </li>\n                <li>\n                2. Booster Parameters<br/>\n                2.1. Parameter for Tree Booster<br/>\n                eta:  control the learning rate: scale the contribution of each tree by a factor of 0 < eta < 1 when it is added to the current approximation.<br/>\n                Used to prevent overfitting by making the boosting process more conservative.<br/>\n                Lower value for eta implies larger value for nrounds: low eta value means model more robust to overfitting but slower to compute. Default: 0.3<br/>\n                gamma: minimum loss reduction required to make a further partition on a leaf node of the tree. the larger, the more conservative the algorithm will be.<br/>\n                max_depth: maximum depth of a tree. Default: 6<br/>\n                min_child_weight: minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger, the more conservative the algorithm will be. Default: 1<br/>\n                </li>\n                <li>\n                3. Task Parameters<br/>\n                objective: specify the learning task and the corresponding learning objective, users can pass a self-defined function to it. The default objective options are below:<br/>\n                reg:squarederror -Regression with squared loss (Default).<br/>\n                reg:logistic logistic regression.<br/>\n                binary:logistic -logistic regression for binary classification. Output probability.<br/>\n                binary:logitraw -logistic regression for binary classification, output score before logistic transformation.<br/>\n                num_class: set the number of classes. To use only with multiclass objectives.<br/>\n                multi:softmax -set xgboost to do multiclass classification using the softmax objective. Class is represented by a number and should be from 0 to num_class - 1.<br/>\n                multi:softprob -same as softmax, but prediction outputs a vector of ndata * nclass elements, which can be further reshaped to ndata, nclass matrix. The result contains predicted probabilities of each data point belonging to each class.<br/>\n                rank:pairwise -set xgboost to do ranking task by minimizing the pairwise loss.<br/>\n                base_score: the initial prediction score of all instances, global bias. Default: 0.5<br/>\n                eval_metric: evaluation metrics for validation data. Users can pass a self-defined function to it. Default: metric will be assigned according to objective(rmse for regression,\n                and error for classification, mean average precision for ranking). List is provided in detail section.<br/>\n                data: training dataset. xgb.train accepts only an xgb.DMatrix as the input. xgboost, in addition, also accepts matrix, dgCMatrix, or name of a local data file.<br/>\n                nrounds: max number of boosting iterations.<br/>\n                verbose: If 0, xgboost will stay silent. If 1, it will print information about performance. If 2, some additional information will be printed out. Note that setting verbose > 0 automatically engages the cb.print.evaluation(period=1) callback function.<br/>\n                print_every_n: Print each n-th iteration evaluation messages when verbose>0. Default is 1 which means all messages are printed. This parameter is passed to the cb.print.evaluation callback.<br/>\n                label: vector of response values. Should not be provided when data is a local data file name or an xgb.DMatrix.<br/>\n                missing: by default is set to NA, which means that NA values should be considered as 'missing' by the algorithm. Sometimes, 0 or other extreme value might be used to represent missing values. This parameter is only used when input is a dense matrix.<br/>\n                </li>\n                </ul>\n                <b>Details</b></br>\n                The evaluation metric is chosen automatically by Xgboost (according to the objective) when the eval_metric parameter is not provided.</br>\n                User may set one or several eval_metric parameters. Note that when using a customized metric, only this single metric can be used. The following is the list of built-in metrics for which Xgboost provides optimized implementation:</br>\n                rmse root mean square error. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss negative log-likelihood. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss multiclass logloss. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error Binary classification error rate. It is calculated as (# wrong cases) / (# all cases). By default, it uses the 0.5 threshold for predicted values to define negative and positive instances.</br>\n                Different threshold (e.g., 0.) could be specified as \"error@0.\"</br>\n                merror Multiclass classification error rate. It is calculated as (# wrong cases) / (# all cases).</br>\n                auc Area under the curve. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve for ranking evaluation.</br>\n                aucpr Area under the PR curve. https://en.wikipedia.org/wiki/Precision_and_recall for ranking evaluation.</br>\n                ndcg Normalized Discounted Cumulative Gain (for ranking task). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                The following callbacks are automatically created when certain parameters are set:</br>\n                cb.print.evaluation is turned on when verbose > 0; and the print_every_n parameter is passed to it.</br>\n                cb.evaluation.log is on when watchlist is present.</br>\n                cb.early.stop: when early_stopping_rounds is set.</br>\n                cb.save.model: when save_period > 0 is set.</br></br>\n                <b>Value</b></br>\n                An object of class xgb.Booster with the following elements:</br>\n                handle a handle (pointer) to the xgboost model in memory.</br>\n                raw a cached memory dump of the xgboost model saved as R's raw type.</br>\n                niter: number of boosting iterations.</br>\n                evaluation_log: evaluation history stored as a data.table with the first column corresponding to iteration number and the rest corresponding to evaluation metrics' values. It is created by the cb.evaluation.log callback.</br>\n                call: a function call.</br>\n                params: parameters that were passed to the xgboost library. Note that it does not capture parameters changed by the cb.reset.parameters callback.</br>\n                callbacks callback functions that were either automatically assigned or explicitly passed.</br>\n                best_iteration: iteration number with the best evaluation metric value (only available with early stopping).</br>\n                best_ntreelimit: the ntreelimit value corresponding to the best iteration, which could further be used in predict method (only available with early stopping).</br>\n                best_score: the best evaluation metric value during early stopping. (only available with early stopping).</br>\n                feature_names: names of the training dataset features (only when column names were defined in training data).</br>\n                nfeatures: number of features in training data.</br>\n                <b>References</b></br>\n                Tianqi Chen and Carlos Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>See Also</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Examples</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## A simple xgb.train example:</br>\n                # These functions could be used by passing them either:</br>\n                ## An 'xgboost' interface example:</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
    }
}