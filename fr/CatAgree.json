{
  "title": "Accord Catégorique",
  "navigation": "Accord Catégorique",
  "ratervars": "Variables de Rater :",
  "cilevel": "Niveau de Confiance :",
  "options": "Avancé",
  "categLabels": "Spécifiez optionnellement toutes les évaluations possibles séparées par , par exemple 1,2,3,4,5 ou \"bas\",\"moyen\",\"élevé\" ",
  "weightschkbox": "Afficher les poids des catégories",
  "weights": "Sélectionner les poids des catégories",
  "N": "Taille de la population finie (le cas échéant)",
  "help": {
    "title": "Accord Catégorique",
    "r_help": "aide(fleiss.kappa.raw, paquet=irrCAC)",
    "body": "\n<b>Description</b></br>\nCalcule le pourcentage d'accord, le kappa de Conger/Cohen, le kappa de Fleiss, AC1/AC2 de Gwet, l'alpha de Krippendorff et le coefficient de Brennan-Prediger parmi plusieurs évaluateurs (2, 3, +) lorsque les données d'entrée représentent les évaluations brutes rapportées pour chaque sujet et chaque évaluateur. Des versions non pondérées et pondérées de ces statistiques sont disponibles, avec différentes méthodes de pondération des catégories.</br>\nLes valeurs manquantes sont intégrées dans les calculs d'accord aléatoire selon Gwet (2014).</br>\nNous offrons également la possibilité d'afficher les pondérations des catégories d'évaluation.</br>\nPour plus d'informations, voir : </br></br>Gwet, K.L. (2014, ISBN:978-0970806284). “Manuel de Fiabilité Inter-Rater,” 4ème Édition. Advanced Analytics, LLC\n</br></br>Klein, D. (2018) doi:https://doi.org/10.1177/1536867X1801800408. “Mise en œuvre d'un cadre général pour évaluer l'accord inter-rater dans Stata,” The Stata Journal Volume 18, Numéro 4, pp. 871-901.</br></br> \n<b>Utilisation</b>\n<br/>\n<br/>\n<ul>\n<li>\n<b>Variables de Rater :</b>  Variables correspondant à chaque évaluateur où chaque colonne représente un évaluateur et chaque ligne un sujet. Elles peuvent être numériques, facteurs, ordinales ou des variables de caractères.</br>\n</li>\n<li>\n<b>Niveau de Confiance :</b>  Le niveau de confiance associé à l'intervalle de confiance. Sa valeur par défaut est 0,95.</br>\n</li>\n<li>\n<b>Afficher les poids des catégories :</b> affiche les poids numériques pour les catégories d'évaluateurs selon les poids sélectionnés. Le poids sélectionné doit correspondre à la proximité souhaitée entre les catégories.\n</li>\n<li>\n<b>Sélectionner les poids des catégories :</b> Un paramètre obligatoire qui est soit une variable de chaîne soit une matrice. La chaîne décrit l'un des poids prédéfinis et doit prendre l'une des valeurs (\"non pondéré\",\"quadratique\", \"ordinal\", \"linéaire\", \"radical\", \"ratio\", \"circulaire\", \"bipolaire\"). </br>\nSi ce paramètre est une matrice, elle doit être une matrice carrée qxq où q est le nombre de catégories possibles dans lesquelles un sujet peut être classé. Si certaines des q catégories possibles ne sont pas utilisées, il est fortement conseillé de spécifier la liste complète des catégories possibles. Sinon, le programme peut ne pas fonctionner.</br>\nREMARQUE : La spécification d'une matrice n'est PAS prise en charge dans la syntaxe. Vous devez coller le code et modifier pour spécifier une matrice.</br>\n</li>\n<li>\n<b>Taille de la population finie (le cas échéant) :</b> Paramètre optionnel représentant la taille de la population (le cas échéant). Il peut être utilisé pour effectuer une correction de population finie à la variance. Sa valeur par défaut est l'infini.\n</li>\n<li>\n<b>Spécifiez optionnellement toutes les évaluations possibles :</b> Liste optionnelle utilisée dans le cas où aucun évaluateur n'utilise une catégorie possible. Si rien n'est spécifié, on suppose que les catégories observées constituent l'ensemble complet des catégories.\n</li>\n</ul>\n<b>Paquet R</b></br>\nirrCAC</br>\n<b>Aide</b></br>\nCette boîte de dialogue utilise les fonctions pa.coeff.raw, conger.kappa.raw, fleiss.kappa.raw, gwet.ac1.raw, krippen.alpha.raw et bp.coeff.raw.\nPour une aide détaillée, voir l'aide du paquet irrCAC dans le menu à trois points > Aide > Aide du paquet R.\n"
  }
}