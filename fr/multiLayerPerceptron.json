{
  "title": "Perceptron multicouche, utilisant le package RSNNS",
  "navigation": "Perceptron multicouche",
  "label1": "VEUILLEZ CODE DUMMY LES VARIABLES FACTORIELLES, VOIR DONNÉES > CALCULER DES VARIABLES DUMMY (GARDER TOUS LES NIVEAUX A.K.A ENCODAGE ONE HOT). ÉCHELLEZ ET CENTREZ LES VARIABLES NUMÉRIQUES, VOIR DONNÉES > STANDARDISER LES VARIABLES",
  "model": "Entrez un nom de modèle",
  "dependentvar": "Variable dépendante",
  "independentvars": "Variable(s) indépendante(s)",
  "seed": "Définir la graine",
  "iter": "Maximiser les itérations d'apprentissage",
  "tf": "Fonction d'apprentissage",
  "label2": "Nombre de couches cachées et de neurones par couche cachée",
  "layers": "Spécifiez le nombre de neurones dans chaque couche, par exemple 1. Pour 5 neurones dans 1 couche, entrez 5. Pour 5 neurones dans la couche 1, 6 neurones dans la couche 2, 7 neurones dans la couche 3, entrez 5,6,7.",
  "learnfuncparams": "Paramètres de la fonction d'apprentissage",
  "help": {
    "title": "Perceptron multicouche, utilisant le package RSNNS",
    "r_help": "help(mlp, package ='RSNNS')",
    "body": "\n            <b>REMARQUE</b></br>\n            Lorsque vous spécifiez une seule variable dépendante, elle peut être numérique ou factorielle. Si la variable dépendante spécifiée est un facteur, nous codons automatiquement la variable facteur en utilisant l'encodage one-hot avec la fonction decode dans le package RSNNS.</br></br>\n            De plus, si vous utilisez l'encodage one-hot pour coder une variable facteur, vous pouvez spécifier plus d'une variable dépendante dans la boîte de dialogue. Dans ce cas, les variables dépendantes doivent être de type numérique.</br></br>\n            Vous pouvez utiliser \"Données > Calculer des variables dummy\", choisissez le paramètre « Garder tous les niveaux » pour obtenir l'encodage one-hot.</br></br>\n            Pour les variables dépendantes de type facteur, nous afficherons une matrice de confusion, une ROC et des statistiques de précision du modèle lors de l'évaluation d'un ensemble de données à l'aide du modèle construit. Les prédictions générées sont de type facteur car nous prédisons la classe. Celles-ci seront enregistrées dans l'ensemble de données avec les probabilités prédites lors de l'évaluation.</br></br>\n            Lorsqu'il y a des variables dépendantes codées en dummy, nous n'afficherons pas de matrice de confusion, de ROC et de statistiques de précision du modèle lors de l'évaluation d'un ensemble de données à l'aide du modèle construit. Cependant, les prédictions seront enregistrées dans l'ensemble de données lors de l'évaluation de l'ensemble de données. Les prédictions sont les probabilités associées aux variables dépendantes codées en dummy.</br></br>\n            Il est généralement préférable de standardiser les variables indépendantes (elles doivent également être numériques). Voir « Données > Standardiser les variables. »</br></br>\n            Si vous avez des variables indépendantes catégorielles, utilisez l'encodage one-hot pour coder les variables facteur.</br></br>\n            <b>Description</b></br>\n            Cette fonction crée un perceptron multicouche (MLP) et l'entraîne. Les MLP sont des réseaux entièrement connectés en avant, et probablement l'architecture de réseau la plus courante utilisée. L'entraînement est généralement effectué par rétropropagation d'erreur ou une procédure connexe.</br>\n            Il existe de nombreuses fonctions d'apprentissage différentes présentes dans SNNS qui peuvent être utilisées avec cette fonction, par exemple, Std_Backpropagation, BackpropBatch, BackpropChunk, BackpropMomentum, BackpropWeightDecay, Rprop, Quickprop, SCG (gradient conjugué mis à l'échelle), ...</br>\n            <b>Utilisation</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## Méthode S3 par défaut:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>Arguments</b><br/>\n            <ul>\n            <li>\n            x: une matrice avec des entrées d'entraînement pour le réseau\n            </li>\n            <li>\n            ... : paramètres de fonction supplémentaires (actuellement non utilisés)\n            </li>\n            <li>\n            y: les valeurs cibles correspondantes\n            </li>\n            <li>\n            size: nombre d'unités dans la ou les couches cachées\n            </li>\n            <li>\n            maxit: maximum d'itérations pour apprendre\n            </li>\n            <li>\n            initFunc: la fonction d'initialisation à utiliser\n            </li>\n            <li>\n            initFuncParams: les paramètres pour la fonction d'initialisation\n            </li>\n            <li>\n            learnFunc: la fonction d'apprentissage à utiliser\n            </li>\n            <li>\n            learnFuncParams: les paramètres pour la fonction d'apprentissage\n            </li>\n            <li>\n            updateFunc: la fonction de mise à jour à utiliser\n            </li>\n            <li>\n            updateFuncParams: les paramètres pour la fonction de mise à jour\n            </li>\n            <li>\n            hiddenActFunc: la fonction d'activation de toutes les unités cachées\n            </li>\n            <li>\n            shufflePatterns: les motifs doivent-ils être mélangés ?\n            </li>\n            <li>\n            linOut: définit la fonction d'activation des unités de sortie sur linéaire ou logistique (ignoré si outputActFunc est donné)\n            </li>\n            <li>\n            outputActFunc: la fonction d'activation de toutes les unités de sortie\n            </li>\n            <li>\n            inputsTest: une matrice avec des entrées pour tester le réseau\n            </li>\n            <li>\n            targetsTest: les cibles correspondantes pour l'entrée de test\n            </li>\n            <li>\n            pruneFunc: la fonction de taille à utiliser\n            </li>\n            <li>\n            pruneFuncParams: les paramètres pour la fonction de taille. Contrairement aux autres fonctions, ceux-ci doivent être donnés dans une liste nommée. Voir les démos de taille pour plus d'explications.\n            </li>\n            </ul>\n            <b>Détails</b></br>\n            Std_Backpropagation, BackpropBatch, par exemple, ont deux paramètres, le taux d'apprentissage et la différence de sortie maximale. Le taux d'apprentissage est généralement une valeur comprise entre 0,1 et 1. Il spécifie la largeur du pas de descente de gradient. La différence maximale définit combien de différence entre la sortie et la valeur cible est traitée comme une erreur nulle, et non rétropropagée. Ce paramètre est utilisé pour éviter le surentraînement. Pour une liste complète des paramètres de toutes les fonctions d'apprentissage, voir le manuel de l'utilisateur SNNS, pp. 67.</br>\n            Les valeurs par défaut qui sont définies pour les fonctions d'initialisation et de mise à jour n'ont généralement pas besoin d'être modifiées.</br>\n            <b>Valeur</b><br/>\n            un objet rsnns.\n            <b>Références</b><br/>\n            Rosenblatt, F. (1958), 'Le perceptron : un modèle probabiliste pour le stockage et l'organisation de l'information dans le cerveau', Revue Psychologique 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Groupe, P. R. (1986), Traitement distribué parallèle : explorations dans la microstructure de la cognition, Mit, Cambridge, MA, etc.<br/>\n            Zell, A. et al. (1998), 'Manuel de l'utilisateur du simulateur de réseau de neurones SNNS Stuttgart, version 4.2', IPVR, Université de Stuttgart et WSI, Université de Tübingen.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), Simulation des réseaux neuronaux, Addison-Wesley. (en allemand)<br/>\n            <b>Exemples</b><br/>\n            <code> \n            data(iris)<br/>\n            #mélanger le vecteur<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #matrice de confusion avec méthode 402040<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>Paquet</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>Aide</b></br>\n            Pour une aide détaillée, cliquez sur l'icône R dans le coin supérieur droit de cette superposition de dialogue ou exécutez la commande suivante dans l'éditeur de syntaxe R</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
  }
}