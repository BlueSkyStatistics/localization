{
  "title": "ニューラルネットのトレーニング、neuralnetパッケージを使用",
  "navigation": "ニューラルネット",
  "label1": "ダミーコードのファクタ変数を作成してください。データ > ダミー変数を計算する（すべてのレベルを保持、すなわち1ホットエンコーディング）。数値変数をスケールおよびセンタリングしてください。データ > 変数を標準化するを参照。",
  "model": "モデル名を入力してください",
  "dependentvar": "従属変数",
  "independentvars": "独立変数",
  "seed": "シードを設定",
  "iter": "学習の最大ステップ数",
  "tf": "アルゴリズム",
  "threshold": "しきい値",
  "label2": "隠れ層の数と隠れ層ごとのニューロン数",
  "layers": "各層のニューロン数を指定してください。例えば、1層に5ニューロンの場合は5と入力します。1層目に5ニューロン、2層目に6ニューロン、3層目に7ニューロンの場合は5,6,7と入力します。",
  "OutActFunc": "出力活性化関数を指定してください",
  "rep": "ニューラルネットトレーニングの繰り返し回数",
  "label3": "上限および下限学習率の乗数因子",
  "minus": "上限（マイナス）",
  "upper": "下限（プラス）",
  "lifesign": "ニューラルネットの計算中にどれだけ印刷するかの設定",
  "lifesignstep": "フルライフサインモードで最小しきい値を印刷するためのステップサイズ",
  "errfct": "誤差の計算に使用される微分可能な関数",
  "linearoutput": "出力ニューロンに活性化関数を適用しない",
  "likelihood": "尤度",
  "advanced_lbl" : "高度 (こうど)",
  "help": {
    "title": "ニューラルネットのトレーニング、neuralnetパッケージを使用",
    "r_help": "help(neuralnet, package='neuralnet')",
    "body": "\n            <b>注意</b></br>\n            単一の従属変数を指定する場合、それは数値またはファクタであることができます。指定された従属変数がファクタの場合、RSNNSパッケージのデコード関数を使用して、ダミーコードのファクタ変数を自動的に1ホットエンコーディングします。</br></br>\n            さらに、ファクタ変数をダミーコードするために1ホットエンコーディングを使用している場合、ダイアログで複数の従属変数を指定できます。この場合、従属変数は数値型でなければなりません。</br></br>\n            \"データ > ダミー変数を計算する\"を使用し、\"すべてのレベルを保持\"設定を選択して1ホットエンコーディングを取得します。</br></br>\n            ファクタ型の従属変数の場合、構築されたモデルを使用してデータセットをスコアリングする際に、混同行列、ROC、およびモデル精度統計を表示します。生成された予測はファクタ型であり、クラスを予測します。これらはスコアリング時に予測確率と共にデータセットに保存されます。</br></br>\n            ダミーコードされた従属変数がある場合、構築されたモデルを使用してデータセットをスコアリングする際に、混同行列、ROC、およびモデル精度統計は表示されません。ただし、スコアリング時にデータセットに予測が保存されます。予測はダミーコードされた従属変数に関連する確率です。</br></br>\n            独立変数は標準化するのが最良です（それらも数値でなければなりません）。\"データ > 変数を標準化する\"を参照してください。</br></br>\n            カテゴリカルな独立変数がある場合、ファクタ変数をダミーコードするために1ホットエンコーディングを使用してください。</br></br>\n            <b>説明</b></br>\n            バックプロパゲーション、レジリエントバックプロパゲーション（RPROP）を使用してニューラルネットワークをトレーニングします（Riedmiller, 1994）または重みのバックトラッキングなし（Riedmiller and Braun, 1993）またはAnastasiadis et al.（2005）による修正されたグローバル収束バージョン（GRPROP）。この関数は、エラーおよび活性化関数のカスタム選択を通じて柔軟な設定を可能にします。さらに、一般化された重みの計算（Intrator O.およびIntrator N., 1993）が実装されています。\n            <b>使用法</b>\n            <br/>\n            <code> \n            neuralnet(formula, data, hidden = 1, threshold = 0.01,\n              stepmax = 1e+05, rep = 1, startweights = NULL,\n              learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,\n              plus = 1.2), learningrate = NULL, lifesign = \"none\",\n              lifesign.step = 1000, algorithm = \"rprop+\", err.fct = \"sse\",\n              act.fct = \"logistic\", linear.output = TRUE, exclude = NULL,\n              constant.weights = NULL, likelihood = FALSE)\n            </code> <br/>\n            <b>引数</b><br/>\n            <ul>\n            <li>\n            formula: フィッティングされるモデルの象徴的な説明。\n            </li>\n            <li>\n            data: formulaで指定された変数を含むデータフレーム。\n            </li>\n            <li>\n            hidden: 各層の隠れニューロン（頂点）の数を指定する整数のベクトル。\n            </li>\n            <li>\n            threshold: 停止基準としての誤差関数の偏微分のしきい値を指定する数値。\n            </li>\n            <li>\n            stepmax: ニューラルネットワークのトレーニングの最大ステップ数。これに達すると、ニューラルネットワークのトレーニングプロセスが停止します。\n            </li>\n            <li>\n            rep: ニューラルネットワークのトレーニングの繰り返し回数。\n            </li>\n            <li>\n            startweights: 重みの開始値を含むベクトル。NULLに設定するとランダム初期化されます。\n            </li>\n            <li>\n            learningrate.limit: 学習率の最低および最高限界を含むベクトルまたはリスト。RPROPおよびGRPROPにのみ使用されます。</li>\n            <li>\n            learningrate.factor: 上限および下限学習率の乗数因子を含むベクトルまたはリスト。RPROPおよびGRPROPにのみ使用されます。\n            </li>\n            <li>\n            learningrate: 従来のバックプロパゲーションによって使用される学習率を指定する数値。従来のバックプロパゲーションにのみ使用されます。\n            </li>\n            <li>\n            lifesign: ニューラルネットワークの計算中にどれだけ印刷するかを指定する文字列。'none'、'minimal'、または'full'。\n            </li>\n            <li>\n            lifesign.step: フルライフサインモードで最小しきい値を印刷するためのステップサイズを指定する整数。\n            </li>\n            <li>\n            algorithm: ニューラルネットワークを計算するためのアルゴリズムタイプを含む文字列。次のタイプが可能です：'backprop'、'rprop+'、'rprop-'、'sag'、または'slr'。 'backprop'はバックプロパゲーションを指し、'rprop+'および'rprop-'は重みのバックトラッキングの有無に関するレジリエントバックプロパゲーションを指し、'sag'および'slr'は修正されたグローバル収束アルゴリズム（grprop）の使用を誘導します。詳細については、詳細を参照してください。\n            </li>\n            <li>\n            err.fct: 誤差の計算に使用される微分可能な関数。代わりに、'sse'および'ce'という文字列を使用することができます。これらはそれぞれ二乗誤差の合計と交差エントロピーを表します。\n            </li>\n            <li>\n            act.fct: 共変量またはニューロンと重みのクロスプロダクトの結果を平滑化するために使用される微分可能な関数。さらに、'logistic'および'tanh'という文字列がロジスティック関数および双曲線正接に対して可能です。\n            </li>\n            <li>\n            linear.output: 論理。出力ニューロンにact.fctを適用しない場合はlinear outputをTRUEに設定し、そうでない場合はFALSEに設定します。\n            </li>\n            <li>\n            exclude: 計算から除外される重みを指定するベクトルまたは行列。ベクトルとして与えられた場合、重みの正確な位置を知っている必要があります。n行と3列の行列はn個の重みを除外し、最初の列は層、2番目の列は入力ニューロン、3番目の列は重みの出力ニューロンを表します。\n            </li>\n            <li>\n            constant.weights: トレーニングプロセスから除外され、固定として扱われる重みの値を指定するベクトル。\n            </li>\n            <li>\n            likelihood: 論理。誤差関数が負の対数尤度関数と等しい場合、情報基準AICおよびBICが計算されます。さらに、confidence.intervalの使用が意味を持ちます。\n            </li>\n            </ul>\n            <b>詳細</b><br/>\n            グローバル収束アルゴリズムは、重みのバックトラッキングなしのレジリエントバックプロパゲーションに基づいており、さらに1つの学習率を修正します。最小の絶対勾配に関連する学習率（sag）または最小学習率（slr）自体のいずれかです。grpropアルゴリズムの学習率は、learningrate.limitで定義された境界に制限されます。\n            ​<b>値</b><br/>\n            neuralnetはnnクラスのオブジェクトを返します。nnクラスのオブジェクトは、最大で次のコンポーネントを含むリストです：<br/>\n            call: 一致した呼び出し。<br/>\n            response: データ引数から抽出。<br/>\n            covariate: データ引数から抽出された変数。<br/>\n            model.list: formula引数から抽出された共変量と応答変数を含むリスト。<br/>\n            err.fct: 誤差関数。<br/>\n            act.fct: 活性化関数。<br/>\n            data: データ引数。<br/>\n            net.result: 各繰り返しのニューラルネットワークの全体的な結果を含むリスト。<br/>\n            weights: 各繰り返しのニューラルネットワークのフィッティングされた重みを含むリスト。<br/>\n            generalized.weights: 各繰り返しのニューラルネットワークの一般化された重みを含むリスト。<br/>\n            result.matrix: 各繰り返しのしきい値、必要なステップ、誤差、AICおよびBIC（計算された場合）および重みを含む行列。各列は1つの繰り返しを表します。<br/>\n            startweights: 各繰り返しのニューラルネットワークの開始重みを含むリスト。<br/>\n            ​<b>例</b><br/>\n            <code> \n            ​library(neuralnet)\n            ​# バイナリ分類\n            nn <- neuralnet(Species == \"setosa\" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## 実行しない: print(nn)\n            ## 実行しない: plot(nn)\n            # マルチクラス分類\n            nn <- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)\n            ## 実行しない: print(nn)\n            ## 実行しない: plot(nn)\n            # カスタム活性化関数\n            softplus <- function(x) log(1 + exp(x))\n            nn <- neuralnet((Species == \"setosa\") ~ Petal.Length + Petal.Width, iris, \n                            linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)\n            ## 実行しない: print(nn)\n            ## 実行しない: plot(nn)\n            </code> <br/>\n            <b>パッケージ</b></br>\n            neuralnet;NeuralNetTools</br>\n            <b>ヘルプ</b></br>\n            詳細なヘルプについては、このダイアログオーバーレイの右上にあるRアイコンをクリックするか、R構文エディタで次のコマンドを実行してください</br>\n            help(neuralnet, package='neuralnet')\n\t\t\t"
  }
}