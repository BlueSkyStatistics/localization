{
  "title": "多層感知器，使用RSNNS套件",
  "navigation": "多層感知器",
  "label1": "請對因子變數進行虛擬編碼，參見數據 > 計算虛擬變數（保留所有級別即1熱編碼）。縮放和中心化數值變數，參見數據 > 標準化變數",
  "model": "輸入模型名稱",
  "dependentvar": "因變數",
  "independentvars": "自變數",
  "seed": "設置種子",
  "iter": "最大學習迭代次數",
  "tf": "學習函數",
  "label2": "隱藏層數量及每個隱藏層的神經元數量",
  "layers": "指定每層的神經元數量，例如1。對於1層中的5個神經元，輸入5；對於第1層5個神經元，第2層6個神經元，第3層7個神經元，輸入5,6,7",
  "learnfuncparams": "學習函數參數",
  "help": {
    "title": "多層感知器，使用RSNNS套件",
    "r_help": "help(mlp, package ='RSNNS')",
    "body": "\n            <b>注意</b></br>\n            當您指定一個因變數時，它可以是數值型或因子型。如果指定的因變數是因子型，我們會使用RSNNS套件中的解碼函數自動進行虛擬編碼，採用一熱編碼。</br></br>\n            此外，如果您使用一熱編碼對因子變數進行虛擬編碼，您可以在對話框中指定多個因變數。在這種情況下，因變數必須是數值型。</br></br>\n            您可以使用“數據 > 計算虛擬變數”，選擇“保留所有級別”設置以獲得一熱編碼。</br></br>\n            對於因子型的因變數，當使用構建的模型對數據集進行評分時，我們將顯示混淆矩陣、ROC和模型準確性統計信息。生成的預測為因子型，因為我們預測的是類別。這些將與預測概率一起保存在數據集中。</br></br>\n            當存在虛擬編碼的因變數時，我們在使用構建的模型對數據集進行評分時將不顯示混淆矩陣、ROC和模型準確性統計信息。然而，預測將在評分數據集時保存在數據集中。預測是與虛擬編碼因變數相關的概率。</br></br>\n            通常最好對自變數進行標準化（它們也必須是數值型），請參見“數據 > 標準化變數”。</br></br>\n            如果您有分類自變數，請使用一熱編碼對因子變數進行虛擬編碼。</br></br>\n            <b>描述</b></br>\n            此函數創建一個多層感知器（MLP）並對其進行訓練。MLP是完全連接的前饋網絡，可能是使用中最常見的網絡架構。訓練通常通過誤差反向傳播或相關程序進行。</br>\n            SNNS中存在許多不同的學習函數，可以與此函數一起使用，例如Std_Backpropagation、BackpropBatch、BackpropChunk、BackpropMomentum、BackpropWeightDecay、Rprop、Quickprop、SCG（縮放共軛梯度）等。</br>\n            <b>用法</b>\n            <br/>\n            <code> \n            mlp(x, ...)<br/>\n            ## 默認S3方法:<br/>\n            mlp(x, y, size = c(5), maxit = 100,\n              initFunc = \"Randomize_Weights\", initFuncParams = c(-0.3, 0.3),\n              learnFunc = \"Std_Backpropagation\", learnFuncParams = c(0.2, 0),\n              updateFunc = \"Topological_Order\", updateFuncParams = c(0),\n              hiddenActFunc = \"Act_Logistic\", shufflePatterns = TRUE,\n              linOut = FALSE, outputActFunc = if (linOut) \"Act_Identity\" else\n              \"Act_Logistic\", inputsTest = NULL, targetsTest = NULL,\n              pruneFunc = NULL, pruneFuncParams = NULL, ...)\n            </code> <br/>\n            <b>參數</b><br/>\n            <ul>\n            <li>\n            x: 網絡的訓練輸入矩陣\n            </li>\n            <li>\n            ... : 額外的函數參數（當前未使用）\n            </li>\n            <li>\n            y: 相應的目標值\n            </li>\n            <li>\n            size: 隱藏層中的單元數量\n            </li>\n            <li>\n            maxit: 學習的最大迭代次數\n            </li>\n            <li>\n            initFunc: 要使用的初始化函數\n            </li>\n            <li>\n            initFuncParams: 初始化函數的參數\n            </li>\n            <li>\n            learnFunc: 要使用的學習函數\n            </li>\n            <li>\n            learnFuncParams: 學習函數的參數\n            </li>\n            <li>\n            updateFunc: 要使用的更新函數\n            </li>\n            <li>\n            updateFuncParams: 更新函數的參數\n            </li>\n            <li>\n            hiddenActFunc: 所有隱藏單元的激活函數\n            </li>\n            <li>\n            shufflePatterns: 模式是否應被打亂？\n            </li>\n            <li>\n            linOut: 將輸出單元的激活函數設置為線性或邏輯（如果給定outputActFunc則忽略）\n            </li>\n            <li>\n            outputActFunc: 所有輸出單元的激活函數\n            </li>\n            <li>\n            inputsTest: 用於測試網絡的輸入矩陣\n            </li>\n            <li>\n            targetsTest: 測試輸入的相應目標\n            </li>\n            <li>\n            pruneFunc: 要使用的修剪函數\n            </li>\n            <li>\n            pruneFuncParams: 修剪函數的參數。與其他函數不同，這些必須以命名列表的形式給出。關於進一步說明，請參見修剪演示。\n            </li>\n            </ul>\n            <b>詳細信息</b></br>\n            Std_Backpropagation、BackpropBatch等有兩個參數，學習率和最大輸出差異。學習率通常是0.1到1之間的值。它指定了梯度下降的步長。最大差異定義了輸出和目標值之間的差異被視為零誤差的程度，並且不進行反向傳播。此參數用於防止過度訓練。關於所有學習函數的參數的完整列表，請參見SNNS用戶手冊，第67頁。</br>\n            通常不需要更改為初始化和更新函數設置的默認值。</br>\n            <b>值</b><br/>\n            一個rsnns對象。\n            <b>參考文獻</b><br/>\n            Rosenblatt, F. (1958), '感知器：信息存儲和組織的概率模型', 心理學評論 65(6), 386–408.<br/>\n            Rumelhart, D. E.; Clelland, J. L. M. & Group, P. R. (1986), 並行分布處理：認知微觀結構的探索, Mit, 劍橋, MA等.<br/>\n            Zell, A. et al. (1998), 'SNNS斯圖加特神經網絡模擬器用戶手冊, 版本4.2', IPVR, 斯圖加特大學和圖賓根大學WSI.<br/> http://www.ra.cs.uni-tuebingen.de/SNNS/welcome.html<br/>\n            Zell, A. (1994), 神經網絡的模擬, Addison-Wesley.（德文）<br/>\n            <b>示例</b><br/>\n            <code> \n            data(iris)<br/>\n            #打亂向量<br/>\n            iris <- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]<br/>\n            irisValues <- iris[,1:4]<br/>\n            irisTargets <- decodeClassLabels(iris[,5])<br/>\n            #irisTargets <- decodeClassLabels(iris[,5], valTrue=0.9, valFalse=0.1)<br/>\n            iris <- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)<br/>\n            iris <- normTrainingAndTestSet(iris)<br/>\n            model <- mlp(iris$inputsTrain, iris$targetsTrain, size=5, learnFuncParams=c(0.1),\n                          maxit=50, inputsTest=iris$inputsTest, targetsTest=iris$targetsTest)<br/>\n            summary(model)<br/>\n            model<br/>\n            weightMatrix(model)<br/>\n            extractNetInfo(model)<br/>\n            par(mfrow=c(2,2))<br/>\n            plotIterativeError(model)<br/>\n            predictions <- predict(model,iris$inputsTest)<br/>\n            plotRegressionError(predictions[,2], iris$targetsTest[,2])<br/>\n            confusionMatrix(iris$targetsTrain,fitted.values(model))<br/>\n            confusionMatrix(iris$targetsTest,predictions)<br/>\n            plotROC(fitted.values(model)[,2], iris$targetsTrain[,2])<br/>\n            plotROC(predictions[,2], iris$targetsTest[,2])<br/>\n            #使用402040方法的混淆矩陣<br/>\n            confusionMatrix(iris$targetsTrain, encodeClassLabels(fitted.values(model),\n                                                                   method=\"402040\", l=0.4, h=0.6))<br/>\n            </code> <br/>\n            <b>套件</b></br>\n            RSNNS;NeuralNetTools</br>\n            <b>幫助</b></br>\n            要詳細幫助，請單擊此對話框覆蓋右上角的R圖標或在R語法編輯器中運行以下命令</br>\n            help(mlp, package ='RSNNS')\n\t\t\t"
  }
}