{
  "title": "極端梯度提升",
  "navigation": "極端梯度提升",
  "label1": "虛擬編碼因子變量，請參見變量>計算>虛擬編碼使用選項保留所有級別。要預測多個類別，請閱讀幫助（點擊對話框上的？圖標）。",
  "model": "輸入模型名稱",
  "dependentvar": "因變量",
  "independentvars": "自變量",
  "objective": "目標",
  "seed": "設置種子",
  "nrounds": "最大提升迭代次數",
  "maxdepth": "最大深度",
  "minchildweight": "最小子權重",
  "maxdeltastep": "最大增量步長",
  "eta": "學習率（eta）",
  "gamma": "伽馬",
  "numclasses": "類別數量。**僅與multi:softmax和multi:softprob一起使用",
  "basescore": "基礎分數",
  "Verbose": "詳細模式（0=靜默，1=性能信息，2=其他信息）",
  "printevery": "當詳細信息>0時，打印每n次迭代評估消息",
  "help": {
    "title": "極端梯度提升",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>描述</b></br>\n                創建一個極端梯度提升模型\n                <br/>\n                <b>注意</b></br>\n                1.要預測因子類型的因變量，您需要將因變量重新編碼為從0開始的數值。例如，如果因子變量中有3個級別，則數值變量將包含值0,1,2。</br>\n                請參見數據>重新編碼變量。或者只需將因子變量轉換為數值，通常級別將映射到從1開始的整數，並從結果變量中減去1。這將為您提供一個從0開始的數值變量。</br>\n                您還需要將目標設置為multi:softmax（預測類別）或multi:softprob（預測概率），並在類別數量文本框中輸入類別數量。</br>\n                類別數量僅在multi:softmax和multi:softprob中輸入。對於其他目標，如果輸入類別數量，將會生成錯誤。</br>\n                2.您需要對獨立因子變量進行虛擬編碼，使用1-Hot編碼，請參見數據>計算虛擬變量</br>\n                3.當選擇的目標是reg:squarederror、reg:logistic、binary:logitraw和rank:pairwise之一時，不會生成混淆矩陣和ROC曲線，因為預測函數\n                不返回因變量的類別。預測函數返回的數值預測（在reg:squarederror的情況下）、分數、排名等存儲在數據集中。有關更多詳細信息，請參見help(predict.xgb.Booster)</br>\n                4.當選擇的目標是multi:softprob時，不會生成混淆矩陣，因為預測函數返回預測概率而不是因變量的類別。預測概率保存在數據集中，並生成ROC曲線。要查看混淆矩陣，請選擇multi:softmax作為目標。</br>\n                5.當選擇multi:softmax作為目標時，不會生成ROC曲線，因為預測函數返回類別而不是預測概率。要查看ROC曲線，請選擇multi:softprob作為目標。</br>\n                <br/>\n                <b>用法</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>參數</b><br/>\n                <ul>\n                <li>\n                params: 參數列表。完整的參數列表可在http://xgboost.readthedocs.io/en/latest/parameter.html找到。以下是簡短的摘要:<br/>\n                </li>\n                <li>\n                1. 一般參數<br/>\n                此對話框使用gbtree（樹提升器）<br/>\n                </li>\n                <li>\n                2. 提升器參數<br/>\n                2.1. 樹提升器的參數<br/>\n                eta: 控制學習率：將每棵樹的貢獻按0 < eta < 1的因子縮放，當它被添加到當前近似值時。<br/>\n                用於通過使提升過程更保守來防止過擬合。<br/>\n                eta值越低，nrounds值越大：低eta值意味著模型對過擬合更穩健，但計算速度較慢。默認值：0.3<br/>\n                gamma: 進一步劃分樹葉節點所需的最小損失減少。越大，算法越保守。<br/>\n                max_depth: 樹的最大深度。默認值：6<br/>\n                min_child_weight: 子節點中所需的實例權重（海森矩陣）的最小總和。如果樹劃分步驟導致的葉節點的實例權重總和小於min_child_weight，則構建過程將放棄進一步的劃分。在線性回歸模式下，這簡單對應於每個節點中所需的最小實例數量。越大，算法越保守。默認值：1<br/>\n                </li>\n                <li>\n                3. 任務參數<br/>\n                objective: 指定學習任務和相應的學習目標，用戶可以傳遞自定義函數。默認目標選項如下:<br/>\n                reg:squarederror -平方損失回歸（默認）。<br/>\n                reg:logistic 邏輯回歸。<br/>\n                binary:logistic -二元分類的邏輯回歸。輸出概率。<br/>\n                binary:logitraw -二元分類的邏輯回歸，在邏輯變換之前輸出分數。<br/>\n                num_class: 設置類別數量。僅與多類目標一起使用。<br/>\n                multi:softmax -設置xgboost進行多類分類，使用softmax目標。類別由數字表示，應該從0到num_class - 1。<br/>\n                multi:softprob -與softmax相同，但預測輸出一個ndata * nclass元素的向量，可以進一步重塑為ndata，nclass矩陣。結果包含每個數據點屬於每個類別的預測概率。<br/>\n                rank:pairwise -設置xgboost進行通過最小化成對損失的排名任務。<br/>\n                base_score: 所有實例的初始預測分數，全局偏差。默認值：0.5<br/>\n                eval_metric: 驗證數據的評估指標。用戶可以傳遞自定義函數。默認情況下，指標將根據目標分配（回歸的rmse，分類的錯誤，排名的平均精度）。詳細部分提供了列表。<br/>\n                data: 訓練數據集。xgb.train僅接受xgb.DMatrix作為輸入。xgboost還接受矩陣、dgCMatrix或本地數據文件的名稱。<br/>\n                nrounds: 最大提升迭代次數。<br/>\n                verbose: 如果為0，xgboost將保持靜默。如果為1，它將打印性能信息。如果為2，將打印一些額外信息。請注意，設置verbose > 0會自動啟用cb.print.evaluation(period=1)回調函數。<br/>\n                print_every_n: 當verbose>0時，打印每n次迭代評估消息。默認值為1，表示打印所有消息。此參數傳遞給cb.print.evaluation回調。<br/>\n                label: 響應值的向量。當數據是本地數據文件名或xgb.DMatrix時，不應提供。<br/>\n                missing: 默認設置為NA，這意味著算法應將NA值視為“缺失”。有時，0或其他極端值可能用於表示缺失值。此參數僅在輸入為稠密矩陣時使用。<br/>\n                </li>\n                </ul>\n                <b>詳細信息</b></br>\n                當未提供eval_metric參數時，評估指標由Xgboost自動選擇（根據目標）。</br>\n                用戶可以設置一個或多個eval_metric參數。請注意，當使用自定義指標時，僅可以使用此單個指標。以下是Xgboost提供優化實現的內置指標列表：</br>\n                rmse 均方根誤差。http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss 負對數似然。http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss 多類對數損失。http://wiki.fast.ai/index.php/Log_Loss</br>\n                error 二元分類錯誤率。計算為（#錯誤案例）/（#所有案例）。默認情況下，它使用0.5閾值來定義負實例和正實例。</br>\n                可以指定不同的閾值（例如，0.）作為“error@0。”</br>\n                merror 多類分類錯誤率。計算為（#錯誤案例）/（#所有案例）。</br>\n                auc 曲線下面積。http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve用於排名評估。</br>\n                aucpr PR曲線下面積。https://en.wikipedia.org/wiki/Precision_and_recall用於排名評估。</br>\n                ndcg 標準化折扣累積增益（用於排名任務）。http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                當設置某些參數時，以下回調會自動創建：</br>\n                cb.print.evaluation在verbose > 0時啟用；並且print_every_n參數傳遞給它。</br>\n                cb.evaluation.log在watchlist存在時啟用。</br>\n                cb.early.stop: 當設置early_stopping_rounds時。</br>\n                cb.save.model: 當設置save_period > 0時。</br></br>\n                <b>值</b></br>\n                一個xgb.Booster類的對象，具有以下元素：</br>\n                handle 指向內存中xgboost模型的句柄（指針）。</br>\n                raw xgboost模型的緩存內存轉儲，保存為R的原始類型。</br>\n                niter: 提升迭代次數。</br>\n                evaluation_log: 評估歷史記錄存儲為數據表，第一列對應於迭代次數，其餘列對應於評估指標的值。由cb.evaluation.log回調創建。</br>\n                call: 函數調用。</br>\n                params: 傳遞給xgboost庫的參數。請注意，它不捕獲cb.reset.parameters回調更改的參數。</br>\n                callbacks 回調函數，可能是自動分配或明確傳遞的。</br>\n                best_iteration: 具有最佳評估指標值的迭代次數（僅在提前停止時可用）。</br>\n                best_ntreelimit: 對應於最佳迭代的ntreelimit值，可以在predict方法中進一步使用（僅在提前停止時可用）。</br>\n                best_score: 在提前停止期間最佳評估指標值（僅在提前停止時可用）。</br>\n                feature_names: 訓練數據集特徵的名稱（僅在訓練數據中定義了列名時）。</br>\n                nfeatures: 訓練數據中的特徵數量。</br>\n                <b>參考文獻</b></br>\n                Tianqi Chen和Carlos Guestrin，“XGBoost：可擴展的樹提升系統”，第22屆SIGKDD知識發現與數據挖掘會議，2016年，https://arxiv.org/abs/1603.02754</br>\n                <b>另請參見</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>示例</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## 一個簡單的xgb.train示例:</br>\n                # 這些函數可以通過傳遞它們來使用：</br>\n                ## 一個'xgboost'接口示例：</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}