{
  "title": "Boosting de Gradient Extrême",
  "navigation": "Boosting de Gradient Extrême",
  "label1": "VARIABLES FACTORIELLES DE CODE DUMMY, VOIR VARIABLES>CALCULER>OPTION D'UTILISATION DU CODE DUMMY GARDER TOUS LES NIVEAUX. POUR PRÉDIRE DES CLASSES MULTIPLES, LIRE L'AIDE (CLIQUER SUR L'ICÔNE ? DANS LA BOÎTE DE DIALOGUE).",
  "model": "Entrez le nom du modèle",
  "dependentvar": "Variable dépendante",
  "independentvars": "Variable(s) indépendante(s)",
  "objective": "Objectif",
  "seed": "Définir la graine",
  "nrounds": "Nombre maximum d'itérations de boosting",
  "maxdepth": "Profondeur maximale",
  "minchildweight": "Poids minimum de l'enfant",
  "maxdeltastep": "Pas de delta maximum",
  "eta": "eta (taux d'apprentissage)",
  "gamma": "gamma",
  "numclasses": "Nombre de classes. **Utiliser uniquement avec multi:softmax et multi:softprob",
  "basescore": "Score de base",
  "Verbose": "Mode verbeux (0=Silencieux, 1=info de performance, 2= autre info)",
  "printevery": "Imprimer chaque n-ième message d'évaluation d'itération lorsque verbeux > 0",
  "help": {
    "title": "Boosting de Gradient Extrême",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>Description</b></br>\n                Créer un modèle de Boosting de Gradient Extrême\n                <br/>\n                <b>NOTE</b></br>\n                1. Pour prédire une variable dépendante de type facteur, vous devez recoder la variable dépendante en numérique avec des valeurs commençant à 0. Par exemple, s'il y a 3 niveaux dans la variable facteur, la variable numérique contiendra les valeurs 0,1,2.</br>\n                Voir Données > Recode Variables. Alternativement, il suffit de convertir la variable facteur en numérique, typiquement les niveaux seront mappés à des entiers commençant à 1 et soustraire 1 de la variable résultante. Cela vous donnera une variable numérique avec des valeurs commençant à 0.</br>\n                Vous devez également définir l'objectif sur multi:softmax (prédit les classes) ou multi:softprob (prédit les probabilités) et entrer le nombre de classes dans la zone de texte du nombre de classes.</br>\n                Le nombre de classes doit être entré uniquement pour multi:softmax et multi:softprob. Des erreurs seront générées si le nombre de classes est entré pour les autres objectifs.</br>\n                2. Vous devez coder en dummy les variables indépendantes, utiliser l'encodage 1-Hot voir Données > Calculer les Variables Dummy</br>\n                3. Une matrice de confusion et une courbe ROC ne sont pas générées lorsque l'objectif sélectionné est l'un des reg:squarederror, reg:logistic, binary:logitraw et rank:pairwise car la fonction de prédiction \n                ne renvoie pas la classe de la variable dépendante. Les prédictions numériques (dans le cas de reg:squarederror), scores, rangs, etc. que la fonction de prédiction renvoie sont stockés dans le jeu de données. Voir help(predict.xgb.Booster) pour plus de détails</br>\n                4. Une matrice de confusion n'est pas générée lorsque l'objectif sélectionné est multi:softprob car la fonction de prédiction renvoie les probabilités prédites et non la classe de la variable dépendante. Les probabilités prédites sont enregistrées dans le jeu de données et la courbe ROC est générée. Pour voir la matrice de confusion, sélectionnez multi:softmax comme objectif.</br>\n                5. Une courbe ROC n'est pas générée lorsque l'objectif de multi:softmax est sélectionné car la fonction de prédiction renvoie la classe et non les probabilités prédites. Pour voir la courbe ROC, sélectionnez multi:softprob comme objectif.</br>\n                <br/>\n                <b>Utilisation</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>Arguments</b><br/>\n                <ul>\n                <li>\n                params: la liste des paramètres. La liste complète des paramètres est disponible sur http://xgboost.readthedocs.io/en/latest/parameter.html. Voici un résumé plus court:<br/>\n                </li>\n                <li>\n                1. Paramètres généraux<br/>\n                Cette boîte de dialogue utilise gbtree (booster d'arbre)<br/>\n                </li>\n                <li>\n                2. Paramètres du Booster<br/>\n                2.1. Paramètre pour le Booster d'Arbre<br/>\n                eta:  contrôle le taux d'apprentissage: échelle la contribution de chaque arbre par un facteur de 0 < eta < 1 lorsqu'il est ajouté à l'approximation actuelle.<br/>\n                Utilisé pour prévenir le surajustement en rendant le processus de boosting plus conservateur.<br/>\n                Une valeur plus basse pour eta implique une valeur plus grande pour nrounds: une faible valeur d'eta signifie que le modèle est plus robuste au surajustement mais plus lent à calculer. Par défaut: 0.3<br/>\n                gamma: réduction de perte minimale requise pour effectuer une partition supplémentaire sur un nœud feuille de l'arbre. Plus c'est grand, plus l'algorithme sera conservateur.<br/>\n                max_depth: profondeur maximale d'un arbre. Par défaut: 6<br/>\n                min_child_weight: somme minimale du poids des instances (hessien) nécessaire dans un enfant. Si l'étape de partition de l'arbre aboutit à un nœud feuille avec la somme du poids des instances inférieure à min_child_weight, alors le processus de construction abandonnera la partition supplémentaire. En mode régression linéaire, cela correspond simplement au nombre minimum d'instances nécessaires dans chaque nœud. Plus c'est grand, plus l'algorithme sera conservateur. Par défaut: 1<br/>\n                </li>\n                <li>\n                3. Paramètres de tâche<br/>\n                objectif: spécifier la tâche d'apprentissage et l'objectif d'apprentissage correspondant, les utilisateurs peuvent y passer une fonction définie par l'utilisateur. Les options d'objectif par défaut sont ci-dessous:<br/>\n                reg:squarederror -Régression avec perte au carré (Par défaut).<br/>\n                reg:logistic régression logistique.<br/>\n                binary:logistic -régression logistique pour la classification binaire. Probabilité de sortie.<br/>\n                binary:logitraw -régression logistique pour la classification binaire, sortie du score avant transformation logistique.<br/>\n                num_class: définir le nombre de classes. À utiliser uniquement avec des objectifs multiclasses.<br/>\n                multi:softmax -définir xgboost pour faire de la classification multiclasses en utilisant l'objectif softmax. La classe est représentée par un nombre et doit être de 0 à num_class - 1.<br/>\n                multi:softprob -même que softmax, mais la prédiction renvoie un vecteur d'éléments ndata * nclass, qui peut être ensuite remodelé en matrice ndata, nclass. Le résultat contient les probabilités prédites de chaque point de données appartenant à chaque classe.<br/>\n                rank:pairwise -définir xgboost pour effectuer une tâche de classement en minimisant la perte pairwise.<br/>\n                base_score: le score de prédiction initial de toutes les instances, biais global. Par défaut: 0.5<br/>\n                eval_metric: métriques d'évaluation pour les données de validation. Les utilisateurs peuvent y passer une fonction définie par l'utilisateur. Par défaut: la métrique sera assignée selon l'objectif (rmse pour la régression,\n                et erreur pour la classification, précision moyenne pour le classement). La liste est fournie dans la section détail.<br/>\n                data: jeu de données d'entraînement. xgb.train n'accepte qu'un xgb.DMatrix comme entrée. xgboost, en plus, accepte également une matrice, dgCMatrix, ou le nom d'un fichier de données local.<br/>\n                nrounds: nombre maximum d'itérations de boosting.<br/>\n                verbose: Si 0, xgboost restera silencieux. Si 1, il imprimera des informations sur la performance. Si 2, des informations supplémentaires seront imprimées. Notez que le fait de définir verbose > 0 engage automatiquement la fonction de rappel cb.print.evaluation(period=1).<br/>\n                print_every_n: Imprimer chaque n-ième message d'évaluation d'itération lorsque verbeux>0. Par défaut, c'est 1, ce qui signifie que tous les messages sont imprimés. Ce paramètre est passé à la fonction de rappel cb.print.evaluation.<br/>\n                label: vecteur de valeurs de réponse. Ne doit pas être fourni lorsque les données sont un nom de fichier de données local ou un xgb.DMatrix.<br/>\n                missing: par défaut est défini sur NA, ce qui signifie que les valeurs NA doivent être considérées comme 'manquantes' par l'algorithme. Parfois, 0 ou une autre valeur extrême peut être utilisée pour représenter des valeurs manquantes. Ce paramètre n'est utilisé que lorsque l'entrée est une matrice dense.<br/>\n                </li>\n                </ul>\n                <b>Détails</b></br>\n                La métrique d'évaluation est choisie automatiquement par Xgboost (selon l'objectif) lorsque le paramètre eval_metric n'est pas fourni.</br>\n                L'utilisateur peut définir un ou plusieurs paramètres eval_metric. Notez que lors de l'utilisation d'une métrique personnalisée, seule cette métrique unique peut être utilisée. La liste suivante est celle des métriques intégrées pour lesquelles Xgboost fournit une mise en œuvre optimisée:</br>\n                rmse erreur quadratique moyenne. http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss log-vraisemblance négative. http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss logloss multiclasses. http://wiki.fast.ai/index.php/Log_Loss</br>\n                error Taux d'erreur de classification binaire. Il est calculé comme (# cas erronés) / (# tous les cas). Par défaut, il utilise le seuil de 0.5 pour les valeurs prédites pour définir les instances négatives et positives.</br>\n                Un seuil différent (par exemple, 0.) pourrait être spécifié comme \"error@0.\"</br>\n                merror Taux d'erreur de classification multiclasses. Il est calculé comme (# cas erronés) / (# tous les cas).</br>\n                auc Aire sous la courbe. http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve pour l'évaluation du classement.</br>\n                aucpr Aire sous la courbe PR. https://en.wikipedia.org/wiki/Precision_and_recall pour l'évaluation du classement.</br>\n                ndcg Gain Cumulé Normalisé Discounté (pour la tâche de classement). http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                Les rappels suivants sont automatiquement créés lorsque certains paramètres sont définis:</br>\n                cb.print.evaluation est activé lorsque verbeux > 0; et le paramètre print_every_n est passé à celui-ci.</br>\n                cb.evaluation.log est activé lorsque watchlist est présent.</br>\n                cb.early.stop: lorsque early_stopping_rounds est défini.</br>\n                cb.save.model: lorsque save_period > 0 est défini.</br></br>\n                <b>Valeur</b></br>\n                Un objet de classe xgb.Booster avec les éléments suivants:</br>\n                handle un handle (pointeur) vers le modèle xgboost en mémoire.</br>\n                raw un dump de mémoire mis en cache du modèle xgboost enregistré sous le type brut de R.</br>\n                niter: nombre d'itérations de boosting.</br>\n                evaluation_log: historique d'évaluation stocké sous forme de data.table avec la première colonne correspondant au numéro d'itération et le reste correspondant aux valeurs des métriques d'évaluation. Il est créé par le rappel cb.evaluation.log.</br>\n                call: un appel de fonction.</br>\n                params: paramètres qui ont été passés à la bibliothèque xgboost. Notez qu'il ne capture pas les paramètres modifiés par le rappel cb.reset.parameters.</br>\n                callbacks fonctions de rappel qui ont été soit automatiquement assignées soit explicitement passées.</br>\n                best_iteration: numéro d'itération avec la meilleure valeur de métrique d'évaluation (uniquement disponible avec l'arrêt précoce).</br>\n                best_ntreelimit: la valeur ntreelimit correspondant à la meilleure itération, qui pourrait être utilisée ultérieurement dans la méthode de prédiction (uniquement disponible avec l'arrêt précoce).</br>\n                best_score: la meilleure valeur de métrique d'évaluation pendant l'arrêt précoce. (uniquement disponible avec l'arrêt précoce).</br>\n                feature_names: noms des caractéristiques du jeu de données d'entraînement (uniquement lorsque les noms de colonnes ont été définis dans les données d'entraînement).</br>\n                nfeatures: nombre de caractéristiques dans les données d'entraînement.</br>\n                <b>Références</b></br>\n                Tianqi Chen et Carlos Guestrin, \"XGBoost: A Scalable Tree Boosting System\", 22ème Conférence SIGKDD sur la Découverte de Connaissances et l'Extraction de Données, 2016, https://arxiv.org/abs/1603.02754</br>\n                <b>Voir aussi</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>Exemples</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## Un exemple simple de xgb.train:</br>\n                # Ces fonctions pourraient être utilisées en leur passant soit:</br>\n                ## Un exemple d'interface 'xgboost':</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}