{
  "title": "Construire un modèle ajusté avec validation croisée k-Fold",
  "navigation": "Validation croisée k-Fold",
  "modelname": "Entrez le nom du modèle ajusté",
  "folds": "Entrez le nombre de plis",
  "dependent": "Variable à prédire",
  "levelOfInterest": "Lorsque la variable à prédire a 2 niveaux, spécifiez le niveau d'intérêt. La matrice de confusion et les statistiques associées sont affichées avec le niveau d'intérêt spécifié comme référence",
  "independent": "Variables indépendantes",
  "label1": "CERTAINS MODÈLES SONT SENSIBLES AUX VALEURS MANQUANTES OU AUX VARIABLES DÉPENDANTES QUI SONT DES FACTEURS. POUR TRAITER LES VALEURS MANQUANTES, VOIR [ VARIABLES > VALEURS MANQUANTES ] ET POUR LE CODAGE DUMMY DES VARIABLES FACTEURS, VOIR [ VARIABLES > CALCULER > CODAGE DUMMY ]. Cliquez sur l'icône ? en haut à droite de la boîte de dialogue pour plus de détails.",
  "ModelSelection": "Sélectionnez une catégorie de modèle puis le type de modèle",
  "help": {
    "title": "Construire un modèle ajusté avec validation croisée k-Fold",
    "r_help": "help(train, package=caret)",
    "body": "\n<b>Description</b></br>\nLa validation croisée k-Fold utilise les fonctions trainControl et train du package caret pour créer un modèle ajusté/optimisé.<br/>\nUn modèle ajusté est construit en rééchantillonnant les résultats à travers les paramètres d'ajustement. Les résultats de l'ajustement sont affichés sous les tableaux en bas de la sortie<br/>\nUne fois le modèle ajusté construit, nous faisons des prédictions en utilisant le modèle ajusté contre le jeu de données d'entraînement (utilisé pour construire le modèle) et affichons la matrice de confusion et les statistiques associées.<br/>\nREMARQUE : voir l'URL à https://www.blueskystatistics.com/Articles.asp?ID=330 pour des détails sur les modèles qui nécessitent que les variables dépendantes soient codées en dummy et les exigences correspondantes pour les variables dépendantes prises en charge. De plus, certains modèles ne prennent pas en charge les valeurs manquantes, s'il y a des erreurs affichées lors de la construction du modèle, supprimez les valeurs manquantes.<br/>\nLe modèle ajusté est stocké dans l'objet de classe train avec un nom que vous avez spécifié dans le contrôle de la zone de texte.<br/>\nLes valeurs des paramètres sélectionnés pour le modèle ajusté final sont affichées en bas dans la sortie.<br/>\nDans le cas de prédicteurs avec 2 niveaux, vous avez la possibilité de sélectionner le niveau de référence/niveau d'intérêt.<br/>\nLa matrice de confusion et les statistiques associées sont créées en utilisant le niveau d'intérêt spécifié.<br/>\n<code> \ntrain_control <- caret::trainControl(method = \"cv\", number = Entrez le nombre de plis)<br/>\n#Syntaxe générale<br/>\ntunedModel <- caret::train(x = échantillon de données pour entraîner le modèle, y = variable dépendante, trControl = train_control, method = \"le modèle à utiliser\", prob.model = TRUE )<br/>\n#Syntaxe peuplée d'exemple<br/>\ntunedModel <- caret::train(as.data.frame(TrainData), variableDépendante, trControl = train_control, method = \"adaboost\", preProcess = NULL, prob.model = TRUE )<br/>\n</code>\nNous générons une matrice de confusion et des statistiques de précision pour le modèle ajusté. Cela se fait comme suit\n<ul>\n<li>\nNous générons des valeurs prédites à partir du jeu de données d'entraînement en appelant la fonction predict sur le modèle optimisé de classe train (Internement, predict.train est la fonction dans le package caret que R appelle)<br/>\n<code> \nvaleursPrédites = predict(modèleAjusté retourné par la fonction train)<br/>\n</code> <br/>\n</li>\n<li>\nNous calculons les statistiques de précision à partir de la matrice de confusion générée comme ci-dessous<br/>\n<code> \ncaret::confusionMatrix (prédictions = valeursPrédites, référence = variable dépendante du modèle)<br/>\n</code> \n</li>\n</ul>\nVous pouvez utiliser le modèle final ajusté/optimisé pour évaluer un jeu de données. Pour ce faire, suivez les étapes ci-dessous<br/>\n1.  Allez au jeu de données que vous souhaitez évaluer. Remarque : Les noms de variables dans le jeu de données à évaluer (a.k.a. variables indépendantes) doivent correspondre aux noms de variables que vous avez utilisés dans le jeu de données pour créer à l'origine le modèle ajusté. Sinon, une erreur s'affichera. Ce message d'erreur s'affichera sous la section des tests diagnostiques dans la boîte de dialogue d'évaluation du modèle, voir le point 2 ci-dessous. <br/>\n2.\tOuvrez la boîte de dialogue d'évaluation du modèle sous Évaluation du modèle>Prédire>Évaluation du modèle. <br/>\n3.\tSélectionnez un modèle que vous souhaitez utiliser pour évaluer le jeu de données. Vous pouvez filtrer les modèles que vous avez construits par classe. <br/>\n4.\tUne fois que vous avez sélectionné le modèle, spécifiez un préfixe à utiliser pour stocker les valeurs prédites. Vous pouvez éventuellement enregistrer des intervalles de confiance pour les prédictions et générer une matrice de confusion. Vous devez spécifier un préfixe pour la variable contenant les prédictions/notes. Les prédictions et les probabilités prédites, le cas échéant, sont stockées comme nouvelles variables à la fin du jeu de données. Par exemple, si le préfixe spécifié est AA, les prédictions sont stockées dans une variable AA_nom de variable dépendante_Prévisions, les probabilités prédites sont stockées dans une variable AA_Niveau1_ProbabilitésPrédites. <br/>\n5.  Cliquez sur le bouton Exécuter pour évaluer le jeu de données. <br/>\n<b>Package</b></br>\ncaret</br>\n<b>Aide</b></br>\nPour une aide détaillée, cliquez sur l'icône R en haut à droite de cette boîte de dialogue ou exécutez la commande suivante help(train, package ='caret') dans la fenêtre de l'éditeur R\n"
  }
}