{
  "title": "極端梯度提升",
  "navigation": "極端梯度提升",
  "label1": "虛擬碼因子變數，請參見變數>計算>虛擬碼使用選項保留所有級別。對於預測多個類別，請閱讀幫助（點擊對話框上的 ? 圖標）。",
  "model": "輸入模型名稱",
  "dependentvar": "因變量",
  "independentvars": "自變量",
  "objective": "目標",
  "seed": "設置隨機種子",
  "nrounds": "最大提升迭代次數",
  "maxdepth": "最大深度",
  "minchildweight": "最小子權重",
  "maxdeltastep": "最大增量步驟",
  "eta": "eta（學習率）",
  "gamma": "gamma",
  "numclasses": "類別數。**僅用於multi:softmax和multi:softprob",
  "basescore": "基準分數",
  "Verbose": "詳細模式（0=靜音，1=性能信息，2=其他信息）",
  "printevery": "當詳細模式 > 0 時，打印每 n 次迭代評估消息",
  "OptvarTaskparam": "任務參數",
  "OptvarAdvDiagnostics": "高級診斷",
  "OptvarTreeBoostparam": "樹提升參數",  
  "help": {
    "title": "極端梯度提升",
    "r_help": "help(xgboost, package ='xgboost')",
    "body": "\n                <b>描述</b></br>\n                創建一個極端梯度提升模型\n                <br/>\n                <b>注意</b></br>\n                1.對於預測因變量類型為因子的情況，您需要將因變量重新編碼為從 0 開始的數值。例如，如果因子變量有 3 個級別，則數值變量將包含值 0,1,2。</br>\n                請參見數據 > 重新編碼變量。或者只需將因子變量轉換為數值，通常級別將映射到從 1 開始的整數，然後從結果變量中減去 1。這將為您提供一個從 0 開始的數值變量。</br>\n                您還需要將目標設置為 multi:softmax（預測類別）或 multi:softprob（預測概率），並在類別數文本框中輸入類別數。</br>\n                類別數僅在 multi:softmax 和 multi:softprob 中輸入。對於其他目標，如果輸入類別數，將會產生錯誤。</br>\n                2. 您需要對自變量因子變數進行虛擬編碼，使用 1-Hot 編碼，請參見數據 > 計算虛擬變量</br>\n                3. 當選擇的目標為 reg:squarederror、reg:logistic、binary:logitraw 和 rank:pairwise 時，不會生成混淆矩陣和 ROC 曲線，因為預測函數\n                不會返回因變量的類別。預測函數返回的數值預測（在 reg:squarederror 的情況下）、分數、排名等將存儲在數據集中。請參見 help(predict.xgb.Booster) 獲取更多詳細信息</br>\n                4. 當選擇的目標為 multi:softprob 時，不會生成混淆矩陣，因為預測函數返回預測概率而不是因變量的類別。預測概率將保存在數據集中，並生成 ROC 曲線。要查看混淆矩陣，請選擇 multi:softmax 作為目標。</br>\n                5. 當選擇的目標為 multi:softmax 時，不會生成 ROC 曲線，因為預測函數返回類別而不是預測概率。要查看 ROC 曲線，請選擇 multi:softprob 作為目標。</br>\n                <br/>\n                <b>用法</b>\n                <br/>\n                <code> \n                xgboost(data = NULL, label = NULL, missing = NA, weight = NULL,\n                  params = list(), nrounds, verbose = 1, print_every_n = 1L,\n                  early_stopping_rounds = NULL, maximize = NULL, save_period = NULL,\n                  save_name = \"xgboost.model\", xgb_model = NULL, callbacks = list(), ...)\n                </code> <br/>\n                <b>參數</b><br/>\n                <ul>\n                <li>\n                params: 參數列表。完整的參數列表可在 http://xgboost.readthedocs.io/en/latest/parameter.html 獲得。以下是簡短的摘要:<br/>\n                </li>\n                <li>\n                1. 一般參數<br/>\n                此對話框使用 gbtree（樹增強器）<br/>\n                </li>\n                <li>\n                2. 增強器參數<br/>\n                2.1. 樹增強器的參數<br/>\n                eta: 控制學習率：將每棵樹的貢獻按 0 < eta < 1 的因子進行縮放，當它被添加到當前近似值時。<br/>\n                用於通過使增強過程更加保守來防止過擬合。<br/>\n                eta 的較低值意味著 nrounds 的較大值：低 eta 值意味著模型對過擬合更具穩健性，但計算速度較慢。默認值：0.3<br/>\n                gamma: 進一步劃分樹葉節點所需的最小損失減少。越大，算法越保守。<br/>\n                max_depth: 樹的最大深度。默認值：6<br/>\n                min_child_weight: 子節點中所需的實例權重（海森）最小總和。如果樹劃分步驟導致的葉節點的實例權重總和小於 min_child_weight，則構建過程將放棄進一步劃分。在線性回歸模式下，這簡單地對應於每個節點中所需的實例數量。越大，算法越保守。默認值：1<br/>\n                </li>\n                <li>\n                3. 任務參數<br/>\n                objective: 指定學習任務及相應的學習目標，用戶可以將自定義函數傳遞給它。默認目標選項如下:<br/>\n                reg:squarederror -平方損失的回歸（默認）。<br/>\n                reg:logistic 邏輯回歸。<br/>\n                binary:logistic -二元分類的邏輯回歸。輸出概率。<br/>\n                binary:logitraw -二元分類的邏輯回歸，輸出邏輯變換前的分數。<br/>\n                num_class: 設置類別數。僅用於多類目標。<br/>\n                multi:softmax -設置 xgboost 以使用 softmax 目標進行多類分類。類別由數字表示，應從 0 到 num_class - 1。<br/>\n                multi:softprob -與 softmax 相同，但預測輸出為 ndata * nclass 元素的向量，可以進一步重塑為 ndata, nclass 矩陣。結果包含每個數據點屬於每個類別的預測概率。<br/>\n                rank:pairwise -設置 xgboost 以通過最小化成對損失來執行排名任務。<br/>\n                base_score: 所有實例的初始預測分數，全局偏差。默認值：0.5<br/>\n                eval_metric: 驗證數據的評估指標。用戶可以將自定義函數傳遞給它。默認情況下，根據目標將分配指標（回歸的 rmse，分類的錯誤，排名的平均精度）。詳細部分提供了列表。<br/>\n                data: 訓練數據集。xgb.train 只接受 xgb.DMatrix 作為輸入。xgboost 另外還接受矩陣、dgCMatrix 或本地數據文件的名稱。<br/>\n                nrounds: 最大提升迭代次數。<br/>\n                verbose: 如果為 0，xgboost 將保持靜音。如果為 1，則會打印有關性能的信息。如果為 2，則會打印一些額外的信息。請注意，設置 verbose > 0 會自動啟用 cb.print.evaluation(period=1) 回調函數。<br/>\n                print_every_n: 當 verbose>0 時，打印每 n 次迭代評估消息。默認為 1，這意味著打印所有消息。此參數傳遞給 cb.print.evaluation 回調。<br/>\n                label: 響應值的向量。當數據是本地數據文件名稱或 xgb.DMatrix 時，則不應提供。<br/>\n                missing: 默認設置為 NA，這意味著算法應將 NA 值視為“缺失”。有時，0 或其他極端值可能用於表示缺失值。此參數僅在輸入為稠密矩陣時使用。<br/>\n                </li>\n                </ul>\n                <b>詳細信息</b></br>\n                當未提供 eval_metric 參數時，評估指標由 Xgboost 自動選擇（根據目標）。</br>\n                用戶可以設置一個或多個 eval_metric 參數。請注意，使用自定義指標時，僅可以使用這個單一指標。以下是 Xgboost 提供優化實現的內置指標列表：</br>\n                rmse 均方根誤差。http://en.wikipedia.org/wiki/Root_mean_square_error</br>\n                logloss 負對數似然。http://en.wikipedia.org/wiki/Log-likelihood</br>\n                mlogloss 多類對數損失。http://wiki.fast.ai/index.php/Log_Loss</br>\n                error 二元分類錯誤率。計算為（# 錯誤案例） / （# 所有案例）。默認情況下，使用 0.5 的閾值來定義負實例和正實例。</br>\n                可以指定不同的閾值（例如，0.）作為 \"error@0.\"</br>\n                merror 多類分類錯誤率。計算為（# 錯誤案例） / （# 所有案例）。</br>\n                auc 曲線下面積。http://en.wikipedia.org/wiki/Receiver_operating_characteristic#'Area_under_curve 用於排名評估。</br>\n                aucpr PR 曲線下面積。https://en.wikipedia.org/wiki/Precision_and_recall 用於排名評估。</br>\n                ndcg 正規化折扣累積增益（用於排名任務）。http://en.wikipedia.org/wiki/NDCG</br>\n                </br>\n                當設置某些參數時，會自動創建以下回調：</br>\n                cb.print.evaluation 在 verbose > 0 時啟用；並且傳遞了 print_every_n 參數。</br>\n                cb.evaluation.log 在 watchlist 存在時啟用。</br>\n                cb.early.stop: 當設置 early_stopping_rounds 時。</br>\n                cb.save.model: 當設置 save_period > 0 時。</br></br>\n                <b>值</b></br>\n                一個 xgb.Booster 類的對象，具有以下元素：</br>\n                handle 指向內存中 xgboost 模型的句柄（指針）。</br>\n                raw xgboost 模型的緩存內存轉儲，保存為 R 的原始類型。</br>\n                niter: 提升迭代次數。</br>\n                evaluation_log: 評估歷史記錄存儲為 data.table，第一列對應於迭代次數，其餘列對應於評估指標的值。由 cb.evaluation.log 回調創建。</br>\n                call: 函數調用。</br>\n                params: 傳遞給 xgboost 庫的參數。請注意，它不捕獲由 cb.reset.parameters 回調更改的參數。</br>\n                callbacks 回調函數，這些函數是自動分配的或明確傳遞的。</br>\n                best_iteration: 具有最佳評估指標值的迭代次數（僅在早期停止時可用）。</br>\n                best_ntreelimit: 對應於最佳迭代的 ntreelimit 值，可以進一步用於預測方法（僅在早期停止時可用）。</br>\n                best_score: 在早期停止期間的最佳評估指標值。（僅在早期停止時可用）。</br>\n                feature_names: 訓練數據集特徵的名稱（僅當在訓練數據中定義了列名時）。</br>\n                nfeatures: 訓練數據中的特徵數量。</br>\n                <b>參考文獻</b></br>\n                Tianqi Chen 和 Carlos Guestrin，\"XGBoost: A Scalable Tree Boosting System\"，第 22 屆 SIGKDD 知識發現與數據挖掘會議，2016，https://arxiv.org/abs/1603.02754</br>\n                <b>另請參見</b></br>\n                callbacks, predict.xgb.Booster, xgb.cv</br>\n                <b>示例</b></br>\n                data(agaricus.train, package='xgboost')</br>\n                data(agaricus.test, package='xgboost')</br>\n                ## 一個簡單的 xgb.train 示例：</br>\n                # 這些函數可以通過傳遞它們來使用：</br>\n                ## 一個 'xgboost' 接口示例：</br>\n                bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label,\n                               max_depth = 2, eta = 1, nthread = 2, nrounds = 2,\n                               objective = \"binary:logistic\")</br>\n                pred <- predict(bst, agaricus.test$data)</br>\n                "
  }
}